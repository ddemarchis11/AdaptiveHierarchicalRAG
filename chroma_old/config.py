import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Optional
from dotenv import load_dotenv

env_path = Path(".") / "elastic-start-local" / ".env"
load_dotenv(dotenv_path=env_path)

@dataclass
class ElasticsearchConfig:
    url: str = os.getenv("ES_LOCAL_URL", "http://localhost:9200")
    user: str = os.getenv("ES_LOCAL_USER", "elastic")
    password: str = os.getenv("ES_LOCAL_PASSWORD")
    index_name: str = "chunks_hybrid"
    verify_certs: bool = False
    

@dataclass
class Neo4jConfig:
    uri: str = "bolt://localhost:7687"
    user: str = "neo4j"
    password: str = "password"
    database: str = "graph-chunks"


@dataclass
class ChromaConfig:
    persist_directory: str = "chroma-data"
    collection_name_chunks: str = "chunks_300"
    collection_name_communities: str = "communities"


@dataclass
class EmbeddingConfig:
    model_name: str = "BAAI/bge-large-en-v1.5"
    batch_size: int = 32
    embedding_size: int = 1024


@dataclass
class ChunkingConfig:
    chunk_size: int = 300         
    overlap_ratio: float = 0.1
    extensions: List[str] = field(default_factory=lambda: [".txt", ".md"])


@dataclass
class SimilarityConfig:
    top_k_neighbors: int = 5      
    similarity_threshold: float = 0.6  


@dataclass
class GraphConfig:
    graph_name: str = "chunk_graph"
    chunk_dim: int = 1024          
    gds_concurrency: int = 4
    community_algo: str = "louvain"
    community_model: str = "llama-3.3-70b-versatile"
    top_n_chunks: int = 5


@dataclass
class BaselineHybridBM25Config:
    topK_dense: int = 100
    topK_sparse: int = 100
    rrf_k: int = 60
    top_k_final: int = 5          
    verbose: bool = True


@dataclass
class HybridRetrieverConfig:
    rrf_k: int = 60
    w_dense: float = 1.0
    w_sparse: float = 1.0


@dataclass
class QueryConfig:
    lambda_ppr: float = 0.15
    candidate_multiplier: int = 5  


@dataclass
class LLMConfig:
    api_key: str = ":)"           
    model: Optional[str] = "moonshotai/kimi-k2-instruct-0905"
    temperature: float = 0.0
    max_tokens: int = 1024
    # reasoning_effort: str = "medium"


@dataclass
class EvaluationConfig:
    input_file: str = ""
    out_dir: str = ""
    model: str = "gpt-4o"
    project_endpoint: str = ""
    api_version: str ="2024-10-21"
    temperature: float = 0
    eval_dir: str = "llm_evaluations"
    system_prompt = """You are an impartial evaluator for QA systems.
You receive: 
(1) The User's Question
(2) The Gold Answer (Ground Truth - usually concise)
(3) The Model's Answer (Generated - usually detailed)
(4) The Retrieved Context

**CRITICAL INSTRUCTION ON BIAS MITIGATION:**
The Gold Answer is often a short, factual summary. The Model Answer generated by the RAG system is expected to be more detailed, explanatory, and verbose. 
**DO NOT PENALIZE** the Model Answer for being longer or providing more context than the Gold Answer, as long as that extra information is:
1. Relevant to the question.
2. Supported by the Retrieved Context.
3. Consistent with the core truth of the Gold Answer.

Evaluate using this rubric (0.0 to 1.0):

- answer_correctness:
  Focus on **Information Containment**.
  1.0 = The Model Answer **contains** the core meaning of the Gold Answer. It may add correct details/context not present in the Gold Answer.
  0.5 = The Model Answer misses some nuances or is only partially correct.
  0.0 = Wrong, irrelevant, or fails to answer the question.

- faithfulness_to_context:
  Focus on **Grounding**.
  1.0 = All claims in the answer are supported by the Context.
  0.5 = Mostly grounded, but includes some unsupported details.
  0.0 = Contradicts the context or hallucinates.
  *NOTE*: If the context is irrelevant but the model answers correctly using internal knowledge, Faithfulness is LOW (0), but Correctness is HIGH.

- evidence_coverage:
  Focus on **Key Facts Retrieval**.
  Use the Gold Answer as the checklist of "must-have" facts.
  1.0 = The Model Answer includes **all** specific entities/facts (dates, names, places) mentioned in the Gold Answer.
  0.5 = Covers some facts but misses others.
  0.0 = Misses the central factual points.

Return ONLY a valid JSON object:
{
    "answer_correctness": float,
    "faithfulness_to_context": float,
    "evidence_coverage": float,
    "explanation": "string"
}
"""


@dataclass
class CommunitySimilarityConfig:
    top_k_neighbors: int = 20
    similarity_threshold: float = 0.78
    cross_doc_only: bool = True
    distance_to_sim: str = "inv"


@dataclass
class CommunityExpansionConfig:
    hops: int = 1
    max_expanded_communities: int = 30
    min_edge_sim: float = 0.0


@dataclass
class PipelineConfig:
    documents_path: Path = Path("")
    questions_path: Path = Path("")

    neo4j: "Neo4jConfig" = field(default_factory=lambda: Neo4jConfig())
    es: ElasticsearchConfig = field(default_factory=ElasticsearchConfig)
    chroma: "ChromaConfig" = field(default_factory=lambda: ChromaConfig())
    embeddings: "EmbeddingConfig" = field(default_factory=lambda: EmbeddingConfig())
    chunking: "ChunkingConfig" = field(default_factory=lambda: ChunkingConfig())
    similarity: "SimilarityConfig" = field(default_factory=lambda: SimilarityConfig())
    graph: "GraphConfig" = field(default_factory=lambda: GraphConfig())

    retrieval: "BaselineHybridBM25Config" = field(default_factory=lambda: BaselineHybridBM25Config())
    hybrid: "HybridRetrieverConfig" = field(default_factory=lambda: HybridRetrieverConfig())
    evaluation: "EvaluationConfig" = field(default_factory=lambda: EvaluationConfig())
    llm: "LLMConfig" = field(default_factory=lambda: LLMConfig())

    query: "QueryConfig" = field(default_factory=lambda: QueryConfig())

    community_similarity: CommunitySimilarityConfig = field(default_factory=CommunitySimilarityConfig)
    community_expansion: CommunityExpansionConfig = field(default_factory=CommunityExpansionConfig)

    query_top_k_chunks: int = 10
    query_top_k_communities: int = 15
    query_tot_seeds: int = 80


DEFAULT_CONFIG = PipelineConfig()