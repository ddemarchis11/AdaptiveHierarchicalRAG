{"corpus_name": "tesi_magistrale-7e36f6fd", "text": "Indice\n1 Introduzione 1\n2 Prerequisiti 1\n2.1 Inverted index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Positional index e Phrase Queries . . . . . . . . . . . . . . . . . . 2\n2.3 Tokenizzazione ed Analyzers . . . . . . . . . . . . . . . . . . . . . 2\n2.4 Metriche nella Full-Text Search . . . . . . . . . . . . . . . . . . . 4\n2.5 Problemi Tipici del Full-Text Retrieval . . . . . . . . . . . . . . . 5\n2.6 Dense retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.7 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.8 Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.8.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.9 Hybrid Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.10 Approximate Nearest Neighbor (ANN) e HNSW . . . . . . . . . 11\n2.11 Community Detection e Clustering Semantico . . . . . . . . . . . 12\n2.11.1 Louvain e Leiden . . . . . . . . . . . . . . . . . . . . . . . 13\n2.12 Communities nel Retrieval . . . . . . . . . . . . . . . . . . . . . . 14\n2.13 Language Models Generativi . . . . . . . . . . . . . . . . . . . . 14\n2.14 Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.15 LLM-as-a-Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.15.1 Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.16 SDK e Cloud Providers . . . . . . . . . . . . . . . . . . . . . . . 19\n2.17 LangChain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.18 Groq . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3 Retrieval Augmented Generation 20\n3.1 Semantic Collapse, Hubness e Chunk Dependency . . . . . . . . 21\n3.2 Proposta Iniziale . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4 Chunking 23\n4.1 Sentence Transformers . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.2 Strategie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n4.2.1 Fixed e Recursive Chunking . . . . . . . . . . . . . . . . . 26\n4.2.2 Semantic Chunking . . . . . . . . . . . . . . . . . . . . . . 26\n4.3 Considerazioni su SQuAD ed NQ Dataset . . . . . . . . . . . . . 28\n5 Architetture RAG 31\n5.1 Naive RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.1.1 Azure SDK . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n5.2 Hierarchical RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.2.1 RAPTOR RAG . . . . . . . . . . . . . . . . . . . . . . . . 38\n5.2.2 Microsoft GraphRAG . . . . . . . . . . . . . . . . . . . . 40\n5.2.3 Semantic Chunk Graph . . . . . . . . . . . . . . . . . . . 43\n5.2.4 Chunking e Communities . . . . . . . . . . . . . . . . . . 45\ni\n5.3 CoT RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.3.1 Scratchpad . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.3.2 Step-Back Prompting . . . . . . . . . . . . . . . . . . . . 47\n5.4 RAG Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n5.4.1 GraphRAG Bench . . . . . . . . . . . . . . . . . . . . . . 51\n5.4.2 Risultati . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n5.4.3 HotpotQA . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.5 UI Streamlit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.5.1 Text Extraction . . . . . . . . . . . . . . . . . . . . . . . . 62\n6 RAG per Structured Data 63\n6.1 Data Lakes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n6.2 Semantic Table Graph . . . . . . . . . . . . . . . . . . . . . . . . 63\n6.3 HybridQA Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n6.4 Risultati . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nii\n1 Introduzione\nQuesto lavoro di tesi adotta una prospettiva principalmentesperimentale,\nanzi: l’obiettivo non è realizzare un sistema production-ready né ottimizzare\nun’architettura rispetto a latenza, costi o vincoli di governance dei dati. Lo\nscopo è invece capire quali scelte incidano di più sulla qualità del retrieval e,\ndi conseguenza, sulle risposte prodotte da un Language Model in una pipeline\nRAG. Il punto di partenza è che le prestazioni di un sistema RAG dipendono in\ngran parte dal retrieval e dal modo in cui rendi l’informazione interrogabile; per\nquesto, si valutano e confrontano approcci RAG diversi: da soluzioni semplici\na pipeline più strutturate, discutendo di come queste si collochino rispetto alle\ncriticità fondamentali del RAG moderno. L’analisi si concentra sulle famiglie:\nRAG gerarchico, Naive ed Adattivo. Questi approcci sono particolarmente in-\nteressati quando la base documentale è eterogenea, come nei Data Lake: non\nsolo testo, ma anche tabelle relazionali, metadati... che difficilmente si riducono\nsenza eccessiva perdita d’informazione ad un insieme di chunk indipendenti; le\nstrutture graph based permettono di modellare il retrieval, rispetto a query non\nSQL like, anche per dati strutturati come questi. Dal punto di vista metodo-\nlogico, la tesi propone un confronto sperimentale osservando come cambino le\nprestazioni al variare di:\n•strategia di indicizzazione(indice piatto vs strutture gerarchiche / a\ngrafo),\n•strategia di retrieval(single-shot top-kvs comunità/percorsi),\n•modalità di integrazione delle fonti(testo non strutturato e dati\nstrutturati tabellari),\nL’implementazione usa più tecnologie: Azure AI Foundry come baseline per un\nNaive RAG, LangChain e ChromaDB per costruire varianti più articolate (ad\nesempio CoT e GraphRAG) e Neo4j per gestire strutture graph-based. Anche\nqueste scelte seguono l’impostazione del lavoro: non cercare “la” soluzione, ma\nmisurare il tradeoff che c’è tra prestazioni e retrieval.\nMoltecriticitàattribuiteaimodelligenerativi(allucinazioni, incoerenze, risposte\ngeneriche) dipendono spesso dal problema che c’è a monte: recuperare informa-\nzione pertinente alla domanda; i capitoli successivi formalizzano il paradigma\nRAG, discutono i limiti degli approcci naïve e presentano il confronto sperimen-\ntale tra i metodi considerati, con particolare enfasi sulle soluzioni gerarchiche e\ngraph-based.\n2 Prerequisiti\nQuesto capitolo ha l’obiettivo di fornire le nozioni essenziali per interpretare\ncorrettamente le scelte progettuali e le componenti tecniche del lavoro. Alcuni\nconcetti verranno introdotti da zero durante la discussione, altri richiamati per\n1\ncoerenza, ma per la maggior parte dedicare ulteriore tempo, ogni volta, alle defi-\nnizioni necessarie appesantirebbe la trattazione e distoglierebbe eccessivamente\nl’attenzione dal tema principale; per questo motivo, spezziamo i prerequisiti in\nuna sezione dedicata.\n2.1 Inverted index\nL’inverted index(indice invertito) è la struttura dati principe dei motori di ri-\ncerca tradizionali: associa a ogni termine l’elenco dei documenti in cui compare\ne si basa interamente sull’idea che una query possa essere risposta da un dato set\ndi documenti che puoi definire rilevanti poiché posseggono una sovrapposizione\nlessicale esatta rispetto alla richiesta stessa; difatti, così la valutazione di una\nquery evita la scansione completa del corpus e si riduce alla combinazione delle\nposting lists, ovvero le liste di documenti, associate ad ogni termine della query.\nCiascuna lista contiene tipicamente gli identificativi dei documenti (docID) e,\nspesso, informazioni utili al ranking come la frequenza del termine nel documen-\nto. L’indice invertito rende, quindi, efficiente la ricerca anche su collezioni molto\ngrandi perché sposta il costo computazionale dalla lettura dei documenti all’o-\nperazione dimergetra liste ordinate; nella stragrande maggioranze dei motori\ndi ricerca, come Google ad esempio, si sfrutta un’intersezione delle posting lists,\novvero default AND trai termini della query, appunto richiamando il concetto\nprecedente di sovrapposizione lessicale.\n2.2 Positional index e Phrase Queries\nUnpositional indexestende l’indice invertito memorizzando, per ogni termine\ne per ogni documento, anche le posizioni (offset) in cui esso appare. Questa\ninformazione aggiuntiva abilita anche query di tipo diverso: lephrase queries;\nqueste sono concettualmente semplici, poiché a te non interessa la sovrappo-\nsizione lessicale esatta indipendentemente dalla posizione, nel caso precedente\nl’AND trai termini della query impone solo che questi debbano comparire nei\ndocumenti di riferimento ma, potenzialmente, anche in ordine diverso, ma vuoi\nmantenere anche l’ordine posizionale. Infatti, richiedono che i termini com-\npaiano adiacenti e nell’ordine corretto, basando la verifica sul confronto tra le\ninformazioni posizionali: dato un documento candidato, si controlla tipicamente\nse, per ogni coppia di termini, la differenza tra le posizioni relative è la stessa\nnel documento e nella query. Le phrase queries estendono il concetto stesso\nalla base della ricerca cosiddetta full - text: sovrapposizione lessicale esatta è\nrilevanza, difatti coincidono con il caso in cui la ricerca token - based performi,\nin realtà, meglio in generale; su questo concetto e sui problemi che ne derivano\nci torneremo successivamente.\n2.3 Tokenizzazione ed Analyzers\nDiscutendo di Inverted e Positional Index abbiamo presupposto che quei ’termi-\nni’ a cui associ le posting lists tu li abbia già: in realtà questo comporta spesso\n2\ndelle vere e proprie pipeline che applichi al testo grezzo in ingresso, formalmente\nqueste vengono definiteanalyzer. Tra le trasformazioni più comuni rientrano il\ncase-folding, la rimozione distopworde procedure di stemming o lemmatizzazio-\nne per ricondurre varianti morfologiche a una forma più stabile, con l’obiettivo\nprincipale di ridurre la variabilità del testo e aumentare larecall, accettando\nperò situazioni in cui parole semanticamente diverse vengono ricondotte alla\nstessa radice sintattica. Gli analyzer più comuni differiscono soprattutto per\nla strategia di tokenizzazione: un analyzerstandarddivide il testo basandosi\nsui confini “parola” secondo lo standard Unicode, mentre un analyzer basato su\nwhitespacesi limita a spezzare sugli spazi, risultando semplice ma spesso troppo\ngrezzo in presenza di punteggiatura soprattutto; esistono altre varianti come il\nsimple, stop analyzer... ma le due precedenti sono le più famose.\nSebbene la tokenizzazione sembri un processo semplice, in realtà è particolar-\nmente complesso in relazione alle esigenze: quando si parla di tokenizer orientati\nal retrieval, tipicamente si sfruttano set di regole semplici, come quelle Unicode\no Whitespace discusse precedentemente, diverso è nel caso di modelli di NLP\ncome Transformer, che discuteremo successivamente. In questi casi, sono diffusi\nmetodi di tokenizzazione subword comeByte Pair Encoding(BPE) eWord-\nPiece. Senza scendere eccessivamente nel dettaglio, il che non è lo scopo di\nquest’introduzione, i suddetti metodi spezzano le parole in sotto - unità per\ngestire vocaboli rari, addirittura ’inventati’ e la variabilità morfologica nel caso\ngenerico, migliorando la generalizzazione del modello alla variabilità stessa del\ntesto che può trovarsi a dover processare. Tuttavia, questi schemi sono spesso\npoco adatti alla ricerca full-text classica: introducono un disallineamento tra\nl’intuizione dell’utente (che formula query in parole) e l’unità effettivamente in-\ndicizzata (subword), riducono l’interpretabilità dihighlighte snippet e, soprat-\ntutto, aumentano il numero di token per documento, con un impatto diretto\nsulla dimensione dell’indice e sulla lunghezza delle posting lists. È interessante\nnotare come le conseguenze siano quasi complementari nei Transformer e nei\nsistemi di retrieval tradizionali. Nei modelli Transformer, la segmentazione in\nsubword (ad esempio BPE) è vantaggiosa perché riduce drasticamente la dimen-\nsione del vocabolario: invece di memorizzare un token distinto per ogni parola\nrara o morfologicamente complessa (es.fortissimo), il modello può comporla\na partire da unità più frequenti (es. una radiceforte un suffisso##issimo,\nesempio esclusivamente intuitivo), ottimizzando la copertura lessicale a parità\ndi grandezza del vocabolario. Nel caso di uninverted index, invece, l’obiettivo\nè diverso: si desidera che l’unità indicizzata coincida il più possibile con l’unità\ndi ricerca dell’utente e che i termini sianostabili. Segmentare sistematicamente\nin subword tende ad avere l’effetto opposto: aumenta il numero di termini in-\ndicizzati, rende le posting lists più dense (perché subword frequenti compaiono\nin molte parole diverse) e può peggiorare la precisione del matching, dato che\nframmenti condivisi da parole non correlate generano corrispondenze rumorose\ndall’intersezione. Mentre, quindi, nei Transformer la scomposizione subword è\nuna strategia ottima per comprimere il vocabolario e generalizzare meglio, nel\nretrieval full-text classico rischia di trasformare il problema in un matching su\n3\nunità troppo atomiche, incentivando un qualcosa che in letterature si indica co-\nmeVocabulary Mismatch Problem: la bontà dei risultati restituiti da un motore\ndi ricerca dipende da come l’utente formula la query, o meglio, da quanto la\ndistribuzione lessicale della stessa sia allineata rispetto a quella dei documenti,\nper questo si cerca di mantenere il più possibilmente la tokenizzazione allineata\ncon quello che viene naturalmente espresso dall’utente e la segmentazione in\nsotto - unità crea più problemi che vantaggi; ciò non vuol dire che tu non possa\nagire sulla query, ad esempio tramite riscritture, vuole semplicemente dire che\ntokenizzare come fanno i Transformer non è una grande idea.\n2.4 Metriche nella Full-Text Search\nI documenti ottenuti dalle posting lists vengono, poi, ordinati sulla base di de-\ngli score derivanti dall’importanza che i token della query assumono all’interno\ndel documento; come il retrieval si basa su sovrapposizione lessicale, anche gli\nscore riprendono questo concetto per costruire il cosiddettoranking. Due delle\nmetriche più utilizzate in questo contesto sonoTF-IDFeBM25, entrambe fon-\ndate sull’idea che l’informatività di un termine dipende mutuamente dalla sua\nimportanza locale e globale: se è un termine molto diffuso tra tutti i documen-\nti, allora non è una discriminante, altrimenti se è molto diffuso all’interno del\nsingolo documento e poco/mediamente negli altri è una discriminante.\nTF-IDFTF-IDF (Term Frequency – Inverse Document Frequency) assegna\nun peso ai termini della query in funzione della: frequenza del termine nel docu-\nmento (TF), catturando quanto quel termine è rappresentativo dello stesso, e la\nfrequenza del termine nell’intera collezione (globale) (IDF), che riduce l’impor-\ntanza di parole eccessivamente comuni in corpus soprattutto tematici, si pensi\nad un corpora di documenti medici ad esempio. Concettualmente, il punteggio\ndi un documento rispetto a una query cresce quando i termini della query com-\npaiono spesso nel documento, ma soprattutto quando tali termini sono rari nel\ncorpus, in maniera totalmente allineata con quanto accennato prima. TF-IDF\nè semplice, efficace e costituisce una base naturale per il ranking lessicale, pur\nsoffrendo del fatto che la componente TF può crescere in modo eccessivo per\ndocumenti molto lunghi o ripetitivi: più un documento è lungo e maggiore sarà,\nmediamente, la TF dei termini; questo crea un bias negli score attribuiti, che\ntendono a preferire sistematicamente documenti più lunghi e non più rilevanti.\nBM25Aggiunge a TF-IDF una normalizzazione non lineare alla Term Fre-\nquency: meno dura di quella diretta rispetto alla lunghezza del documento, ma\ncon gli stessi vantaggi. L’idea è sempre la stessa: un documento che ha un buon\ncompromesso tra frequenza locale e globale dei termini della query è un docu-\nmento rilevante. La normalizzazione per lunghezza vuole semplicemente evitare\nche i documenti più lunghi vengano sempre restituiti e ridurre il bias che discu-\ntevamo precedentemente; questa, come suddetto, viene applicata al termine TF\ndella TF - IDF, essendo che l’IDF non dipende dalla lunghezza del documento.\n4\nCon il Chunking, discusso nei capitoli successivi, si vede un’indicizzazione per\n’porzioni locali’ del documento: generalmente questo ti permette anche di avere\nuna normalizzazione strutturale per TF-IDF senza ricorrere a BM25, tuttavia\nquest’ultima rimane lo standard de facto essendo che, dipendentemente dalla\nstrategia, potresti non avere tutti chunk di dimensione identica.\n2.5 Problemi Tipici del Full-Text Retrieval\nAbbiamo già discusso ilVocabulary Mismatch Probleme tutte le criticità del-\nla Full-Text search si riconducono a questo; possiamo studiarlo anche in modo\ndiverso, in funzione prevalentemente della distribuzione lessicale. Infatti, vale\nl’osservazione complementare: una maggiore sovrapposizione lessicale non im-\nplica necessariamente, in generale, una maggiore sovrapposizione semantica; la\npresenza delle stesse parole in query e documento non garantisce che stiano fa-\ncendo riferimento allo stesso significato. Questo fenomeno è strettamente legato\nallapolisemiadel dominio, per cui un termine può assumere accezioni diverse\na seconda del contesto, aumentando l’ambiguità e degradando la precisione del\nretrieval basato su sovrapposizione lessicale; al contrario, in domini piùmo-\nnosemici, si osserva spesso che i termini sono più stabili e meno ambigui, di\nconseguenza il matching lessicale risulta più affidabile semanticamente.\nGià da qui si capisce un concetto su cui torneremo più volte nel RAG: il re-\ntrieval deve essere progettato, tarato e ottimizzato in funzione del dominio di\nappartenenza, a meno di accontentarsi di prestazioni subottimali. Quando è\npossibile stimare con buona accuratezza la distribuzione lessicale del dominio,\nsi possono spesso ottenere prestazioni elevate anche con approcci relativamente\nsemplici, evitando di introdurre complessità non necessaria oltre la ricerca full-\ntext tradizionale. Questa dipendenza dal dominio diventa ancora più evidente\nquando si adotta lahybrid search: combinare segnali lessicali e densi richiede\ndi pesare opportunamente i rispettivi score e tali pesi non sono universali ma\nemergono soprattutto dalle caratteristiche del corpus e delle query attese; so-\nstanzialmente quindi, la scelta tra full-text, dense retrieval o una combinazione\ndei due non è fortemente guidata dal dominio e dalla distribuzione delle query.\n2.6 Dense retrieval\nIldense retrievalè un paradigma di ricerca in cui documenti e query vengono\nrappresentati vettorialmente in uno spazio continuo ed il retrieval avviene trami-\nte meccanismi di vicinanza trai vettori definenti query e documenti; si sfruttano\nmisure di similarità, principalmente di tipo coseno: metrica che quantifica l’an-\ngolo compreso tra due vettori, più è piccolo e più i vettori che tracci dall’origine\ndel piano sono coincidenti e, quindi, i punti nello spazio rappresentanti query\no documenti coincidono nella stessa posizione e sono simili. Operativamente, il\nsistema calcola tale vettore (embedding) per la query, interroga un indice vetto-\nriale e restituisce ikdocumenti con massima similarità. Il vantaggio principale\n5\nrispetto al full-text classico è la capacità di catturare corrispondenzesemanti-\nche: un documento può risultare rilevante anche senza condividere esattamente\nle stesse parole della query, riducendo strutturalmente ilVocabulary Mismatch\nProblem, difatti la rappresentazione densa si concentra proprio sul catturare il\nsignificato del testo e della query, evitando di modellarlo in maniera ’latente’\nrispetto alla sovrapposizione lessicale.\n2.7 Embeddings\nQuando si parla diembeddingsi fa riferimento ai suddetti vettori; più formal-\nmente: un embedding è una funzione che mappa un oggetto discreto (parola,\nfrase, documento) in un vettore realee∈R d, con l’obiettivo proiettare in uno\nspazio dove la geometria dello stesso rifletta relazioni utili: punti vicini dovreb-\nbero corrispondere a contenuti semanticamente simili, viceversa lontani. In que-\nst’ottica, la semantica emerge proprio rispetto alla disposizione dei punti nello\nspazio: concetti simili tendono a formare regioni dense ocluster, mentre con-\ncetti distanti si collocano in aree che devono essere il più possibilmente separate\ndello spazio; la qualità di questo dipende non solo dal modello, ma soprattutto\ndal criterio di addestramento. Nei sistemi moderni di retrieval si usano spesso\nobiettivi di addestramentocontrastivi: l’idea è “modellare” lo spazio vettoriale\nin modo che una query risulti vicina ai documenti veramente rilevanti e lontana\nda quelli irrilevanti. Nel concreto, dato un esempio positivo(q, d+)e un insieme\ndi negativi{d −\n1 , . . . , d−\nm}, si ottengono gli embeddingq,d+ ed −\ni e si ottimizza\nuna loss che aumentas(q,d+)e riduces(q,d −\ni ), doves(·,·)è la misura di simi-\nlarità adottata. Intuitivamente, la loss forza la query a “scegliere” il documento\ncorretto tra molte alternative, o meglio, gli stiamo direttamente indicando cosa\nper noi è semanticamente correlato da cosa non lo sia.\nNelle discussioni successive richiameremo esplicitamente la relazione traani-\nsotropiae losscontrastive, quindi non è utile anticipare ora tutti i dettagli;\nè però importante evidenziare un punto chiave: il solo pre-training raramente\nè sufficiente per ottenere embedding adatti al retrieval semantico. Modelli co-\nme BERT, ad esempio, apprendono rappresentazioni vettoriali perché vengono\naddestrati a modellare le relazioni grammaticali, semantiche, posizionali... nel\nlinguaggio tramite cosiddetti auxiliary task come ilmasked language modeling.\nTuttavia, il fatto che un modello sappia rappresentare bene il linguaggio non\nimplica automaticamente che gli embeddings prodotti siano anche geometri-\ncamente ben strutturati secondo le necessità precedenti: detta informalmente,\nil pre-training insegna al modello a rappresentare il significato contestuale del\nlinguaggio, ovvero che la stessa parola può assumere una semantica diversa a\nseconda del contesto (ad esempioapplecome frutto oppure come azienda), in-\nsegnando indirettamente anche il significato delle parole, tuttavia questo non\nimplica automaticamente che lo spazio degli embeddings venga ben separato\ncome ci aspettiamo e come vuole un retrieval denso; difatti, la nozione di “dif-\nferenza” che aiuta a separare lo spazio è un qualcosa di relativo alla fase di\npost-training e strettamente collegato alla loss che utilizzi. Questo passaggio è\n6\ncruciale per rendere efficace l’interrogazione tramite nearest neighbors e chiari-\nsce perché un buon language model non coincida necessariamente con un buon\nmodello di retrieval; d’altronde, le contrastive losses corrispondono spesso e\nvolentieri a fine tunings successivi, dove come obiettivo ulteriore hai anche quel-\nlo di modellare sfumature di significato complesse nel tuo dominio e che un\npre-training generalista non ti consente.\n2.8 Transformer\nNon l’abbiamo detto direttamente, però leggendo un po’ tra le righe: gli embed-\ndings, ad oggi, vengono tutti ricavati ma modelli di Deep Learning Transformer\nbased; questo non necessariamente implica che si sia sempre fatto così, anzi: ap-\nprocci come Word2Vec o GloVe non si basano su Transformer, ma più sull’idea\ndimmi con chi vai e ti dirò chi sei, modellando in diversi modi ul significa-\nto contestuale. Il grosso ’boost’ a livello di bontà degli embeddings prodotti\nstoricamente è avvenuto proprio con le architetture Transformer: l’attention\npermette di risolvere il problema dei suddetti approcci tradizionali di ’staticità’\ndell’embedding prodotto; GloVe, come Word2Vec, ad inferenza forniscono ma-\ntrici statiche tali per cui puoi ottenere un embedding come prodotto vettoriale\ntra una rappresentazione OneHotEncoded della singola parola e la matrice di\nembedding stessa, sostanzialmente un lookup, dove l’embedding di frase coin-\ncideva spesso con una media aritmetica di quelli dei token singoli; il problema\nè che, qui, hai un embedding singolo per ogni parola ed indipendentemente\nda quello che sta nell’intorno della stessa: non c’è significato contestuale, l’at-\ntention, invece, parte da un embedding statico e lo modifica in funziona degli\nembedding di tutti gli altri token vicini, producendone una rappresentazione\nenhanced.\n2.8.1 BERT\nDiscutere di BERT è fondamentale per comprendere meglio i Bi e Cross-Encoder\nche discuteremo nei capitoli successivi, essendo che quest’ultimi, sebbene più so-\nfisticati, si basano comunque sull’architettura Encoder only introdotta proprio\nda BERT.Bidirectional Encoder Representations from Transformersè un mo-\ndello basato sull’omonima architettura Transformer che utilizza esclusivamente\nlostackdiencoder, elabora una sequenza testuale tramiteself-attentione\nproduce, per ogni token, una rappresentazione vettoriale contestuale. La carat-\nteristica distintiva di BERT è la naturabidirezionale: ogni token può integrare\ninformazione proveniente sia dal contesto a sinistra sia da quello a destra, poi-\nché durante il training non viene applicata unacausal maskcome avviene nei\nmodelli generativi come GPT; BERT, appunto, non è auto - regressivo ed il\ngrosso vantaggio nell’NLP dell’utilizzo di architetture di questo tipo sta, pro-\nprio, nell’interazione tra ogni token. Nel caso di modelli generativi questo non\navviene: c’è il vincolo di causalità, il tokenT+ 1viene generato sulla base del\ntokenTprodotto a passo precedente e, di conseguenza, sfruttare un meccanismo\ndi attention che permetta di far interagire i token precedenti con quelli futuri\n7\nperde di senso semplicemente perché non li conosci; ci sono, poi, considerazioni\nche si potrebbero fare a livello di ’peaking’ nel training, ma intuitivamente il\nconcetto rimane solido anche così. La bidirezionalità non dipende, quindi, dalla\nself-attention “in sé”, ma dal fatto che non venga limitata a vedere solo il passato\ne questo è coerente con l’obiettivo dicomprensionepiù che generazione.\nIl pre-training originale di BERT è formulato comemultitask learninge com-\nbina principalmente due compiti:Masked Language Modeling (MLM)e\nNext Sentence Prediction (NSP). Nel task MLM si seleziona tipicamente\ncirca il15%dei token e si applica la regola80/10/10: nell’80%dei casi il to-\nken viene sostituito con[MASK], nel10%con un token casuale e nel restante\n10%rimane invariato. La sequenza così perturbata viene data in input all’en-\ncoder e il modello deve ricostruire il token originale nelle posizioni selezionate;\nla loss (cross-entropy) viene calcolatasolosu tali posizioni. Complessivamente,\nl’addestramento tramite MLM induce BERT a produrre embedding abbastanza\nricchi: per predire correttamente i token mascherati, il modello deve sfrutta-\nre dipendenze sintattiche e semantiche presenti in tutta la frase, cosa che la\nself-attention permette in modo naturale; questo rende BERT particolarmente\nefficace in compiti di language understanding, come classificazione del testo e\nNamed Entity Recognition (NER).\nIl task NSP, introdotto nella versione originale, mira a modellare relazioni tra\nfrasi: dato un input del tipo[CLS] frase_A [SEP] frase_B [SEP], il model-\nlo predice se la seconda frase segue effettivamente la prima nel testo sorgente.\nLa decisione viene presa a partire dall’embedding del token speciale[CLS], che\nnon ha significato lessicale ma, grazie alla self-attention, può raccogliere infor-\nmazione dall’intera sequenza; il ’come’ faccia questo è abbastanza semplice: è\nun token neutro (inventato) che non compare nei testi di addestramento, viene\nposto all’inizio della sequenza in modo, proprio, che possa assorbire la totalità\ndel significato di tutti i token successivi, quindi l’interezza della frase, grazie\nall’attention stessa. Per questo motivo,[CLS]tende a diventare una rappresen-\ntazione“globale” utileincompitialivellodifrase. Insuccessiveimplementazioni,\ncome RoBERTa, il task NSP è stato rimosso: l’idea di fondo è che ottenere un\nembedding[CLS]general purposegià ben calibrato per molti compiti sia un\nobiettivo troppo ambizioso per il solo pre-training, e che l’utilità pratica emer-\nga soprattutto dopo unfine-tuningsupervisionato sul task specifico. BERT\nè concepito come modellogeneral-purposeper la comprensione del linguaggio,\ntuttavia i grossi vantaggi vengono quando si specializza l’utilizzo ad un deter-\nminato task: si noti che per specializzare non si intende per forza qualcosa di\nrelativo ad un dato dominio, come i fine tunings per sfumature di singificato,\nma proprio un compito ’operativo’. Ci sono diversi esempi:\n•task disentence classification(ad esempio sentiment analysis), dove si\naggiunge una semplice testa di classificazione che prende in ingresso l’em-\nbedding finale del token speciale[CLS]. Poiché la funzione di perdita su-\npervisiona direttamente questa rappresentazione, il training spinge[CLS]\n8\na condensare l’informazione dell’intera sequenza in un vettore unico, utile\nper decisioni “globali” a livello di frase\n•Nei tasktoken-levelcome laNamed Entity Recognition, invece, l’obietti-\nvo non è assegnare un’etichetta al testo nel suo complesso, ma produrre\nuna predizione per ciascuna posizione. Si applica quindi una testa di\nclassificazione a ogni embedding contestuale in uscita dall’encoder, otte-\nnendo una sequenza di etichette (ad esempioB-PER,I-ORG,O, ...). In\nquesto scenario, la qualità del modello dipende soprattutto dalla capacità\ndi rappresentare accuratamente il contestolocaledi ogni token, più che\ndall’esistenza di un singolo vettore riassuntivo.\n•Nel retrieval invece, BERT viene adattato con un fine-tuning che compren-\nde le loss contrastive suddette; architetture come SBERT introducono que-\nsta tendenza esplicita ad ottimizzare le rappresentazioni del testo affinché\nrisultino confrontabili con una metrica di similarità (coseno o prodotto\nscalare). L’embedding testuale, ottenuto tipicamente da[CLS]oppure\ntramite pooling sulle posizioni\nPer concludere, a mio avviso, è utile dare una panoramica generale di ciò che\navviene “all’interno” di BERT durante la costruzione degli embeddings. In in-\ngresso, ciascun token viene trasformato in un embedding iniziale tramite una\nmatriceW, anch’essa appresa durante il training: per ciascuno, viene portata\nin conto l’informazione posizionale tramite una strategia di positional encoding,\ndipendente dalla versione di BERT che si considera; evitiamo di discutere ul-\nteriormente l’encoding posizionale poiché è la cosa che interessa di meno in\nrealtà. Il meccanismo diSelf-Attentionopera proprio su tali embeddings ini-\nziali: operazione chiave con cui viene costruito per token una rappresentazione\ncontestuale integrando informazione dagli altri token della sequenza tramite,\nsostanzialmente, combinazioni lineari opportunamente pesate. Per ciascun em-\nbedding si calcolano tre proiezioni lineari:Query(Q),Key(K) eValue(V).\nL’attenzionesiottieneconfrontandoogniquerycontuttelekeytramiteprodotto\nscalare, applicando poi una softmax per ottenere pesi normalizzati:\nAttn(Q,K,V) = softmax\n\u0012 QK⊤\n√dk\n\u0013\nV.\nIl risultato è una combinazione pesata deivalue: ogni token “decide” quanta\ninformazione assorbire dagli altri, assegnando pesi maggiori alle posizioni più\nrilevanti per il contesto corrente. Intuitivamente, laQuery(Q) è “la domanda”\nche ogni token pone al resto della sequenza: per il tokeni-esimo,qi chiede “quali\naltri token sono rilevanti per me?”. LaKey(K) è invece la “chiave” con cui\nogni token si descrive: il vettorekj del tokenjindica “in che modo io potrei\nessere rilevante per gli altri”. Infine, laValue(V) è il contenuto informativo\nche viene realmente aggregato: il vettorevj contiene l’informazione che, pesata\ndall’attenzione, contribuisce a costruire l’output; la rappresentazione enhanced\ncontestualmente dell’embedding di ogni token viene, infatti, da una combina-\nzione lineare dei pesi di attention, dati dalla softmax, rispetto ai valuesV. La\n9\nMulti-HeadAttentionripetelostessocalcolosupiùtesteinparalleloconpro-\niezioni differenti: teste diverse possono specializzarsi nel catturare segnali anche\ndiversi dalla semplice semantica, ad esempio dipendenze sintattiche o anche po-\nsizionali più complesse. Le uscite delle teste vengono concatenate e riproiettate,\nl’intero blocco è accompagnato daresidual connectionselayer normalization,\nutili a stabilizzare l’ottimizzazione e a preservare il flusso informativo tra strati.\nAccanto all’attenzione, ogni layer include unaFeed-Forward Neural Net-\nwork(FFNN)position-wise, applicata indipendentemente a ciascun token: è ti-\npicamente una MLP a due strati con non linearità che espande la dimensionalità\nnel layer hidden e, poi, la riporta a quella originale:\nFFN(x) =W 2 σ(W1x+b 1) +b 2.\nSe l’attenzione realizza ilmixingdell’informazione tra token, la FFNN svol-\nge la trasformazione non lineare chemodellae riorganizza l’informazione per\nciascun token, aumentando la capacità espressiva del modello e rendendo le\nrappresentazioni via via più complete ed informative. Anche in questo caso, re-\nsidui e normalizzazione facilitano la composizione di molti strati encoder senza\ndegradare incorrere nei classici problemi del training nelle architetture Deep.\n2.9 Hybrid Search\nAlla luce di quanto discusso, è facile capire l’idea dihybrid search: combinre\nsegnali lessicali (full-text) e segnali semantici (dense retrieval) per sfruttare la\ncomplementarità trai problemi di uno ed i vantaggi dell’altro. L’idea, appunto,\nè che i modelli lessicali (ad esempio BM25) eccellono quando la query contiene\ntermini discriminanti e quando l’utente richiede corrispondenze esatte, mentre\nil dense retrieval è più efficace nel gestire sinonimia, parafrasi e query formulate\nin modo diverso dal testo del documento. Operativamente infatti, un sistema\nibrido può recuperare candidati da entrambi i canali e poi fonderli, calcolando\nun punteggio combinato; in entrambi i casi, la fusione mira ad ottenere un ran-\nking che porti strutturalmente conto sia dello score full-text che di quello denso,\nrestituendo i documenti migliori per compromesso.\nUn approccio comune è recuperare due liste top-k(lessicale e vettoriale) e\nunificarle con:\n•Reciprocal Rank Fusion (RRF): invece di combinare direttamente gli\nscore, si combinano iranghi. Se un documentodcompare in posizione\nr1(d)nella lista lessicale e in posizioner2(d)nella lista densa, il punteggio\nfinale può essere definito come\nRRF(d) =\nX\ni∈{1,2}\n1\nk0 +r i(d) ,\ndovek 0 è una costante che smorza l’effetto delle primissime posizioni.\nIn questo modo vengono premiati i documenti che risultano “buoni” in\n10\nentrambi i ranking full-text e denso, ottenendo una fusione abbastanza\nrobusta soprattutto al caso in cui gli score singoli delle liste non sono\nconfrontabili; RRFèpotente, proprioperchénondevipreoccupartiapriori\ndinormalizzaregliscoreinunascalacomuneequestosicapiscebeneanche\nguardando la combinazione convessa, dove avresti un termine molto più\npreponderante dell’altro e che, indipendentemente dal peso, rischierebbe\ndi dominare nella definizione del ranking\n•Combinazione convessa (weighted sum): si normalizzano gli score\ndei due canali e si calcola un punteggio aggregato tramite una somma\npesata:\ns(d) =α s lex(d) + (1−α)s dense(d), α∈[0,1].\nIl parametroαcontrolla il compromesso tra corrispondenza lessicale e\naffinità semantica. Questa strategia è molto semplice ed efficace, tuttavia\nrichiede attenzione alla normalizzazione e scelta diα\nIn generale, l’hybrid search è particolarmente utile nei casi in cui una delle due\ncomponenti fallisce in modo sistematico: ad esempio, una query con termini\nmoltospecifici(codici, nomipropri, acronimi)tendeafavorirelaricercalessicale,\nmentre query “concettuali” o parafrasate beneficiano del canale denso; la fusione\nmira quindi a ridurre la probabilità dizero-hite a rendere mediamente migliore\nla qualità del retrieval.\n2.10 Approximate Nearest Neighbor (ANN) e HNSW\nLa ricerca deik-Nearest Neighbors(KNN) è l’operazione fondamentale alla base\ndeldense retrieval: datounembeddingdiqueryqeuninsiemedichunk/document\nembeddings{d 1, . . . ,dN }, l’obiettivo è trovare ikvettori “più vicini” secondo\nuna data metrica, tipicamente similarità coseno; l’assunzione chiave del retrie-\nval denso, come suddetto, è geometrica: se lo spazio degli embedding è ben\nstrutturato, la prossimità tra punti nello spazio approssima la rilevanza rispetto\nall’information needdella query, quindi i documenti più vicini aqsono buo-\nni candidati da usare come contesto in un processo di retrieval. Tuttavia, il\nKNNesattodiventa rapidamente proibitivo su corpora molto estesi: una ricer-\nca brute-force richiede di confrontareqcon tutti iNvettori, con costo lineare\ninN, oltre al costo di calcolo della similarità rispetto alla dimensioneddegli\nembeddings; sebbene spesso trascurato può diventare rilevante.\nIn scenari reali di retrieval (milioni o miliardi di vettori, vincoli di latenza real-\ntime), questa scansione completa è assolutamente ingestibile: serve quindi un\nindicecheconsentadilimitaredrasticamenteilnumerodiconfronti, mantenendo\ncomunque alta larecall. Gli algoritmi diApproximate Nearest Neighbor(ANN)\nnascono proprio per rispondere a quest’esigenza: accettano un’approssimazione\ndei vicini, abbandonando l’idea di risolvere il problema all’ottimo, ma restitui-\nre un parco di candidatiquasiottimali in cambio di una riduzione importante\ndelle risorse di calcolo necessarie alla procedura. Molti approcci si basano sulla\n11\ncostruzione e popolazione di una struttura dati che organizzi i vettori in modo\nda esplorare, a query-time, solo una piccola porzione dello spazio, evitando la\nscansione completa ed ottimizzando il trade-off esplicito tra accuratezza e pre-\nstazioni classico delle approssimazioni.\nTra i metodi di ANN più diffusi,HNSW(Hierarchical Navigable Small World)\nè uno standard de facto per la dense search: costruisce un grafo di prossimità\nin cui ogni nodo è un vettore e gli archi collegano vettori “vicini”; la struttura è\ngerarchica: ai livelli superiori il grafo è più sparso e serve per una navigazione\nad alto livello verso regioni promettenti per la query, mentre ai livelli inferiori\nè più denso e consente di rendere la ricerca più precisa ragionando localmente;\nsi parla spesso di alternanza tracoarseefine search. Per essere proprio precisi:\n“più sparso” significa che i livelli alti contengono molti meno nodi, difatti non\ntutti i vettori compaiono a tutti i livelli, e l’assegnazione di un nodo ad un li-\nvello è probabilistica, con un nodo che può comparire anche in più livelli. La\ngerarchia funge da riassunto ad alto livello dello spazio nel suo complesso, utile\nper fare, intuitivamente, “salti lunghi” verso la regione corretta, mentre i livelli\nbassi, più popolati, consentono di rispondere alla query più precisamente. A\nquery-time, si parte da unentry pointnei livelli alti e si procede con una visita\ngreedy: si esplora un insieme limitato di candidati, si aggiorna iterativamente\nil miglior vicino e si scende di livello man mano che ci si avvicina alla regione\npiù promettente, fino al livello base dove si selezionano i top-kfinali. Questo\nschema ottiene tipicamente alta recall con latenze molto basse, rendendo HNSW\nparticolarmente adatto alle condizioni applicative nei casi reali di dense search.\nNon stupisce, infatti, come HNSW sia estremamente usato anche nelle SDK for-\nnite dai maggiori cloud providers: ad esempio, Azure AI Search espone HNSW\ncome algoritmo ANN di riferimento per la vector search, proprio perché offre il\nmigliore trade-off.\n2.11 Community Detection e Clustering Semantico\nClusteringeCommunity Detectionsono concetti molto simili, ma non equi-\nvalenti. Il clustering, nel senso tradizionale, parte da una rappresentazione di\npunti in uno spazio e cerca di raggrupparli in insiemi: con algoritmi come K-\nmeans Clustering, DBSCAN... la nozione di “vicinanza” dipende interamente\ndalla rappresentazione e dalla metrica adottata, o meglio, da quello che abbiamo\nchiamato ’geometria’ dello spazio discutendo degli embeddings; quando si parla\ndiclustering semantico, infatti, non ci sono sostaziali differenze dal clustering\ntradizionale, se non lo spazio nel quale disponi punti: se tale è uno spazio di em-\nbeddings, clusterizzare significa, quindi, identificare gruppi di chunk/documenti\nche risultanosemanticamente allineati, ovvero che occupano regioni vicine dello\nspazio vettoriale. Da qui è facile capire come la qualità dei cluster dipenda cri-\nticamente dal modello di embedding e dal modo in cui questo struttura lo spazio.\nLaCommunity Detection, invece, assume che i dati siano già rappresentati co-\nme grafo e mira a individuare insiemi di nodidensamente connessial loro\n12\ninterno edebolmente connessiverso l’esterno; la differenza con il clustering\nnon sta, infatti, nel concetto di ’gruppo’, quanto più sulla struttura dati alla\nbase, ovvero la scelta di modellare possibili interazioni trai nodi o meno. Le\ncommunity vengono spesso definite come “moduli” del grafo e sono oggetto di\nun’ampissimo studio in letteratura e soprattutto nel retrieval, ai sensi del fatto\nche il Web è rappresentabile come un grafo dove i nodi sono pagine e gli ar-\nchi collegamenti tra di esse, oltre che l’estensivo utilizzo in domini come social\nnetwork, recommender systems... [Fortunato, 2010].\n2.11.1 Louvain e Leiden\nUn algoritmo molto noto èLouvain, che mira a massimizzare lamodularità\n(denso internamente, poco esternamente) tramite una procedura greedy in due\nfasi ripetute iterativamente [Blondel et al., 2008]. Nella prima fase (local mo-\nving), si parte da una partizione banale in cui ogni nodo è una community e,\nuno alla volta, ciascun nodo viene spostato nella community di un suo vicino\nse tale spostamento produce un incremento della modularità; il processo con-\ntinua finché non si ottengono ulteriori miglioramenti locali. Nella seconda fase\n(aggregation), ogni community trovata viene contratta in unsuper-nodoe i pesi\ndegli archi tra super-nodi corrispondono alla somma dei pesi degli archi tra le\nrispettive community originali (gli archi interni diventano self-loop). Il risultato\nè un grafo più piccolo su cui si ripete lo stesso schema, ottenendo una decom-\nposizione gerarchica che tende a produrre comunità via via più macroscopiche\nad ogni livello.\nNelle pipeline di RAG che presenteremo successivamente, facciamo riferimen-\nto soprattutto aLeiden: nasce come miglioramento di Louvain, mantenendo\nsempre l’idea dell’ottimizzazione di una funzione obiettivo (ancora modularità\ntipicamente), introducendo, però, un passo direfinementche corregge una cri-\nticità ben nota di Louvain, ossia la possibilità di ottenere comunità non ben\nconnesse; se guardi interamente alle communities trovate da Louvain, potre-\nsti vedere che queste si ’clusterizzano’: comunità formate da gruppi di nodi\ndebolmente connessi tra loro o, addirittura, non connessi, il che va contro l’as-\nsunto fondamentale di massimizzazione della densità interna delle communities,\ntendenti ad un sottografo fortemente connesso nel caso ottimo infatti. Leiden,\nquindi, alterna le stesse fasi dilocal movingdei nodi per migliorare l’obiettivo ed\naggregationin super-nodi, aggiungendo unrefinementche garantisce comunità\nmeglio connesse e, ripetendo il processo in modo gerarchico, fornisce migliori\ngaranzie sulla connettività interna delle comunità rispetto a Louvain, nonché ri-\nsulti, spesso, anche più efficiente computazionalmente [Traag et al., 2019]. Per\nessere più precisi, la fase di refinement interviene all’interno di ogni community\nprodotta dal local moving: invece di accettare la community come blocco uni-\nco, Leiden guarda internamente e ’spezza’ componenti o sottoinsiemi che non\nrisultano sufficientemente coesi; i nodi così ottenuti possono essere riallocati in\nsotto-community “figlie” derivanti dalle precedenti, in modo da garantire che\nciascun gruppo risultante sia ben connesso. Solo dopo questa pulizia strutturale\n13\nsi procede alla contrazione in super-nodi: così l’aggregazione avviene su comu-\nnità che rispettano almeno un certo livello di connettività interna, evitando che\nsotto-gruppi mal connessi vengano trascinati ai livelli successivi della gerarchia.\n2.12 Communities nel Retrieval\nDal punto di vista del retrieval, communities e cluster possono essere impiegati\nin modo quasi sovrapposto: entrambi definisconoregionisemanticamente coe-\nrenti su cui instradare una query prima di fare un drill-down verso le componenti\nche le costituiscono (chunk testuali, ad esempio), ricordando che l’information\nneed risiede, proprio, in quest’ultime.\nL’uso più comune, infatti, è quello di recuperocoarse-to-fine: si restringe il\nretrieval ai soli chunk contenuti nelle top communities per rilevanza rispetto\nalla query, migliorando precisione e riducendo rumore; su questo meccanismo\ntorneremo nei capitoli successivi sulle Architetture RAG. Le communities fun-\ngono, quindi, da “contenitore tematico” per ridurre lo spazio di ricerca e rendere\npiù facile collegare informazione distribuita nel singolo documento o tra docu-\nmenti diversi; anche quando la community detection è tecnicamente diversa dal\nclustering, nel retrieval la si intende spesso come un clusteringstrutturato: inve-\nce di raggruppare punti solo per vicinanza nello spazio vettoriale, si raggruppano\nnodi in base a come sono connessi, ottenendo comunque insiemi coerenti rispetto\nalle proprietà alla base degli archi che modelli (semantiche, posizionali...).\n2.13 Language Models Generativi\nConLanguage Model(LM) ci si riferisce a modelli basati sull’architettura Tra-\nsformer, addestrati a stimare distribuzioni di probabilità su un vocabolario di\n’token’ facenti riferimento a testo o, come abbiamo visto, sub-words da concate-\nnare; un LM può essere generativo o meno, BERT è un modello non generativo\nad esempio. Come richiamato nella sezione sui Transformer, l’architettura di\nriferimento è quella introdotta in [Vaswani et al., 2017], basata su blocchi di\nself-attention, feed-forward position-wise, residual connections e layer norma-\nlization: sebbene i moderni modelli generativi GPT-like sianodecoder-only,\nl’architettura nel complesso non è così tanto differente dagliencoder-onlypre-\nsentati con BERT, se non per l’utilizzo atteso ad inferenza del modello stesso;\nl’obiettivo, adesso, è di generazione di token, difatti se in BERT la backbone è\nfondamentale e la testa viene spesso cambiata all’occorrenza, qui anche in fine\ntunings successivi il modulo che utilizzi per generare token rimane identicamen-\nte lo stesso. Un LM, quindi, è riassumibile proprio guardando la distribuzione di\nprobabilità che apprende sul vocabolario: dato un contestox<t, il modello pro-\nduce una distribuzionep(xt |x <t)sui token possibili e, nel caso auto-regressivo,\nla probabilità dell’intera sequenza fattorizza comeQ\nt p(xt |x <t). Il modello\nnon“memorizza” risposte, ma apprende una funzione che associa una distribu-\nzione sul prossimo token da produrre e data, ovviamente, una certa sequenza in\ninput; ciò che ti aspetti il modello impari, quindi relazioni sintattiche, gramma-\n14\nticali, posizionali... è lo stesso che in LM non generativi.\nLa generazione, come suddetto, avviene nellatesta(head) del modello: così\nsi intende il modulo di output che trasforma la rappresentazione nascosta finale\nprodotta dalla backbone Transformer in una predizione nel dominio del task;\nnel caso dei LM generativi, laLM headè tipicamente una proiezione lineare\nWout (spesso con bias) che mappa lo stato nascostoht ∈R d in logitsz t ∈R |V|\nsul vocabolario, seguita da una softmax:\nzt =W outht +b, p(x t |x <t) = softmax(zt).\nQuesta testa, quindi, implementa esplicitamente l’operazione di mappatura ver-\nso il vocabolario e definisce la distribuzione che il modello utilizza ad inferenza.\nEsistono diversi modi di sfruttare la distribuzione appresa: scegliere sempre il\ntoken con maggiore probabilità (la softmax somma ad1), piuttosto che intro-\ndurre parametri, come latemperaturaT, che evitino questo bias strutturale di\nscegliere sempre il token migliore, aumentando l’eterogeneità delle risposte.\nL’attention funziona similmente a BERT, ma stavolta applica unacausal ma-\nskvincolando ogni posizioneta “vedere” solo i token precedenti< t; in questo\nmodo, la generazione èauto-regressiva: avviene un token alla volta, cam-\npionando. Durante il training, questo obiettivo viene implementato in modo\ndiretto tramitenext-token prediction: dato un testox1:T, si costruiscono coppie\ninput/targetshiftandola sequenza di una posizione, cioè il modello riceve in\ninputx 1:T−1 e deve predirex2:T (equivalentemente, per ogni posizionetpredice\nxt datox <t), ottimizzando una cross-entropy sui token target. La causal mask\nè quindi essenziale per evitarepeeking: se la self-attention fosse bidirezionale, la\nrappresentazione al passoT−1potrebbe già “vedere” il tokenTdurante il for-\nward pass, rendendo il compito di predizione artificialmente facile e inducendo\nil modello a imparare corrispondenze inesistenti; ad inference-time, d’altronde,\nnon hai informazione sui token futuri, quindi creeresti un mismatch ta training\ned utilizzo e la causalità serve proprio a questo.\nLLM vs SLMIn letteratura non esiste una soglia universalmente accettata\nche separiSmall Language Models(SLM) eLarge Language Models(LLM): la\ndistinzione è in larga parte relativa allo stato dell’arte. Detto questo, una con-\nvenzione quantitativa ragionevole è trattare comeSLMi modelli nell’ordine di\npochi miliardi di parametri(ad esempio∼1–8B), che rimangono deployable\ncon requisiti più contenuti e spesso adatti a scenari con minore disponibilità\ndi risorse [Subramanian et al., 2025]; molti lavori e survey usanoLLMper ri-\nferirsi a modelli cosiddettifoundationalconoltre∼10B parametrie spesso\nanche nell’ordine di decine o centinaia di miliardi [Subramanian et al., 2025].\nIn una prospettiva più “industriale”, si trovano anche definizioni operative che\ncollocano gli SLM tipicamente tra100M e 10Bparametri, mentre gli LLM ar-\nrivano acentinaia di miliardi o trilioni[Chen, 2025], il che, personalmente,\npreferisco.\n15\n2.14 Chain-of-Thought\nIlChain-of-Thought(CoT) è una famiglia di tecniche di prompting che indu-\nce il modello a simulare una sequenza di passaggi intermedi prima di produrre\nla risposta finale; in realtà, l’approccio a cui si ispira è molto simile al prin-\ncipio didivide et impera: scomporre un problema complesso in sottoproblemi\npiù semplici e risolverli in sequenza; empiricamente il ragionamento step-by-\nstep rende il Language Model molto meno error prone, efficace soprattutto per\nmodelli grandi e task complessi: d’altronde, il CoT stesso coincide con quello\nche comuenemente si chiamareasoningdel modello, ovvero token intermedi che\nutilizza per scomporre il problema ed approcciarlo, come suddetto, step wise\n[Wei et al., 2022].\nOperativamente, il CoT può essere realizzato in diversi modi:\n•Few-Shot: esempi nel prompt in cui, oltre alla risposta, viene mostrata\nuna possibile catena di passaggi da seguire [Wei et al., 2022]\n•Zero-Shot: istruzioni minimaliste del tipo “let’s think step by step”,\ninducendo il reasoning anche senza esempi [Kojima et al., 2022]\n•Self-Consistency: invece di usare una singola catena, si sfruttano diver-\nse, chiamiamole, ’run’ del modello, ovvero più ragionamenti con tempe-\nrature>0, aggregando la risposta più frequente o più coerente; questo\nfa riferimento a concetti simili al majority vote e con forte integrazione\nnell’ensembling di LLM, mitigando la variabilità della singola generazione\n[Wang et al., 2022]\nSe si guarda alle tecniche modere di training dei LLM, principalmente i co-\nsiddetti post-trainings, si vede come i modelli vengano direttamente allenati\nsu problemi (matematica, programmazione...) dove è direttamente disponibile\nuno o più ragionamenti con cui costruire la soluzione: nei modelli moderni il\nreasoning non è solamente indotto dal prompting, bensì si cerca di instillare di-\nrettamente capacità di ragionamento nella distribuzione appresa ed in maniera\ntale che la CoT stessa sia più efficace.\nIn questo lavoro, il CoT è stato utilizzato soprattutto come meccanismo di\nadattività del retrieval: prima di generare la risposta, il modello svilup-\npa una catena di ragionamento scomponendo la query in sotto - domande più\nsemplici per interpretare meglio la richiesta e stimare quali evidenze siano ne-\ncessarie, quali siano già coperte dal contesto recuperato e, soprattutto, che cosa\nmancaancora per soddisfare l’information need; così, il CoT non serve solo a\n“ragionare meglio” ma a guidare la ricerca stessa, rendendo la pipeline, appun-\nto, adattiva alla variabilità della complessità delle query, con buon aumento dei\ncosti di gestione.\n16\n2.15 LLM-as-a-Judge\nValutare un motore di ricerca non è semplice: nelle sezioni precedenti abbia-\nmo discusso dei problemi che ci sono nel retrieval full-text, come il Vocabulary\nMismatch, monosemia o polisemia del dominio... tutte caratteristiche ragione-\nvolmente mitigate dalla ricerca densa; trapela un aspetto che, qui, diventa fon-\ndamentale: le performance di un motore di ricerca dipendono prevalentemente\nda come gli utenti formulano le query, quindi dovresti fissare una distribuzione\ndi riferimento per ottenere metriche significative al contesto applicativo; questo\nnon è sempre possibile e né facile fare, nonché anche costruire una ground truth\nè arbitrariamente complesso: se ti riduci a metriche come Precision, Recall,\nF1-Score... presupponi che, per ogni query che valuti fissata la distribuzione,\ntu possa avere un ground-truth sulla base di tutti i documenti che rispondono,\no meno, all’informatio need. In corpora molto grandi, anche se non valutassi\ndirettamente chunking, dovresti, per ogni query, avere etichette rispetto a tutti\ngli elementi del corpora: i costi di labeling esplodono, tuttavia necessari per\nottenere, soprattutto sulla Recall, valutazioni significative.\nPer superare criticità del genere, indipendentemente dal contesto di retrieval,\nsi è diffuso il paradigmaLLM-as-a-Judge: utilizzare un Language Model co-\nmevalutatoreche, dato un input ed una o più risposte candidate, produce\nuna label seguendo criteri di valutazione espliciti indicati tramite prompting.\nSostanzialmente, le capacità del modello non vengono sfruttate solo per gene-\nrare una risposta alla query, ma anche pervalutarla: l’LLM può confrontare\nl’output con una reference, oppure stimarne la qualità assegnando etichette di-\nscrete o punteggi numerici facenti riferimento a metriche indicate e spiegate\nnel prompt; in generale decidere quale tra più risposte candidate sia la mi-\ngliore. Questo approccio è diventato popolare soprattutto per la semplicità:\nconsente valutazioni rapide e relativamente economiche rispetto ad annotazio-\nne umana, pur mantenendo significatività rispetto alle valutazioni prodotte.\n[Zheng et al., 2023, Liu et al., 2023].\nI pattern tipici di LLM-as-a-Judge includono:\n•Pairwise preference: dato (prompt, risposta A, risposta B), il giudice\nsceglie la migliore e/o fornisce una motivazione; spesso più stabile di uno\nscore assoluto, d’altronde puoi sfruttare la descrizione per audit successivi\ne prompt engineering\n•Rubric-based scoring: si fornisce una griglia di criteri (es. correttezza,\ncompletezza, grounding, chiarezza) e si chiede un punteggio per ciascuno\n[Liu et al., 2023].\n•Reference-based grading: si confronta la risposta con un gold answer;\nutile quando hai una reference affidabile e come metrica per comprendere\nle performance del modello nel judging, non sempre hai dei gold answer\nperò\n17\n•Multi-judge / majority vote: si usano più giudizi (variazioni di prompt\nomodelli)esiaggregaperridurrelavarianzanellerisposte; stessoconcetto\ndi ensembling nel ML, maggiori costi ovviamente\nQuando si tratta di valutare una pipeline di RAG, come spiegheremo succes-\nsivamente, il Judging con LLM è molto interessante nel valutare soprattutto:\nilgrounding, cioè se la risposta è effettivamente supportata dalle evidenze\nrecuperate e non le contraddice, piuttosto che lafactuality, cioè l’assenza di\nallucinazioni o affermazioni non giustificate dal contesto e lacompletezza,\nintesa come copertura dei punti richiesti dalla domanda senza introdurre diva-\ngazioni o contenuti superflui; oltre che potresti pensare di utilizzarlo anche per\nvalutare la bontà dei chunk prodotti, quindi come valutazione per la strategia di\nchunking. Insomma: il vantaggio principe dei LLM, coincidente con le ottime\nprestazioni generaliste (zero - shotting), si vede anche e soprattutto usandoli\ncome valutatori.\n2.15.1 Bias\nMolti vantaggi sicuramente, ma altrettante criticità spesso “velate” e difficili da\ninterpretare; per spiegare meglio quello che voglio dire: spesso modelli cinesi (ad\nes. Qwen, Kimi) non sono buonijudgein contesti general-purpose, non tanto per\nlimiti prestazionali, quanto per questioni dialignmentdel giudizio relativo al\npre-training di questi modelli. La funzione di giudice richiede prevalentemente\ncoerenza nel comprendere e nell’applicare criteri: per dirla proprio informal-\nmente, un LLM è tanto più un bravo Judge quanto più è imparziale, quindi, se\nil training ha insegnato a rispondere seguendo determinate policies, automati-\ncamente riduci l’imparzialità e la bontà del giudizio; ho fatto precedentemente\nl’esempio di modelli cinesi proprio perché sono abbastanza emblematici di que-\nsto: basta provare a chiedergli un giudizio sulla qualità argomentativa di due\ntesti che parlano di argomenti politicamente sensibili, come Taiwan ad esempio,\nper rendersi presto conto di quello che stiamo dicendo.\nI problemi più noti ed interessanti del LLM Judging, ricollegandoci all’intuizione\nprecedente, a mio avviso si riassumono in:\n•Biasdistile: ilLMpuòpreferirerispostepiùlunghe, sicureneltonoome-\nglio formattate anche quando sono meno rilevanti rispetto all’information\nneed\n•Prompt Sensitivity: qui non ha senso fare fine tuning, o meglio, l’utiliz-\nzo stesso del LM come Judge impone voler sfruttare le capacità generaliste\ne fine tunings ti appesantiscono situazioni nelle quali vorresti, ad esempio,\ncambiare facilmente diverse metriche valutative; tuttavia, piccoli cambia-\nmenti nel prompt possono alterare di molto i giudizi ed è necessario fornire\nvalutazioni stabili; per questo, spesso si parla di ’settare latemperatura\na0’: questo è un parametro che indica, intuitivamente, la ’creatività’\ndel modello, ovvero quanto vuoi diverse le risposte che genera a parità\n18\ndi prompt. Per judging, ovviamente, auspicabilmente vorresti che queste\nsiano il più possibilmente deterministiche, low variance insomma\n•Calibrazione degli score: punteggi numerici (es. 1–10) sono spesso poco\ncalibrati e non comparabili tra prompt/modelli; questi sono abbastanza\npericolosi, poiché inducono un’illusione di precisione: il LM risponde, ad\nesempio,5non perché valuti realisticamente rispetto ad una scala di cui\nha cognizione, ma perché è la distribuzione di probabilità sui token da\ngenerare, acquisita durante il pre-training, che gli indica tale score; per\nquesto, a mio avviso, quando si sfrutta un LM come Judge è sempre meglio\nfar riferimento ad etichette categoriche: la semantica dei termini è appresa\ned è, per il modello, più facile modellare un’associazione termine-input che\nvalutazione-input, in quanto i numeri stessi non hanno una semantica ben\ndefinita o collegabile al prompt stesso\nUn aspetto particolarmente interessante è la forte analogia con i problemi clas-\nsici delcrowdsourcing: quando si vuole stimare l’affidabilità di un worker si\nintroducono tipicamente gold standard, majority vote, misure di agreement e\nscore di qualità. Il judging con LLM, difatto, ripropone problematiche molto\nsimili e concettualmente potresti estendere le suddette mitigazioni, usatissime\nnella pratica.\n2.16 SDK e Cloud Providers\nQui tocchiamo una parte più ’tecnica’ relativa agli strumenti utilizzati per co-\nstruire le pipeline di RAG discusse successivamente; manteniamo sempre un\ntono concettuale, ai sensi del fatto che discutere a livello tecnico di ’codice’\nimplementativo, di fondo, non aggiunge nulla al lettore ed appesantisce inutil-\nmente la discussione. PerCloud Providersi intendono aziende che offrono pre-\nvalentemente risorse di calcolo, storage e servizi software tramite infrastrutture\ndistribuite (data centers), tipicamente con l’obiettivo primario di garantirehigh\navailabilitye continuità di servizio: ridondanza, tolleranza ai guasti, scalabilità\nelastica e, più in generale, qualità di servizio difficilmente ottenibili on-premise.\nNonostante, ad oggi, si parli molto più di AI che di Cloud, quest’ultimo riveste\ncomunque un’importanza fondamentale: le necessità computazionali introdotte\nda molti degli strumenti presentati per il retrieval, come modelli di embedding,\nLLM, pipeline di Indexing... sono tali da rendere il cloud un prerequisito in\nmolti contesti applicativi per tutti i vantaggi suddetti.\nTali Cloud Providers forniscono, spesso, cosiddetteSoftware Development Ki-\nts(SDK): sono collezioni di librerie, API e strumenti che facilitano l’accesso\nai servizi del provider, permettendo di interagire con essi ai sensi di creazione,\neliminazione, monitoraggio... delle risorse. Nel nostro caso applicativo di RAG,\nquesta astrazione si traduce nella possibilità di costruire rapidamente una pi-\npeline end-to-end usando componenti predefiniti per storage, ingestion, indiciz-\nzazione, retrieval e integrazione con endpoint LLM/embedding, riducendo dra-\nsticamente iltime-to-first-result; la principale criticità, che discuteremo meglio\n19\nsuccessivamente, riguarda proprio la standardizzazione stessa e la sua tenden-\nza a vincolare molte scelte critiche per la qualità del retrieval: pre-processing,\nchunking, ranking... mitigando, di fondo, molto la semplicità progettuale che\nviene dalle SDK stesse.\n2.17 LangChain\nLangChain è una libreria progettata per costruire applicazioni LLM-centric, of-\nfrendo, anch’essa, un livello di astrazione importante nella gestione di unità di\nlavoro LLM-based all’interno di prodotti più grandi; le considerazioni che ho\ntratto lavorando con LangChain mirano, proprio, sul quello che, a mio avviso,\nè il valore principale: il controllo; permette di definire in modo esplicito loader,\ntrasformazioni di pre-processing, strategie di chunking avanzate, pipeline di em-\nbedding e costruzione di indici vettoriali, oltre a comporre catene più articolate\ncon step intermedi (query rewriting, routing, multi-step retrieval...), rendendo\nLangChain adatto a riprodurre ed estendere pipeline RAG “gestite” dai pro-\nvider, mantenendo però la possibilità di intervenire su punti progettualmente\ncritici. Sostanzialmente quindi, da una parte le SDK cloud accelerano l’integra-\nzione infrastrutturale, dall’altra LangChain accelera l’esplorazione sperimentale\ne principalmente quando, appunto, non devi sottostare a tutti quei vincoli che\nderivano dall’immissione sul mercato di un prodotto.\n2.18 Groq\nGroq è un provider di inferenza focalizzato subassa latenzae altothrough-\nputnell’esecuzione di LLM a costi competitivi, esponendo endpoint compatibili\ncon API standard ed un parco modelli abbastanza ampio per coprire casi d’uso\nanche diversi da quello classico di QA: modelli general-purpose, varianti più leg-\ngere per real-time, Speech-to-Text.. Il principale vantaggio che ho riscontrato\nnell’utilizzo è la semplicità di gestione dell’API tramite libreria Python di Groq,\nrisultando generalmente meno verbosa rispetto a soluzioni anche più blasonate\ncome OpenAI, piuttosto che Google. Nel lavoro, Groq è stata utilizzata preva-\nlentemente per iljudginge per la generazione della risposta finale a partire dal\ncontesto recuperato dal retrieval.\n3 Retrieval Augmented Generation\nI Large Language Models (LLM) hanno reso possibile interagire dinamicamente\ncon una grande quantità di informazione codificata, grazie al pre - training, nei\nparametri stessi del modello. Quando si parla diallucinazioni, si fa riferimento\nad un fenomeno in cui il Language Model restituisce una risposta ’inventando’\ninformazioni che sono sì, spesso e volentieri, semanticamente correlate alla ri-\nchiesta ma non sono supportate da evidenze reali. L’idea del RAG di fondo è\nsemplice: prima di rispondere, il sistema valuta la richiesta dell’utente su un\ncorpus documentale, poi condiziona la generazione sulla base di cosa la ricerca\n20\nrestituisca, ottenendo risposte potenzialmente più accurate e, soprattutto, veri-\nficabili; c’è una forte interazione, quindi, tra retrieval e Language Models, anzi:\nin un certo senso, quest’ultimi servono solo come mezzo di organizzazione del-\nl’informazione pertitente, difatti con system prompt opportuni puoi istruirli ad\nutilizzare la conoscenza pregressa non per rispondere direttamente ma per inter-\npretare l’informazione e costruire una risposta basata unicamente sul ’ground’\nfornito dalla ricerca.\nL’importanza del RAG è principalmente riconducibile, oltre al concetto prece-\ndente di mitigazione delle allucinazioni, ad una necessità strettamente pratica:\nallenare continuamente LM è costoso, anzi, inutile ogniqualvolta l’informazione\nsia caratterizzata da una forte componente didinamismo; il fatto che un LM\npossa essere allenato su testo ed imparare da esso implica che questi abbiano\nun’estensione abbastanza naturale alQuestion Answering, infatti i modelli di\nmaggiore utilizzo ad oggi vengono definitiChat Completion Modelsproprio per\nquesto, tuttavia non necessariamente l’allenamento è la strada migliore per far-\nlo. In contesti, come quelli aziendali ad esempio, l’informazione tipicamente\nevolve molto velocemente, di conseguenza il RAG si concentra sul progettare\nuna pipeline che comprenda dapprima una ricerca efficiente sulla base docu-\nmentale, in quanto se riesci sempre a fornire un contesto corretto ottieni buone\nrisposte alle query dell’utente. Generalmente, si pensa che questo avvenga in-\ndipendentemente non solo dall’allenamento del modello ma anche dal modello\nstesso, permettendo performance migliori anche con Small Language Models:\ntorneremo su questo concetto con i risultati, in quanto, seppur naturalissima,\nnon necessariamente è un’intuizione corretta.\n3.1 Semantic Collapse, Hubness e Chunk Dependency\nIn una implementazionenaïve, il RAG procede tipicamente così:\n1. i documenti vengono segmentati inchunkdi lunghezza fissa,\n2. ogni chunk viene trasformato in un vettore di embedding e inserito in un\nindice vettoriale\n3. a query time si calcola l’embedding della domanda e si recuperano i top-\nkvicini più simili rispetto a metriche come, prevalentemente, lacosine\nsimilarity, seguendo sempre un approccio Nearest Neighbor, o meglio,\nversioni approssimate e più efficienti computazionalmente come ANN ma\nnon cambia il discorso\n4. i chunk recuperati vengono passati all’LLM per la risposta\nQuesto schema funziona bene in molte applicazioni, ma tende a degradare quan-\ndo la base di conoscenza cresce e diventa eterogenea, oltre a particolarità deri-\nvanti dalla distribuzione lessicale del caso; è proprio in questi casi che emerge\nil cosiddettosemantic collapse, ovvero la perdita di separazione utile tra conte-\nnuti davvero rilevanti e contenuti solo semanticamente simili. Il retrieval denso\n21\n(dense retrieval) si basa sull’ipotesi che la similarità geometrica nello spazio\ndegli embedding sia un buon approssimante della rilevanza rispetto la query\n(information need). Tuttavia, quando l’indice aumenta di dimensione, l’accura-\ntezza del recupero può peggiorare sensibilmente: Reimers e Gurevych mostrano\nteoricamente ed empiricamente che, con indici grandi, le rappresentazioni den-\nse possono incorrere in un aumento difalse positive, fino a un punto di svolta\nin cui approcci full - text (es. classic BM25) possono diventare competitivi o\naddirittura superiori [Reimers and Gurevych, 2021]. Questo comportamento è\nperfettamente coerente con aspetti più generali dellacurse of dimensionality,\ndove la nozione di “vicinanza” in spazi ad alta dimensionalità perde di importan-\nza rispetto a quello che normalmente si osserva in un piano euclideo ad esempio\n[Chen et al., 2025a]. Di conseguenza quindi, quando sfrutti un indice vettoria-\nle hai di base un limite alla dimensionalità degli embedding, e quindi alla loro\nrappresentatività, per il problema dellacurse of dimensionalitye, all’aumentare\ndella grandezza del corpora, ti imbatti di base in unsemantic collapse, il che\naiuta a capire meglio, poi, quando discuteremo di approcci Hierarchical.\nUn secondo problema strettamente correlato è laHubness: in spazi ad alta\ndimensionalità, alcuni punti, cosiddetti hub, tendono a comparire con frequen-\nza anomala come vicini di molti altri punti, distorcendo la ricerca dei nearest\nneighbors [Feldbauer and Flexer, 2019]; in un contesto RAG come il nostro, gli\nhub sono sostanzialmente chunk molto “generici” e che si trovano in posizioni\ndefiniamole ’centrali’ dello spazio degli embedding, entrando spesso nel top-ka\nprescindere dalla specificità della domanda, riducendo la qualità dell’informazio-\nne restituita. Quest’ultimo problema di Hubness è fortemente correlato con la\ndipendenza del dense retrieval dalchunking: la segmentazione del testo determi-\nna l’unità informativa che indicizzerai, quindi il tuo corpus, e conseguentemente\nil vantaggio prestazionale che la ricerca semantica aggiunge a metodi tradizio-\nnali; sembra banale che il retrieval denso basato su embedding dipenda da che\ncosa usi per costruirli, ma proprio perché è un concetto basilare l’approccio al\nchunking è fondamentale. I chunk non sono praticamente mai auto esplicativi,\ndifatti se troppo piccoli frammentano il contesto e spezzano il discorso, catene\nlogiche... mentre chunk troppo grandi aumentano rumore, mischiano concetti\ndiversi e riducono molto più facilmente l’allineamento domanda - testo rilevan-\nte. D’altronde, la stessa informazione può essere distribuita su parti lontane di\nun documento: il chunking ti fa intrinsecamente perdere relazioni del genere.\nSostanzialmente quindi, parte della qualità di un RAG è “decisa” prima ancora\ndel retrieval, al momento della definizione dei chunk e dei metadati.\nTorneremo in maniera più approfondita su molti dei concetti precedenti nelle\nsezioni successive.\n3.2 Proposta Iniziale\nLa proposta iniziale prevedeva una combinazione di tre tecniche:\n22\n1. Recursive Chunking come alternativa al Fixed per ottenere una prima\nsegmentazione “strutturale” del documento\n2. Clustering semantico dei chunk tramite distanza tra embedding, con l’o-\nbiettivo di raggruppare contenuti semanticamente simili\n3. LLM Stitching per riscrivere i chunk del cluster semantico in manierà più\ndiretta, oltre che risolvere errori derivanti, ad esempio, da modelli OCR\npreliminari\nIl lavoro si è esteso rispetto alla suddetta formulazione, mantenedo gran parte\ndella struttura ma integrando le intuizioni rispetto alle proposte del relatore;\nin particolare: l’analisi e l’utilizzo di strutture a grafo (GraphRAG) si integra\nmolto bene con il clustering semantico, difatti per identificare sottoinsiemi di\nchunk del genere è possibile utilizzare algoritmi diCommunity Detection\ncome Louvain/Leiden. Il primo passo della tesi rimane comunque l’analisi di\nstrategie di chunking alternative al fixed chunking, privilegiando segmentazio-\nnistructure-awarecome il Recursive Chunking, estendendo anche a soluzioni\nsemantic-aware. Un punto emerso durante lo studio riguarda, poi, l’impatto\ndelle trasformazioni testuali sul retrieval ibrido: non adottiamo più LLM stit-\nching, in quanto una riscrittura, sia questa aggressiva o meno, può indebolire\nla componentefull - textdella ricerca, determinante per una certa classe di\nquery, riducendo proprio la sovrapposizione lessicale tra query e testo, renden-\ndo complessivamente la qualità delle risposte ottenute molto minore per query\ndifatto semplici; sintesi ed astrazioni vengono, tuttavia, considerate unicamente\ncome supporto per ridurre lo ’scope’ della ricerca e, comunque, parte integran-\nte dell’approccio Hierarchical di cui diamo un’intuizione nel paragrafo che segue.\nLa struttura che ho costruito coincide con unsemantic chunk graph, coeren-\ntemente con la proposta del relatore: i nodi rappresentano chunk e gli archi\ncodificano sia relazioni strutturali (es. adiacenza) sia relazioni semantiche (es. si-\nmilarità tra embedding); su questa base, valutiamo la componente diClustering\nCommunity Based: l’idea è instradare la query primariamente su una dimensio-\nne aggregata data da un riassunto ad alto livello della tematica di community,\nper poi valutare un drilldown successivo sui chunk che la compongono. Questa\nimpostazione è coerente con la direzione di approcci gerarchici nella letteratura\nRAG (ad es. RAPTOR), che mostrano i benefici del retrieval a diversi livelli di\nastrazione [Sarthi et al., 2024]; analogamente, lavori come GraphRAG eviden-\nziano come una rappresentazione a grafo e l’aggregazione a livello di community\npossano supportare domande più “globali” e multi-hop [Edge et al., 2024].\n4 Chunking\nMolti aspetti del chunking e delle criticità che introduce sono già stati discussi;\nresta però naturale chiedersi perché sia davvero necessario. Il primo motivo è\nstrutturale: i language model hanno una finestra di contesto limitata, che nella\n23\npratica impedisce quasi sempre di fornire in input un documento intero (o, a\nmaggior ragione, un’intera base documentale); di conseguenza, la scomposizione\ndel testo in unità più atomiche non è una scelta “opzionale”, ma una necessità.\nC’è poi una motivazione legata agli embedding: se l’obiettivo è rappresentare\nin vettori la semantica del testo, allora un contenuto troppo esteso e ricco di\nargomenti tende a mescolare segnali diversi, in quanto l’informazione specifica\nche vorremmo recuperare viene “diluita” in una rappresentazione più generica,\nal contrario chunk eccessivamente brevi rischiano di perdere la semantica globa-\nle e il contesto necessario per interpretarli correttamente. Il chunking, infatti,\nnasce proprio come compromesso tra questi due estremi: preservare abbastanza\ncontesto da mantenere coerenza rispetto al documento di riferimento, ma non\ncosì tanto da perdere l’informazione specifica. Grosso del perché, a livello pra-\ntico, la ricerca sia spesso unaHybrid Search, combinando sia ricerca densa che\nfull - text, deriva proprio dal garantire maggiore specificità del retrieval quan-\ndo la sovrapposizione lessicale lo permette. In questo senso, il RAG si fonda\nsull’assunzione che grandi collezioni documentali non possano essere incluse in-\ntegralmente nel contesto del modello; idealmente, basterebbe fornire ogni volta\nl’intero corpus come contesto al language model e ottenere risposte perfettamen-\nte accurate. Nella pratica, questo approccio è proibitivo per vincoli soprattutto\nrelativi al costo (i LM si pagano per token in input e generati), rendendo il\nchunking l’unica alternativa praticabile rispetto a tutti i vincoli.\n4.1 Sentence Transformers\nI Sentence Transformers sono modelli pensati per produrre embedding di por-\nzioni testuali, ottimizzati per compiti di similarità e retrieval: l’architettura\ntipica riusa un backbone Encoder di tipo Transformer (es. BERT/RoBERTa),\nanche se ad oggi si vedono anche riutilizzi di porzioni delle architetture De-\ncoder di Language Models generativi, ed aggiunge un meccanismo di pooling\n(mean/max/CLS o varianti) per comprimere le rappresentazioni token-level in\nun singolo vettore a dimensione fissa. Gli embeddings risultanti sono proiezioni\nin uno spazio, tipicamente alto - dimensionale, che mirano a catturare la seman-\ntica “a livello di frase”, in modo che testi con significato simile abbiano vettori\nvicini secondo una metrica (spesso similarità coseno) e testi semanticamente\nlontani siano ben separati. L’addestramento avviene con losses contrastive o\nranking-oriented: in configurazioni Siamese/bi-encoder si usano, ad esempio,\ncosine/MSE per regressione di similarità, triplet loss, contrastive loss e soprat-\ntutto Multiple Negatives Ranking Loss (o InfoNCE), che spinge le coppie posi-\ntive ad avvicinarsi e separa le negative nello stesso batch. Tramite le suddette\nloss risolvi un problema fondamentale alla base del pre - training di architetture\nTransformer come BERT: lo spazio degli embedding prodotto è molto schiac-\nciato, tanto che le differenze di cosine similarity che determinano la differenza\no similarità semantica tra due testi sono talmente piccole che, informalmente,\n’tutto sembra uguale a tutto’; ci si riferisce spesso a questo comeanisotropia\ndello spazio.\n24\nIl modello che si usa è molto importante in relazione alle necessità: se la pipeline\ndiRAGviveincontesticonvincolireal-time, alloranonpuoiaspettartidiutiliz-\nzare Sentence Transformer molto grandi, o meglio, quelli che occupano le prime\nposizioni delle leaderboard in MTEB (benchmark), in quanto sono molto molto\nlenti nella costruzione degli embedding, il che va totalmente contro le condizioni\ndi alto traffico. Nel nostro caso, abbiamo utilizzatoBAAI/bge-large-en-v1.5:\nmodello con circa600Mdi parametri, relativamente lento ma molto utilizzato\ntra quelli open - source per progetti di ricerca e complessivamente garantisce un\nbuon tradeoff rispetto ad i vincoli discussi precedentemente. Come suggerisce il\nnome, il modello è adatto solamente per testi in lingua inglese: abbiamo fatto\nquest’assunzione sulla scelta dei dataset di testing, sebbene non necessariamente\nil mondo reale ti mette davanti situazioni sempre di questo tipo, infatti l’utilizzo\ndi Decoder di modelli generativi come SentenceTransformer viene proprio dal\nfatto che il pre - training di questo li rende molto più robusti alla variabilità\ndella lingua.\n4.2 Strategie\nL’importanza del chunking è già stata trattata a livello concettuale e, ai fini di\nquesta sezione, l’infarinatura è sufficiente: importante evidenziare, però, che non\nesista una strategia universalmente ottimale, poiché la bontà della scelta dipen-\nde fortemente dal dominio e dalla natura del testo. In documenti tecnici, come\narticoli scientifici ad esempio, la struttura esplicita di titoli, paragrafi, elenchi...\nè spesso affidabile per segmentare preservando segnale utile, definendo un’uni-\ntà semantica fondamentale; al contrario, in un romanzo la coesione semantica\nsegue dinamiche narrative (scene, dialoghi, personaggi, flashback) che rendo-\nno i confini precedenti un modo di chunkare troppo “meccanico”, rischiando di\nspezzare la continuità sia strutturale che logica. D’altronde, testi provenienti\nda OCR o con una formattazione particolare sono spesso soggetti a vari errori\ndi conversione da modelli commerciali e richiedono una fase di pre - processing\nprima della segmentazione, in quanto ciò che da questi viene dichiarato come ca-\npitolo, sezione... potrebbe essere un semplice testo inboldcome questo, quindi\nnon è sempre un’alternativa viabile.\nQuesto è un punto di discussione importante per inquadrare il lavoro fatto:\nl’obiettivo è quello di ricavare risultati sperimentali relativamente a procedure\nche non vengono costruite specificamente per un singolo dominio, anche se per\nrendere una pipeline di RAG competitiva le scelte che vai a fare devono tas-\nsativamente essere verticali al particolare contesto. Difatti, sebbene non esista\nungold - standardho provato a costruirne uno, quindi degli approcci che\nindipendentemente dal caso ti garantiscono delle performance ragionevolmente\ndecenti ed i discorsi che andiamo a fare sono intendibili come un punto di par-\ntenza sul cui basarsi per scelte progettuali successive e, appunto, più specifiche\nal dominio di riferimento.\n25\n4.2.1 Fixed e Recursive Chunking\nIlFixed Chunkingè, probabilmente, la strategia più famosa: segmenta il\ntesto in blocchi di lunghezza costante (tipicamente per numero di caratteri o\ntoken), spesso con una overlap per non perdere contesto tra chunk adiacenti; è\nsemplice, veloce e facile da parametrizzare, ma introduce problemi strutturali\nevidenti poiché, nonostante l’overlap, nulla ti vieta di produrre chunk dove le\nfrasi vengono interrotte e le parole spezzate, con il rischio di peggiorare sia em-\nbedding che retrieval duplice la poca interpretabilità.\nIlRecursive Chunking, come implementato in LangChain (ad es.Recursive\nCharacterTextSplitter), mantienelastessaideadidimensionalitàfissa(chunk_size\nechunk_overlap), ma invece di tagliare in modo uniforme prova prima a se-\nparare il testo usando una gerarchia di separatori “dal più forte al più debole”\n(ad es. doppi a capo→a capo→spazi→fallback su caratteri), ricorrendo in\nmodo iterativo finché ogni segmento rientra nella soglia: in questo modo tende\na preservare paragrafi e unità locali dove possibile, riducendo la frammenta-\nzione precedente dell’approccio Fixed. Rimane comunque fortemente euristico:\nla qualità dipende dai separatori scelti e dalla formattazione del testo (OCR\nrumoroso, LaTeX, markdown, ecc.), potendo ancora produrre tagli innaturali\nquando non trova delimitatori “buoni” o quando l’informazione semanticamen-\nte rilevante supera le dimensioni imposte che, come suddetto, rimangono fisse.\nTuttavia, il Recursive Chunking conserva il “buono” della semplicità che c’è nel\nFixed Chunking (stessi parametri e costi simili), ma strutturalmente ti permette\ndi attenuare molte delle criticità alla base dell’approccio Fixed; successivamente\nfaremo riferimento proprio a questo: sebbene la strategia sia fortemente dipen-\ndente dal dominio, spesso e volentieri approcci Recursive sono quanto di più\nvicino ci sia ad un gold - standard.\n4.2.2 Semantic Chunking\nIlSemantic Chunkingmira a segmentare un testo seguendo la sua coerenza\nconcettuale, anziché per finestre fisse o strutturalità: un approccio molto usato\ndi recente, e a cui mi sono ispirato, è quellosentence-by-sentence, dove si calcola-\nno gli embedding di frasi (o finestre brevi) consecutive e si identifica un punto di\ntaglio quando la similarità coseno tra porzioni adiacenti scende sotto una soglia,\ninterpretandolo come topic shift. Questo lo differenzia dagli approcci precedenti\ne lo rende più robusto ad impaginazioni irregolari o contenuti in cui la strut-\ntura non coincide con i confini semantici. Esistono molte varianti di Semantic\nChunking, ma quasi tutte ruotano attorno allo stesso principio: segmentare do-\nve si osserva una discontinuità semantica; il problema principale degli approcci\nsentence-by-sentence è prevalentemente dato dalla definizione del punto di ta-\nglio e per questo ho costruito una variante del chunking semantico che sfrutta\nindicatori statistici per rendere l’approccio più robusto. Progettualmente è sor-\nto un aspetto abbastanza delicato e che sarà emblematico quando discuteremo\ndei risultati: se si costruisce progressivamente un chunk “accumulando” frasi e\n26\nrappresentandolo con la media degli embedding già calcolati, allora la rappre-\nsentazione tende spesso a collassare verso un centroide poco informativo, ovvero\nun embedding ’Hub’ che racchiude la totalità semantica del documento e che\nè sempre molto simile a tutto il contenuto testuale successivo, rendendo più\ndifficile rilevare topic shift e producendo chunk valutando il taglio solo quando\nvengono eccedute le dimensioni massime prefissate, collassando praticamente ad\nun fixed, oltre che il chunk stesso potrebbe idealmente convergere alla totalità\ndel documento se tale cap sulle dimensioni idealmente non ci fosse. Il perché\ndi questo è relativo al fatto che usare una media degli embeddings delle fine-\nstre accumulate fino a quel momento è meno oneroso che valutare un ricalcolo\nintero: porzioni testuali più piccole, meno calcoli, più veloce; questa scelta è\nproprio relativa al fatto che la principale inefficienza dell’approccio di Semantic\nChunking è la complessità computazionale: il vantaggio che, intutivamente, la\ndefinizione di unità semanticamente coese porta al retrieval non sempre è giusti-\nficato dalle valutazioni sui test set o dall’incremento eccessivo che c’è del tempo\ndi calcolo e delle risorse impiegate, il che rende proibitivi approcci di chunking\nnon rule - based, come anche quelli che sfruttano LLM, su corpora molto grandi\n[Qu et al., 2025].\nLa strategia di Semantic Chunking che ho costruito introduce due accorgimenti\nper renderlo più stabile:\n•pre-processing delle frasi\n•soglie adattive per il topic shift\nDopo aver segmentato il testo in frasi e averne stimato la lunghezza in token,\nle frasi “troppo corte” vengono unite con usando indicatori statistici: si cal-\ncola la mediana delle lunghezze e la MAD (Median Absolute Deviation), cioè\nla mediana delle deviazioni assolute da quest’utlima, unendo frasi la cui lun-\nghezza è sotto la soglia ’mediana + MAD’; quest’ultimo è meno sensibile agli\noutlier rispetto alla deviazione standard, quindi è adatto quando le lunghezze\nhanno valori anomali e questo succede abbastanza spesso con testi che deriva-\nno, ad esempio, da OCR, oltre che lo split su frasi è basato sempre tramite\nregole euristiche comuni e non è perfetto, anzi, potrebbe generare frasi arbitra-\nriamente corte; l’obiettivo è evitare che unità troppo brevi producano embed-\nding e, di conseguenza, breakpoint rumorosi. Per il segnale semantico, invece\ndi embeddare ogni frase isolata, si costruiscono finestre di 3 frasi (preceden-\nte–corrente–successiva) ed il confronto di similarità è tra queste: per richiamare\ni concetti di overlap, ciascuna finestra coincide con un intorno delle due frasi\nadiacenti data una centrale di riferimento; in parole semplici, quest’approccio\nsemantico vuole far sì di evitare che il SentenceTransformer debba infierire il\nsignificato di una frase ’da solo’, dandogli contesto. Per individuare i breakpoint\nnon si adotta una soglia assoluta, ma una soglia adattiva ricavata dalla distri-\nbuzione delle distanze tra finestre successive nel documento; sostanzialmente, si\ncalcola un percentile delle distanze (tipicamente tra 75°e 90°): il percentile è\nquel valoretale che unacerta quota delleosservazioni(es. l’80%)risulta inferiore\n27\ne, quindi, vengono marcati come candidati solo i punti in cui la distanza supera\nuna soglia “alta”, cioè le differenze più marcate rispetto alla media del testo e\nche più plausibilmente rappresentano i topic shift. In particolare, il percentile\nnon è ovviamente scelto a priori ma relativamente al coefficiente di variazione\nCV=std/mean: quando le distanze sono omogenee (CV basso) la soglia viene\nalzata usando percentili maggiori, mentre quando c’è maggiore irregolarità (CV\nalto) la soglia si abbassa per intercettare più cambi di argomento. A livello del\ntesto di riferimento, questo equivale a dire che la procedura si adatta allo ’stile\ndi scrittura’: un documento con andamento uniforme, ai sensi di transizioni\nsemantiche graduali tra concetti, tende a produrre distanze simili tra finestre\nconsecutive e, quindi, conviene “tagliare” solo sulle differenze davvero marcate\npoiché complessivamente c’è molta omogeneità nel discorso; al contrario, in testi\npiù discontinui abbassare la soglia permette di separare meglio i topic shift, evi-\ntando di considerare due sub topics un chunk unico. I breakpoint vengono poi\nfiltrati imponendo un numero minimo di frasi per chunk e stimando la lunghezza\nin token dei segmenti finali; sebbene si facciano molte operazioni, in realtà il\nbottleneck prestazionale non è relativo agli indicatori statistici, bensì al calcolo\nstesso degli embeddings e computazionalmente puoi considerarla equivalente al\nsentence-by-sentence tradizionale.\n4.3 Considerazioni su SQuAD ed NQ Dataset\nIl dataset SQuAD (Stanford Question Answering Dataset) è un benchmark per\nmachine reading comprehensionin cui, dato un brano, il modello deve rispon-\ndere a una domanda estraendo uno span testuale (cioè una sottostringa) dal\ncontesto; nasce da articoli di Wikipedia selezionati e poi annotati con coppie\nquestion-answer create tramite crowdsourcing, con risposte relative a porzioni\nprecise, appunto sottostringhe, del testo di riferimento. SQuAD è stato usato\ncome base per misurare statistiche di retrieval e confrontare l’impatto delle due\ndiverse strategie di chunking principe di questo lavoro: fixed e semantic chuning.\nIn SQuAD hai diverse domande associate alla stessa pagina di Wikipedia e gli\nesempi vengono costruiti raggruppando tutto il ’contesto’ per ogni domanda,\ndifatto ricostruendo indirettamente la pagina completa e valutando la strategia\ndi chunking sul complesso: non ha senso valutarla solo sulle singole porzioni\ntestuali ovviamente. Data la natura del dominio, quindi testi enciclopedici, le\nquery mantengono una certa sovrapposizione lessicale con il contesto (nomi pro-\npri, termini chiave, date), pur includendo parafrasi occasionalmente.\nUn secondo dataset considerato è Natural Questions (NQ), che differisce da\nSQuAD prevalentemente per la natura delle query. Le domande in NQ derivano\nda interrogazioni “realistiche” effettuate su motori di ricerca, formulate da utenti\nche non conoscono a priori il contenuto del documento target, differentemente\nda quanto avviene in SQuAD; di conseguenza, le query sono meno ancorate al\ntesto e la sovrapposizione lessicale è mediamente più debole rispetto a SQuAD,\ncaso in cui dovrebbe eccellere il vantaggio della ricerca densa. In questo sen-\nso, NQ rappresenta un benchmark più fedele a scenari di retrieval ’reali’, dove\n28\nl’allineamento query–passage non è garantito da annotatori che si concentrano,\nproprio, sui singoli paragrafi; se volessimo, in poche parole, riassumere quanto\nabbiamo detto: in maniera informale NQ è uno SQuAD che soffre maggiormente\ndel problema divocabulary mismatch, rendendo la fase di chunking potenzial-\nmente più critica per non spezzare contesti necessari ad una ricerca semantica\npiù complessa.\nNei nostri esperimenti su SQuAD (1000 documenti, 5825 query, stime reali-\nstiche), le prestazioni risultano sostanzialmente sature già aK= 2(Hit@2 =\n1.0 per entrambi i metodi), con un Hit@1 molto elevato e differenze marginali\ntra le due strategie (0.9888 per il semantic vs 0.9906 per il fixed). Come mo-\nstrato in Tabella 1, le lunghezze dei chunk prodotti da entrambe le strategie\nsono praticamente identiche e non emerge un vincitore chiaro trai due approcci:\ni∼150token non sono stati scelti casualmente, il semantic chunking ricava la\nlunghezza dei chunk in maniera adattiva, come spiegato nella sezione preceden-\nte, e possiamo sfruttare la lunghezza media dei chunk prodotti dalla strategia\nsemantica come window size per la fixed, rendendo perfettamente comparabili\ni risultati tra le due. In un dominio come questo, la scelta trafixedesemantic\nchunkingincide poco sull’efficacia del retrieval. Spostando la valutazione su NQ\nTabella 1: Risultati di Retrieval su SQuAD (1000 Documenti, 5825 Query).\nModello:baai/bge-large-en-v1.5.\nMetrica Semantic Chunking Fixed Token Chunking\nHit@1 0.98880.9906\nHit@21.0000 1.0000\nHit@3 1.0000 1.0000\nHit@4 1.0000 1.0000\nHit@5 1.0000 1.0000\n(sempre 1000 documenti, 5000 query, rappresentativo), il task diventa più di-\nscriminante, con un calo generale delle performance (Hit@1∼0.88). Tuttavia,\nanche in questo scenario più complesso, non emerge un vantaggio delsemantic\nchunking: il metodofixedrisulta leggermente superiore per tutti iKconsiderati\n(Tabella 3) e, nel complesso, i risultati empirici rafforzano il trade-off discusso\nin [Qu et al., 2025]: ilsemantic chunkingintroduce un overhead computazio-\nnale non trascurabile, dovuto principalmente al calcolo degli embedding e alla\nstima delle soglie di breakpoint, senza garantire miglioramenti evidenti in fase\ndi retrieval puro; ovviamente sia NQ che SQuAD sono dataset semplici, basti\nguardare le metriche, tuttavia questi test rafforzano evidenze sperimentali che\nci sono in lavori molto famosi come quelli già citati e permettono, difatto, di\nconfermare le interessanti considerazioni che vengono fatte: d’altronde, se non\nc’è un delta chiaro in task molto semplici, allora non vuol dire che non possa\nesserci un vantaggio più marcato in task di retrieval più complessi ma risulterà\nsempre ragionevolmente diluito ed è difficile che possa sovrastare un aumento\n29\ncosì importante delle risorse di calcolo nel caso medio.\nTabella 2: Risultati di Retrieval su Natural Questions (1000 Documenti, 5000\nQuery). Modello:baai/bge-large-en-v1.5.\nMetrica Semantic Chunking Fixed Token Chunking\nHit@1 0.87680.8816\nHit@2 0.9728 0.9768\nHit@3 0.9860 0.9878\nHit@4 0.9904 0.9918\nHit@5 0.9924 0.9938\nComunque, il dominio Wikpedia è stato scelto proprio per amplificare i con-\ncetti precedenti legati ai topic shift, estendendo con NQ un ulteriore livello di\ncomplessità, evidenziando come il beneficio al retrival di preservare i confini\nsemantici non è sempre automatico. Richiamando al concetto precedente di\ngold-standard: una strategia a finestra fissa (fixed chunking), sebbene più ru-\ndimentale, si dimostra estremamente competitiva nel tradeoff performance-costo\ne nel caso in cui tu non abbia una perfetta conoscenza di, ad esempio, distribu-\nzione lessicale del caso, a mio avviso, l’adozione ad approcci come il Recurisve\nChunking sono la strada più plausibile.\nCambio di setting.Come indicato precedentemente, per garantire la com-\nparabilità tra metodi si allineavano la dimensione dei chunkFixeda quella\nprodotta dal metodo semantico. Per evidenziare meglio i concetti precedenti,\nabbiamo esteso ad un’impostazione diversa: ilFixed Chunkingviene vincolato\na una finestrarigidae predefinita (W= 256); sostanzialmente, non sfruttiamo\npiù l’informazione derivante dal semantic chunking per calibrare la fixed window\nsize. L’obiettivo diventa valutare la capacità del metodo semantico di adattare\nla segmentazione al contenuto rispetto a una baseline con budget di contesto\nfissato.\nI risultati in Tabella 3 mostrano che, in questo setting, ilSemantic Chunking\nmantiene un vantaggio consistente rispetto alFixed Chunkingsu tutte le metri-\nche considerate. Il principale fattore che spiega il vantaggio del metodo semanti-\nco è la sua capacità di produrre chunk che seguono i confini informativi del testo,\nadattando la grandezza in base dinamicità a livello di topic del documento, in-\ndirettamente rispetto alla sua struttura. Nel caso di NQ, dove abbiamo evitato\nSQuAD poiché più semplice ed i risultati sarebbero stati troppo simili a pri-\nma, ciò si traduce in un contesto medio recuperato più ampio (circa406token,\nrappresentativo), che tende a preservare unità tematiche più coese fino a che la\nsemantica stessa lo permetta; al contrario, la finestra fissa delFixed Chunking\n(W= 256) forza una frammentazione molto più aggressiva, che può spezzare\nporzioni di testo necessarie per allineare correttamente query e contenuto, ri-\nducendo le performance del retrieval. Nel complesso, questi nuovi risultati non\n30\nTabella 3: Risultati su Natural Questions (1000 Doc, 5000 Query).\nMetrica Semantic Chunking Fixed Chunking (W=256)\nHit@10.87680.8706\nHit@20.97280.9680\nHit@50.99240.9902\nsmentiscono le considerazioni precedenti, anzi, le estendono: si rimane perfet-\ntamente coerenti con le osservazioni di [Qu et al., 2025], sottolineando come il\nbeneficio del chunking semantico non è universale ma tende a emergere quan-\ndo il testo è particolarmente ricco di tematiche distinte ed un chunking fisso\nperderebbe eccessivamente in coesione, tuttavia nel caso medio il gain presta-\nzionale sul retrieval, benché in alcuni casi può assestarsi attorno ad un≈10%\nin F1-Score come sui lavori citati, non necessariamente cambia il bilanciamento\nnegativo al tradeoff performance-costo del chunking semantico. È interessante\nnotare come il chunking fisso, sfruttando un’adattività della finestra, possa rag-\ngiungere performance comparabili in termine di auto-esplicabilità dei chunk: in\nquesto lavoro ho valutato anche una strategia di chunking a finestra fissa ma\ndove la dimensione fosse calcolata rispetto alla stessa idea di indicatori statistici\ndel semantic chunking proposto, tuttavia, a mio avviso, molta di questa dina-\nmicità viene comunque garantita strutturalmente da approcci come il Recursive\nChunking che nel complesso hanno maggiori benefici e con meno ’calcoli’ da\nvalutare, da cui si rafforza l’idea precedente digold-standard.\n5 Architetture RAG\nAbbiamo già introddo l’idea fondamental del RAG: paradigma in cui la genera-\nzione di una risposta non dipende esclusivamente dalla conoscenza “interna” del\nmodello ma viene condizionata da contenuti recuperati da una fase preliminare\ndi retrieval; l’LLM si occupa di interpretare e sintetizzare l’informazione, non\nfornirla direttamente. Grossolanamente, un sistema RAG può essere visto come\nuna pipeline composta da due macro-fasi concatenate:retrievalegenerazio-\nneappunto; quando si parla diArchitetture RAGsi fa riferimento a come\nimplementativamente decidi di eseguirle, con particolare enfasi soprattutto sul\nretrieval, nient’altro.\nA partire da questo schema essenziale, gli elementi fondamentali che ricorrono\nin qualunque architettura RAG sono:\n•Knowledge Base ed Indicizzazione: come i documenti vengono ac-\nquisiti, normalizzati e resi ricercabili; in questa fase rientrano chunking,\ntokenizzazione, stopping, stemming, pattern matching, piuttosto che la\nprogettazione anche di complessi modelli di OCR\n31\n•Embeddings e Similarità: non tanto la ricerca vettoriale in sé, ad oggi\ngli algoritmi sono sempre gli stessi e della famiglia ANN, più che altro\nprogettualmente si spinge sul modello che usi, fine tuning...\n•Ranking e Selezione: decidere quali chunk passare al modello ed in che\nordine; il Ranking è un concetto fondamentale e che si capisce molto bene\ncon i motori di ricerca, oggi acquisisce anche nella RAG un’importanza\nparticolare e che discuteremo successivamente\n•Costruzione del prompt e budget di contesto: il contesto utile è\nlimitato (token budget), quindi progettualmente devi decidere quanti e\nquali chunk passare come contesto al modello non solo a livello di Ranking,\nma anche quanto puoi permetterti sopratutto a livello di costi\n•Grounding e verificabilità:l’obiettivo pratico di un RAG non è solo\n“rispondere”, ma rispondere a partire da evidenze; la progettazione del\nflusso deve quindi ridurre l’ambiguità che hai su ’dove’ il modello prenda\nl’informazione che restituisce\nSuccessivamente si presentano le architetture RAG adottate e sperimentate nel\nlavoro, seguendo un percorso progressivo: si parte dal paradigma più diretto e si\nintroducono poi varianti che modificano, prevalentemente, dove e come avviene\nil retrieval e l’organizzazione della conoscenza. Le sezioni successive descrivono\nnel dettaglio le scelte architetturali per ciascun approccio, evidenziando com-\nponenti coinvolte, flussi di esecuzione e implicazioni pratiche, con un focus sui\nvantaggi/svantaggi principali.\n5.1 Naive RAG\nLa varianteNaive RAGrappresenta l’implementazione più diretta e “minimale”\ndi questo paradigma: dato un input testuale, il sistema effettua una ricerca su\nuno o più indici, seleziona ikrisultati più rilevanti e li inserisce (spesso tramite\nsemplice concatenazione) nel contesto del prompt fornito al modello; questa è la\nstessa descrizione che abbiamo fornito nelle sezioni precedenti. Questa imposta-\nzione è estremamente diffusa ed emblematica del concetto fondamentale: l’LLM\nnon deve “sapere” tutto a priori, ma può sfruttare una base di conoscenza ester-\nna aggiornabile e senza ricorrere a training successivi. Tuttavia, la semplicità\ndel Naive RAG è anche la sua principale sorgente di inefficienze: piccoli errori\nnelle fasi a monte (in particolare chunking e ranking) tendono a propagarsi fino\nalla generazione ed è spesso difficile isolare i rapporti causa - effetto; in realtà,\nquesto è vero in tutte le Architetture RAG, quindi è più corretto attribuire il\n’bottleneck’ prestazionale di questa alternativa al fatto che utilizzi una Hybrid\nSearch tradizionale: non ricorri a meccanismi Adattivi/Gerarchici e sei, a parità\ndi bontà del chunking e ranking, esposto ai problemi su Hubness e Dimensiona-\nlity discussi precedentemente.\nSopratutto nella letteratura di oggi, dove sul RAG si fa ancora moltissima ricer-\nca, l’approccio Naive è visto abbastanza ’male’: questa è una tendenza relativa\n32\nal pensare che soluzioni semplici spesso non siano robuste alla variabilità del\ncaso reale, in realtà, sopratutto quando si cerca un gold - standard, il Naive\nRAG è spesso una baseline molto difficile da battere e se le dimensioni del tuo\ncorpora sono abbastanza contenute, probabilmente è l’Architettura RAG più\nvicina al concetto di ’soluzione to - go’ e molto di quello che discuteremo nei\nrisultati fa capolino a questo concetto qui.\n5.1.1 Azure SDK\nNel lavoro svolto, il Naive RAG è stato costruito sfruttando sia l’ecosistema\nAzure che la libreria LangChain; la necessità di questo viene dalla volontà di\nmostrare due aspetti:\n•quanto sia semplice, sopratutto sfruttando SDK di grossi provider come\nMicrosoft, costruire un RAG funzionante\n•quanto, però, sei fortemente vincolato a scelte spesso subottimali su tutti\ni punti progettuali più critici\nCon Azure, una pipeline Naive RAG può essere implementata in modo relativa-\nmente semplice utilizzando Azure AI Search (ex Cognitive Search) come motore\ndi retrieval e Azure Blob Storage come database; concettualmente infatti, quan-\ndo vuoi sfruttare SDK di provider del genere, hai a disposizione librerie che ti\npermettono di concatenare prodotti appartenenti a diversi servizi per, difatto,\nrealizzare le singole funzionalità che cerchi. È importante, quindi, discutere di\ncome Azure ’chiami’ le componenti fondamentali di una pipeline RAG, Naive o\nmeno in realtà:\n•Blob Storage (Data Source): rappresenta l’origine dei dati. I file\n(PDF, DOCX, HTML, testi, ecc.) vengono caricati in un container. Azu-\nre AI Search può essere configurato per leggere direttamente dal container\ntramite unadata sourceche descrive endpoint, credenziali di accesso e\npattern di acquisizione. La fase di estrazione del testo, cosiddettacrac-\nking, viene gestita automaticamente ed è possibile anche utilizzare modelli\ndi Form Recognizing se la sorgente dei dati ha un template standard: in\nquesta tesi non discutiamo dell’importanza dell’OCR nel RAG, quindi i\nrisultati vengono ricavati da testo difatto già estratto, tuttavia è una com-\nponente tanto importante quanto il retrieval e, spesso, il grosso vantaggio\ndei provider come Microsoft non è dato dai Language Model, ma proprio\ndai modelli di OCR che offrono\n•Index: definisce lo schema dell’indice di ricerca. In esso si specificano:\n–i campi testuali (contenuto del chunk, titolo, percorso, ecc.)\n–i campi di metadati (data, autore, categoria, ACL, id documento,\necc.)\n–il campo vettoriale (array numerico) per la ricerca semantica via\nembeddings\n33\n–le impostazioni di ricerca (filtri, faccette, analyzer, e opzioni per\nricerca ibrida/semantica)\nIn un RAG che si rispetti, l’index è spesso ottimizzato per restituire rapi-\ndamente chunk che derivano dalla strategia adottata, con metadati utili a\ntracciare la provenienza\n•Skillset: è il componente che descrive la catena dienrichmentapplicata\nai documenti durante l’ingestion. Sostanzialmente, è un grafo di “skill”\nche trasformano l’input in output indicizzabile; per ’skill’ si intende una\nparticolare trasformazione sul testo estratto: sebbene sembri complesso,\nnon è diverso concettualmente dalla tokenizzazione, stopping o stemming\nclassica che si fà nei motori di ricerca, indipendentemente dal RAG o\nmeno. In un caso come il nostro, le skill più rilevanti sono:\n–Parsing/estrazione: per ottenere testo e struttura dal file;\n–Split/Chunking: per segmentare il testo in unità più piccole;\n–Embedding: per calcolare il vettore associato a ciascun chunk (in\nintegrazione con un endpoint di embedding).\n–Entity Recognition: ricavare entità rilevanti del mondo reale, come\ndel dominio di riferimento, spesso per supportare retrieval che si basi\nsu un Knowledge Graph\nInsomma, lo skillset prende il testo e costruisce l’oggetto che può essere\nindicizzato\n•Indexer: è il “motore” che collega data source, skillset e index. Il funzio-\nnamento può essere riassunto macroscopicamente come segue:\n–legge i documenti dalla data source (Blob)\n–invoca gli skillset per enrichment\n–mappa le informazioni estratte ai campi indicati nell’indice, ovvero\nquelli su cui avverrà la ricerca all’atto pratico\n–scrive i risultati nell’index\n–può essere schedulato o eseguito on-demand\nIl funzionamento è lo stesso, a grandi linee, di quello già discusso, indipenden-\ntemente dal fatto che utilizzi l’SDK di Azure, piuttosto che AWS, Google o che\ntu faccia ’manualmente’ la stragrande maggioranza delle operazioni con Lan-\ngChain; ed il punto è proprio qui: implementativamente, le SDK di provider\nconsentono di orchestrare tutto il ciclo (creazione risorse, aggiornamenti, run\ndell’indexer, query) con un livello di astrazione molto alto che facilita e velociz-\nza la costruzione del prodotto, nonché il mantenimento; d’altronde, nonostante\nsi parli di AI per la maggiore ad oggi, i vantaggi del Cloud Computing sono\nesattamente gli stessi sulla messa in esercizio del prodotto che costruisci.\n34\nIl vantaggio delle SDK diventa, in progetti di ricerca come il mio ma anche\nin applicazioni domain - specific, abbastanza irrisorio ai sensi del fatto che: le\nSDK ti permettono di costruire semplicemente pipeline di RAG che sono già\nstate pensate da chi ha costruito le SDK e diventa più tedioso che costruire\ntutto manualmente andare a definire strategie di chunking personalizzate, come\nalgoritmi o modelli di ranking; difatto richiamiamo il concetto precedente: il\nvero vantaggio dei cloud providers nel RAG è dato solo ed esclusivamente dal-\nl’availability del servizio, non dalle performance che il tuo prodotto raggiunge o\ndalla semplicità di implementazione.\nLa pipeline RAG di Naive “gestita” su Azure implementa tutte le componenti\nprecedentemente discusse: i documenti nel Blob container vengono indicizza-\nti da un Indexer che esegue il cracking ed applica uno Skillset comprendente\nFixed Chunking e calcolo degli embedding context-embedding-ada-002, uno\ndei modelli di Microsoft, proiettando poi il chunk ottenuto sui fields che defini-\nscono l’index. A runtime il retrieval è hybrid (BM25 + ricerca densa ANN sul\ncampo vettoriale) e i top chunk vengono passati al modello con system prompt\nvincolante l’utilizzo unicamente di informazione proveniente dai contesti. Come\nsuddetto, la stessa pipeline è stata emulata con LangChain, dove chunking e tra-\nsformazioni sono completamente personalizzabili per pipeline complessivamente\npiù sofisticate.\nConsiderazioni\nLa pipeline Azure descritta si colloca, quindi, come una soluzione efficace per\navviare rapidamente un sistema RAG e gestire gli aspetti infrastrutturali (stora-\nge, indicizzazione, orchestrazione). Come suddetto, proprio perché costruita in\nmaniera “guidata”, ha abbastanza limitazioni pratiche, che ora descriviamo me-\nglio, quando si richiedono interventi su componenti fondamentali della pipeline.\nIn particolare:\n•Rigidità del pre-processing: molte scelte cruciali (normalizzazione del\ntesto, gestione di tabelle, preservazione della struttura, deduplicazione) ri-\nsultano vincolate alle skill fornite dalla piattaforma; quando emergono esi-\ngenze più specifiche, la complessità implementativa aumenta sensibilmente\npoiché diventa necessario introdurre componenti esterni (ad es. funzioni\no servizi su endpoint dedicati) e orchestrarli nella pipeline\n•Chunking: è relativamente semplice modificare dimensione del chunk o\noverlap entro i limiti delle SDK fornite; è invece complesso implementare\nstrategie avanzate (chunking gerarchico, semantico, recursive...): la mag-\ngioranza dei provider incentiva un chunking “standard”, mentre strategie\npiù sofisticate richiedono generalmente la definizione di funzioni esterne e\nla costruzione di endpoint REST da chiamare all’occorrenza; in Azure, ad\nesempio, l’implementazione di strategie di chunking diverse dalla Fixed\nrichiede generalmente la costruzione di unaCustom Skillsall’interno di\nuno Skillset Azure AI Search: in questi casi, è necessario creare unaAzure\n35\nFunctionche ospiti la logica di chunking personalizzata (utilizzando ma-\ngari librerie come LangChain o modelli di embedding di Azure OpenAI)\ne registrarla come un Web Api Skill, richiamando al concetto precedente.\nPer il Fixed la SDK astrae tutto questo; proprio in questo senso discu-\ntevamo prima di come, per applicazioni reali, usare o non usare SDK ha\npiù o meno lo stesso livello di complessità: ovviamente non vuol dire che\ntu non possa appoggiarti ad un provider, semplicemente che gli strumenti\nmessi a disposizione si concentrano prevalentemente su RAG Naive e non\ncoprono la complessità reale delle esigenze\n•Reranking: anche l’integrazione di un reranker introduce spesso vincoli\npratici; con le SDK offerte dai provider si è tipicamente limitati ai modelli\nesposti dallo, mentre l’adozione di un reranker custom richiede orchestra-\nzione esterna (servizio/API), con impatti su latenza, costi e complessità\npiù in generale\nProvider come Azure rendono agevole implementare una pipeline RAG end-to-\nend, ma lo fanno privilegiando un set di scelte standardizzate. Questo è un\nvantaggio in termini ditime-to-first-result, ma può diventare un vincolo quando\nl’obiettivo si sposta all’avere un sistema ottimizzato sul dominio di riferimento.\nRerankerPer irerankerfacciamo un discorso a parte e che serve ad inqua-\ndrare la scelta di fondo che è stata fatta in questo lavoro: non considerarli. Il\nrerankerè un componente che, dopo una prima fase di retrieval (ad esempio\ntop-kvia BM25, vettoriale o ibrida), ricalcola l’ordine di rilevanza dei risultati\ncon un modello Transformer, in realtà, concettualmente simile ai SentenceTran-\nsformer: un reranker valuta coppiequery–documentocon modelli cross-encoder\nproducendo un punteggio di pertinenza più accurato; in realtà, per la maggiore,\nla coppia viene unita in un unico testo e si sfruttano modelli con attention bidi-\nrezionale come gli encoder only, la vera particolarità sta nel fatto che si produca\nuno score di rilevanza tra la query ed il documento ed il concetto è lo stesso\ndelle Contrastive Loss, ovvero insegnare al modello a separare ciò che è rilevante\nda ciò che non lo sia; approcci moderni, ma come per i SentenceTransformer,\nestendono l’idea anche a modelli LLM-based decoder only.\nA mio avviso, il reranker è spessosopravvalutatonel contesto generale di RAG:\nin molti casi migliora metriche di ranking e questo è un vantaggio, tuttavia il\ngain prestazionale non è per forza giustificato dall’introdurre un secondo model-\nlo a valle del retrieval piuttosto che agire sul SentenceTransformer con, magari,\nun tuning più allineato alle esigenze; se i chunk sono mal costruiti d’altron-\nde, un reranker può al massimo scegliere “il meno peggio” tra candidati già\nsub-ottimali. L’inefficienza principale è, però, la stessa che in contesti reali ti\nvincola sull’embedder: con un reranker introduci ulteriore latenza che, se puoi\nspendere, ha sempre più senso farlo sul SentenceTransformer poiché chunk più\nrilevanti di base producono un ranking strutturalmente migliore. In altre pa-\nrole, il reranker può essere utile come rifinitura se i vincoli lo permettono, ma\n36\nraramente dovrebbe essere il primo strumento a cui ricorrere per “aggiustare”\nun RAG con retrieval debole; proprio in questo senso, nel mio lavoro ho deciso\ndi non considerare reranker.\nQueste considerazioni sono supportate da lavori recenti che osservano che il\nbeneficio del reranking non cresce in modo monotono quando il “primo stadio”\nè già forte. Ad esempio, [Chen et al., 2025b] mostrano esplicitamente dimini-\nshing returns: passando a first-stage retrievers più efficaci, la percentuale di\nmiglioramento ottenuta dal reranker tende a ridursi. In maniera molto simile,\n[Jacob et al., 2024] mettono in discussione l’assunzione “reranker sempre me-\nglio”, evidenziando che, quando si scala il numero di documenti da valutare, i\nreranker possono dare miglioramenti iniziali ma poi calare e perfino degradare\nla qualità oltre una certa soglia.\n5.2 Hierarchical RAG\nL’approccioHierarchicalestende il paradigma RAG introducendo una struttu-\nra di indicizzazione e recuperomulti-livello. Quando abbiamo parlato di chun-\nking avevamo discusso di come questi siano, sostanzialmente, viste locali del\ndocumento e, molto spesso, soprattutto in testi come romanzi dove si ha una\nnarrazione e collegamenti anche tra porzioni di testo spazialmente lontane, si\nperda strutturalmente questa proprietà di interazione; difatti, uno dei domini\ndove approcci di RAG Naive performano peggio è proprio quello delle cosiddet-\nte domande MultiHop: queries che possono essere soddisfatte solo collegando\ninformazione che appartiene a porzioni molto distanti di uno stesso documento\no, addirittura, di documenti diversi. L’informazione utile non è, quindi, unifor-\nmemente distribuita e né sempre recuperabile tramite chunk locali: un indice\ngerarchico consente di rappresentare lo stesso contenuto a diversi livelli di astra-\nzione ed ha una fortissima interazione con i concetti classici della teoria dei grafi,\ncome leCommunities.\nGli approcci gerarchici moderni si basano, infatti, sempre sull’interazione tra\nlo stesso set di componenti:\n•chunk testuali comeunità di base\n•cluster o communitiesper aggregare semanticamente porzioni correlate\ndel testo\n•riassunti a diversi livelliquesti fungono concettualmente come esten-\nsione al testo dirollupsu dati strutturati, con l’obiettivo quindi di rap-\npresentare la tematica fondamentale del Cluster/Community e guidare la\nquery verso una porzione dello spazio degli embedding che comprenda un\nnumero minore di documenti rilevanti\nGià da qui si capisce un concetto importante in relazione ai problemi fondamen-\ntali del RAG: l’approccio gerarchico ha il grosso vantaggio di mitigare struttural-\nmente il problema delsemantic collapsenella ricerca, proprio perché i riassunti\n37\nti permettono di instradare la query facendo riferimento ad un set di chunk\nmolto più ridotto di quello a basso livello; ad esempio, se ogni Community è\ncompresa mediamente da10Chunk, allora hai ridotto la dimensione di ricer-\nca di un fattore10: valuti la query sui riassunti di Community e, poi, prendi\ncome corpora solamente l’insieme dei chunk delle top communities ad esempio,\napproccio semplice e molto diffuso. Concatenando operazioni che coincidono,\ncome accennato prima, arollupedrilldown, garantisci non solo di ridurre la\ndimensione del corpora su cui valuti la query, ma anche a priori interazioni tra\nchunk distanti spazialmente o cross - documento.\nA livello operativo, una pipeline Hierarchical RAG separa due problemi:\n1.Costruzione della gerarchia(index-time): oltre alla classica indicizza-\nzione di chunk, si costruiscono nodi “aggregati” scegliendo la strategia di\nrollup (spesso summary based)\n2.Recupero coarse-to-fine(query-time): invece di selezionare unicamente\ntop-kchunk locali, la query viene dapprima valutata sulla dimensione rol-\nled - up si per massimizzare copertura e contesto, poi si scende in drilldown\nper massimizzare la precisione\nCome suddetto, questo approccio è particolarmente performante in tutti quei\ncasi in cui:\n•la domanda richiedesintesioreasoning multi-hopsingle o cross docu-\nment\n•la collezione è molto grande e si vuole mitigare ilsemantic collapse\nOvviamente, molto della bontà di questo approccio si riduce al come venga pro-\ngettata soprattutto la fase di rollup: se si procede summary - based, come tipi-\ncamente avviene, riassunti rumorosi limitano la massima precisione auspicabile\ndal drilldown successivo.\n5.2.1 RAPTOR RAG\nRAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) è\nun’implementazione di Hierarchical RAG in cui la gerarchia assume la forma\ndi unalbero di riassunticostruito in modo ricorsivo; la particolarità rispetto\nad altri approcci, come quello che presenteremo successivamente, sta proprio\nnell’utilizzo di una struttura Tree based piuttosto che un grafo generico, per il\nresto i concetti fondamentali discussi precedentemente sono tutti perfettamente\nallineati con l’idea dell’approccio. L’obiettivo è sempre lo stesso: superare il\nlimite tipico del Naive RAG nel retrieval di soli chunk contigui (piatti), spesso\nnon catturando il contesto necessario per domande che richiedono ragionamento\nsu parti spazialmente distanti o multidocumento.\n38\nCostruzione dell’albero (index-time)Il processo può essere descritto co-\nme una pipeline bottom-up:\n1.Segmentazione iniziale: il documento viene suddiviso in chunk (foglie\ndell’albero) e per ciascuno si calcola un embedding\n2.Clustering semantico: i chunk (o nodi di un livello) vengono raggrup-\npati in cluster basati sulla similarità degli embedding, con l’obiettivo di\naggregare contenuti concettualmente affini; questo approccio appartiene\nalla famiglia di strategie RAG che utilizza direttamente algoritmi di Clu-\nstering e non Community - based, ma concettualmente l’idea rimane la\nstessa ed è unicamente una scelta progettuale\n3.Riassunto astrattivo per cluster: per ogni cluster si genera unsumma-\nrytramite un Language Model; questo summary diventa un nodo “padre”\nche rappresenta la semantica di gruppo, ovvero la dimensione rolled - up.\nIn questo caso, RAPTOR pubblica esplicitamente i system prompt uti-\nlizzati per generare i riassunti: sfruttano template molto semplici, a mio\navviso necessiterebbe di maggior prompt engineering per una fase così\ncritica\n4.Ricorsione: inodi-padrevengonoalorovoltaembeddede(ri-)clusterizzati\nper costruire livelli superiori, fino a ottenere un numero ridotto di nodi ad\nalta astrazione\nIn quest’ultimo caso, è importante sottolineare che l’operazione di rollup co-\nmunque ha associato un certo grado di information loss: combinandole tra loro\nguadagni in mitigazione di tutte le problematiche precedenti, tuttavia potresti\narrivare, oltre una certa soglia, a performare analogamente (se non peggio) a\nRAG Naive. La difficoltà progettuale maggiore di questo approccio sta, proprio,\nnel rapporto trai summaries ed il livello di astrazione target.\nRecupero e generazione (query-time)A query time, RAPTOR sfrutta\nl’albero di rappresentazioni; puoi decidere di fare diverse cose e, addirittura,\nrendere ancor più sofisticato il retrieval stesso: le query non sono tutte uguali,\nalcune potrebbero essere risposte da una semplice full - text search, altre trovano\ngrosso vantaggio nella ricerca semantica, quindi va bene la struttura gerarchi-\nca ma non è tassativo utilizzarla. Tipicamente, l’approccio più comune vuole\npartire dalla dimensione più ad alto livello e scendere nell’albero ad ogni passo,\nsempre valutando itop - knodi per metrica di similarità, come la coseno, fino ad\nottenere un parco di foglie (chunk effettivi) come base documentale ristretta su\ncui valutare la query; nulla ti vieta, però, di partire da livelli intermedi di astra-\nzione e, quindi, di base basarti su un grado di information loss minore in quello\nche è, fondamentalmente, un meccanismo di routing della query in ingresso. In\nentrambi gli scenari, la gerarchia permette una composizione del contesto in cui\ni livelli alti danno direttamente informazione su dove e cosa potresti andare a\ntrovare scendendo a livello più basso e collegamenti tra concetti, mentre i livelli\n39\nprofondi forniscono l’information need stesso; ovviamente, questi vantaggi non\nsono gratis: il tempo di risposta del sistema aumenta proporzionalmente alla\ncomplessità dell’albero, tuttavia, se si legge tra le righe, non abbiamo discusso\ndi reranker o modelli che ancor di più aumentano la latenza complessiva, quindi\nrispetto a Naive RAG commerciali che sfruttano strumenti come reranker l’ap-\nproccio gerarchico ha tempi di risposta comparabili.\nUna considerazione che, in questo caso, ha assolutamente senso fare e che, suc-\ncessivamente, discuteremo anche con i risultati è la seguente: approcci sofisticati\ncomequestohannosicuramentechiarivantaggi, machediventanoevidentiquan-\ndo puoi stimare la tipologia di query che, per la maggiore, ti troverai a dover\nrispondere a runtime. Le prestazioni di un motore di ricerca sono fortemente\ndipese da come l’utente stesso formula il suo bisogno, quindi dalla distribuzio-\nne lessicale della query: per ricerche full - text, infatti, se questa è fortemente\ndisallineata rispetto alla distribuzione lessicale dei documenti il sistema perfor-\nmerà male ed è abbastanza chiaro; grosso vantaggio sta, infatti, nell’adozione\ndi modelli generativi per query rewriting ed è una delle mitigazioni più forti al\nVocabulary Mismatch Problem. Quando, però, hai query molto semplici e che\npossono essere risposte da sistemi di retrieval molto meno complessi di quello\ngerarchico, tipicamente vedi che: più è sofisticato il sistema che progetti e peggio\nperforma nelle query semplici, che comunque sono sempre una sotto popolazione\nmolto densa della totalità delle query in ingresso. Difatti, quello che rende il\nNaive RAG una baseline difficile da battere, se guardi sempre al concetto di go\n- to solution, sta proprio nel fatto che, con le dovute considerazioni, comunque\nriesce a soddisfare bene l’information need medio e la struttura gerarchica si\nperde nei casi semplici: per questo, adottare spesso classificatori sulla tipologia\ndella query iniziale, indicanti la difficoltà della stessa, permette di poterla valu-\ntare direttamente sulla base documentale a livello più profondo della gerarchia,\nquella complessiva quindi, e senza ridurla dimensionalmente, proprio perché la\nparte full - text dello score ibrido è predominante.\n5.2.2 Microsoft GraphRAG\nMicrosoft GraphRAG propone un’altra forma di Hierarchical RAG, in cui la\ngerarchia non è un albero di riassunti costruito solo da clustering, ma una strut-\nturaa grafocentrata su entità e relazioni estratte dal testo, quindi un qualcosa\ndi strutturalmente più vicino all’approccio che andrò a proporre. L’ipotesi di\nfondo è sempre la stessa, tuttavia qui si fa riferimento ad uno dei concetti prin-\ncipe nel retrieval moderno e che, storicamente, ha fatto la fortuna di motori di\nricerca come Google: i Knowledge Graphs.\nKnowledgeGraphsUnKnowledge Graph(KG)èunarappresentazionestrut-\nturata della conoscenza in cui le informazioni vengono modellate come un insie-\nme dientità(nodi) erelazioni(archi) tra di esse. Formalmente, un KG può\nessere visto come un grafo etichettato che rispetta un determinato modo di co-\nstruire le suddette etichette: spesso tramite triple del tipo(soggetto, predicato,\n40\noggetto), ma più generalmente si parla diOntologieche potrebbero avere an-\nche una forma diversa dalla precedente. Questo consente una transizione da un\ncorpus puramente testuale a una descrizione esplicita di “chi è chi” e, sopratut-\nto, “come le cose sono collegate”, rendendo il contenuto non solo interrogabile a\nlivello di similarità semantico-lessicale, ma soprattutto per connessioni logiche.\nLeproprietàrappresentano gli attributi e le relazioni checaratterizzanoun’en-\ntità, intesa come un oggetto del mondo reale (persone, luoghi, organizzazioni,\nconcetti); in un KG, esse non si limitano a descrivere l’entità, esprimono anche\ni legami tra le stesse, rendendo spesso molto più semplice soddisfare anche in-\nformation need complessi.\nApplicativamente, è facile ora capire come i Knowledge Graphs rispondano ad\nun’esigenza fondamentale del retrieval: introdurre un livello di rappresentazione\npiù stabile del testo e che permetta di ’navigare’ in maniera strutturata l’infor-\nmazione, espandere la query aggiungendo contesti, dipendenze, cause-effetti, ge-\nrarchie e associazioni... anche quando tali collegamenti non emergono in modo\nevidente con un retrieval a chunk; infatti, puoi rispondere sì alla query navi-\ngando direttamente sul grafo se, come nel caso che presenteremo poi, questo è\nun grafo di chunk, tuttavia, come nelquery rewriting, puoi anche sfruttarlo per\nrisolvere l’ambiguità che tipicamente c’è in query reali a motori di ricerca, come\nquelle presenti nel dataset NQ discusso nei capitoli precedenti. Come già accen-\nnato, non tutti i vantaggi vengono ’gratis’: i KG hanno, infatti, costi di gestione\nspesso e volentieri proibitivi, sopratutto quando il tuo dominio è particolarmen-\nte complesso e necessita la modellazione di tante entità e concetti distinti; i\ndue processi fondamentali sono quelli diEntity ExtractionedEntity Linking:\npartendo da una base testuale, hai bisogno di un modo per automatizzare sia\nl’estrazione effettiva delle entities e delle proprietà, nonché il collegamento ad\noggetti già esistenti nel KG corrente; si parla quindi didisambiguazioneper\nl’appunto, oltre che di normalizzazioni all’ontologia dell’informazione che estrai.\nL’Entity Extraction e Linking sono due processi complicatissimi, forse trai più\ncomplicati nel retrieval: esistono modelli di NER, anche basati su Transformer\no LLM, che ti permettono buone prestazioni sul tuo dominio, ma spesso e vo-\nlentieri non sono competitivi in zero - shot; per capire davvero bene non tanto\nle entities, ma più cosa definisce una proprietà relativamente ad essa nel tuo\ndominio, hai bisogno di modelli fine - tuned allenati su tanti esempi etichettati\nditestoecoppie estratte, a meno che tu non voglia ottenere performance sempre\nsubottimali. D’altronde, una ricerca che si fonda interamente sulla navigazione\ndel KG ha prestazioni che dipendono criticamente da Entity Extraction e Lin-\nking: se non hai una buona F1 - Score e sei, ad esempio, sbilanciato sulla Recall\npiuttosto che sulla Precision, allora o rendi non ricercabile una data informa-\nzione contenuta nel testo o rendi ricercabile informazione rumorosa, incompleta\no addirittura fuorviante. L’approccio costruito successivamente, infatti, si basa\nsu un grafo di chunk e non di entities: se cerchi soluzioni go - to nel RAG e non\nti basi su un dominio in analisi, allora fare NER o, più generalmente, popolare\nun KG perde totalmente di senso per i motivi suddetti.\n41\nIndexing e CommunitiesLa pipeline di indicizzazione di Microsoft Graph-\nRAG può essere schematizzata in tre fasi operative:\n1.Costruzione del Knowledge Graph: il corpus viene prima segmen-\ntato inTextUnits(chunk con eventuale overlap) che fungono da unità\nfondamentale di estrazione. Su ciascun TextUnit, un LLM esegueEntity\ne Relationship Extractionproducendo un sottografo locale con: entità de-\nscritte datitle,typee una o più descrizioni generate dal LM stesso sulla\nbase del ground testuale, relazioni direzionali tra coppie di entità, ciascu-\nna con una o più descrizioni testuali. I sottografi vengono poi fusi in un\ngrafo globale con una deduplicazione deterministica: entità con lo stesso\ntitle+typevengono unificate (accumulando le descrizioni), e analogamente\nle relazioni con la stessa coppia (source,target) vengono aggregate. Per\nridurre rumore e ridondanza, GraphRAG applica una fase LLM disum-\nmarizationche comprime l’insieme delle descrizioni accumulate in una\ndescrizione canonica per entità e per relazione, molto simile ai concetti\nprecedenti di rollup\n2.Community detection e gerarchia multi-livello: una volta ottenuto\nil grafo Entities/Relationships, GraphRAG esegue una partizione incom-\nmunitiestramiteHierarchical Leiden, uno degli algoritmi più famosi di\nCommunity Detection, ottenendo cluster di nodi densamente connessi e\nuna gerarchia ricorsiva che rispetti un certo limite alle dimensioni; questa\nla parte Hierarchical dell’approccio Microsoft GraphRAG, d’altronde, nel\nlavoro che presenteremo successivamente, ci rifaremo proprio all’algoritmo\ndi Leiden\n3.Community reports: per ogni community e per ogni livello gerarchi-\nco, GraphRAG generacommunity reportse una versione ulteriormente\ncompressa (shorthand) che sintetizzano concetti chiave di entità centrali,\nrelazioni salienti o, più generalmente, la semantica della comunità. Pa-\nrallelamente, vengono calcolati embedding perTextUnits, descrizioni di\nentità/relazioni e report, così da abilitare retrieval semantico sia sul testo\nnon strutturato sia su ciò che ha costruito il Language Model\nModalità di interrogazione (query-time)GraphRAG distingue tipica-\nmente due strategie complementari:\n•Local Search: adatta a domande entity-centric o focalizzate. Si parte da\nentità rilevanti per la query e si recupera un intorno informativo nel grafo\n(nodi correlati, relazioni, descrizioni) combinandolo con i chunk testuali\ncorrelati per arricchire il contesto; questa è la parte più simile alla ricerca\ndrilled - down precedentemente spiegata\n•Global Search: adatta a domande di sintesi sul corpus. La risposta viene\ncostruita in modo simile ad un approccio di RAG molto famoso: HyDE\n42\n(Hypotetical Document Embeddings), dove, invece di fare retrieval usando\ndirettamente la query, l’LLM genera prima un “documento ipotetico” che\nrisponderebbe alla domanda e si usa l’embedding di quel testo per cercare\ni documenti reali più simili; in Microsoft GraphRAG il concetto è molto\nsimile: il LLM genera una risposta parziale (key-points) utilizzando i top\n- Communities summaries, ottenuti anche percorrendo diversi livelli della\ngerarchia, poi produce una risposta finale infierendo sulla query iniziale\ntramite ikey-pointsestratti; si noti che, nel caso di Global Search, non\nsi valuta drilldown perché ti interessano informazioni ad alto livello come\npresupposto\nAnche se GraphRAG presenta principalmente Global e Local Search, la dispo-\nnibilità di un KG e di una gerarchia di community permette naturalmente le\nstrategie coarse-to-fine discusse precedentemente: selezione di community rile-\nvanti, identificazione di entità/relazioni candidate e successivo drill-down verso\nTextUnits come evidenze, ottenendo di fatto una ricerca multi-step guidata dal\ngrafo.\nMicrosoft GraphRAG è interessante poiché spinge su una costruzione LLM-\ncentric dell’indice: il Language Model estrae entità e relazioni dai chunk e ne\nsintetizza le descrizioni, ottenendo un grafo “navigabile” e utile per retrieval; tut-\ntavia questo design porta anche diversi problemi pratici. Oltre ciò che abbiamo\ndiscusso sul zero-shot, nella pipeline standard non emerge una vera e propria\nfase diEntity Resolutioncross-document: il merge è principalmente determini-\nstico (entità con stesso title+type vengono aggregate), quindi varianti nominali,\nsinonimi o ambiguità tra documenti possono non corrispondere alla stessa entità\n“reale”, frammentando il grafo e incentivando duplicazioni nelle Communities,\ncosa che non necessariamente vuoi; questo è un problema non ancora risolto\ndagli autori e che ho potuto evidenziare studiando alcune delle issues aperte\nsulla repository GitHub del codice.\nTutteleinformazionifondamentalisonostatepresedirettamentedallostudiodel\ncodice pubblico fornito dagli autori e il paper di riferimento [Edge et al., 2024].\n5.2.3 Semantic Chunk Graph\nL’approccio diSemantic Chunk Graphproposto si ispira alle due alternative\nprogettuali discusse precedentemente: RAPTOR e Microsoft GraphRAG. Da\nMicrosoft GraphRAG riprende l’idea di sfruttare la struttura graph-based e di\nfornire una ricerca coarse-to-fine con rollup tramite communities (ottenute con\nclustering di tipo Leiden), così da soddisfare l’idea fondamentale alla base degli\napprocci gerarchici. Tuttavia, più similmente a RAPTOR, non introduce un\nlivello esplicito di entità e relazioni estratte: i nodi del grafo sono direttamente\ni chunk testuali e gli archi rappresentano relazioni semantiche tra chunk (es. si-\nmilarità coseno), piuttosto che relazioni spaziali di vicinanza nel documento; la\nmotivazione di questo deriva dal discorso fatto precedentemente sul zero - shot\n43\ndei Language Models nella costruzione del grafo. Le strutture gerarchiche e i\npassaggi di raffinamento operano, quindi, sul testo segmentato e sulle sue aggre-\ngazioni, mantenendo il grounding sui contenuti originali. Riguardo la strategia\ndi chunking: ho adottato, coerentemente a quanto discusso nei capitoli prece-\ndenti, la strategia che trovo concettualmente più vicina all’idea di soluzione go\n- to, ovvero il Recursive Chunking.\nL’idea di partenza del mio lavoro è ottenere alcuni dei benefici dei moderni\nsistemi GraphRAG senza passare per la costruzione di un knowledge graph a\nentità e relazioni: una scelta che, in un corpus generico e potenzialmente multi-\ndominio, richiederebbe un’ontologia e una fase di estrazione/linking robusta\nche qui non è disponibile. Per questo adotto una rappresentazione più legge-\nra e dominio-agnostica: il grafo è costruitoa livello di chunk testuali, non di\n“concetti”. Ogni nodo rappresenta un passaggio (paragrafo o finestra di alcune\ncentinaia di token), mentre gli archi codificano due famiglie di relazioni comple-\nmentari: (i) similarità semantica calcolata tramite embedding e (ii) contiguità\nstrutturale nel documento.\nDal punto di vista della costruzione, il punto di partenza coincide con un RAG\ntradizionale: i documenti vengono chunkati (Recursive Chunking), per ciascun\nchunk si calcola un embedding (famiglia BAAI/BGE) e gli embedding vengono\nindicizzati in un vector store. La differenza emerge nel passo successivo: inve-\nce di usare il vector store solo per un retrieval top-k, gli embedding vengono\nsfruttati per costruire ungrafo di similaritàtra chunk. Per ogni nodo si se-\nlezionano i vicini più affini imponendo una soglia alta (ad es. cosine similarity\n≥0.85), mantenendo nel grafo solo archiSIMILAR_TOche considero affidabili e\nfiltrando a monte connessioni deboli o rumorose. In parallelo, vengono aggiunti\narchi strutturaliNEXTtra chunk consecutivi dello stesso documento: non espri-\nmono somiglianza di contenuto, ma preservano la sequenzialità e consentono di\nrecuperare contesto locale anche quando la similarità non è sufficiente.\nIl grafo così ottenuto viene proiettato in Neo4j Graph Data Science come grafo\nnon pesato: la scelta non è un dettaglio casuale, ma discende dal fatto che la\nselezione “forte” sui collegamenti avviene già nella fase di design tramite la soglia\nsugli embedding. In altre parole, sposto la complessità sulla costruzione della\nstruttura (quali archi esistono) più che sulla funzione di transizione del random\nwalk (come pesare gli archi). Su questa struttura si applicano: (i) algoritmi di\ncentralitàoffline(PageRank)peridentificarenodirappresentativie(ii)algoritmi\nquery-dependent a runtime (Personalized PageRank) per guidare il retrieval.\nPer introdurre uno strato gerarchico sopra il grafo di chunk, applico una commu-\nnity detection (Leiden) sull’intera rete, ottenendo insiemi di chunk fortemente\ncollegati (per similarità e contiguità). Ogni community tende a catturare un\ntema locale del corpus: un insieme di passaggi che trattano lo stesso argomento\no sotto-argomento con variazioni. All’interno di ciascuna community uso Page-\nRank globale per selezionare i chunk più centrali e, a partire da questi, genero un\nbreve riassunto tramite un LM: il risultato è un insieme di nodiCommunitycon\nuna descrizione testuale che funge da rappresentazione rolled-up della regione\ncorrispondente del grafo.\n44\nLa differenza rispetto a un RAG tradizionale emerge nella fase di query. A\nruntime, ilsistemapartecomunquedaunpassovettoriale: embeddoladomanda\ne interrogo il vector store per ottenere un insieme ristretto diseed chunk. Questi\nseed non sono ancora il contesto finale, ma costituiscono il punto di iniezione\ndel segnale nel grafo: vengono usati come nodi sorgente di un Personalized\nPageRank, che propaga la rilevanza lungo le connessioni (con restart continui\nsui seed) fino a produrre una distribuzione di punteggi sututtii chunk. Il\nsignificato operativo è che la query non attiva solo i vicini diretti in embedding\nspace, ma anche passaggi connessi ai seed tramite cammini multi-hop, spesso\nutili quando l’informazione è distribuita o quando i passaggi rilevanti non sono\ntutti ai primissimi posti per similarità diretta.\nA valle della PPR, si ottiene anche un segnale a livello di community aggregando\ni punteggi dei chunk percommunity_id: questo consente di identificare quali\nregioni tematiche del grafo siano globalmente più attivate dalla domanda, utile\nper domande di sintesi e per una composizione del contesto che includa sia una\nvista globale sia evidenze puntuali.\nSelezione finale del contesto (policy ibrida).La selezione dei contenu-\nti da inviare all’LLM combina tre criteri con un budget massimo di chunk\n(max_context_chunks), evitando di “buttare via” nodi con PPR alto solo per\nvincoli di appartenenza a una community. In ordine di priorità: (i) si include\nanzitutto un sottoinsieme deiseedche risultano anche ben supportati dal grafo\n(buon punteggio PPR), preservando la coerenza con la similarità diretta query–\nchunk; (ii) si aggiungono poi i chunk con punteggio PPR più altoglobalmente,\nindipendentemente dalla community, massimizzando il contributo della diffusio-\nne multi-hop; (iii) infine, si applica un vincolo didiversità per community: se\nle community più rilevanti (da PPR aggregata) risultano sottorappresentate, si\nforza l’inclusione di alcuni chunk appartenenti a tali community per garantire\nuna vista equilibrata. In questo modo, la policy è effettivamente ibrida: pri-\nvilegia i chunk più alti per PPR, ma mantiene (quando utile) sia la copertura\ndei seed sia la rappresentazione delle regioni tematiche principali, ottenendo\nun contesto local-global più stabile rispetto a una selezione puramente top-k\nvettoriale o puramente community-based.\n5.2.4 Chunking e Communities\nConsiderazioni\n5.3 CoT RAG\nL’approccio diCoT RAGimplementato estende il Naive RAG introducendo un\nagenteLLM che guida iterativamente il retrieval tramite, come suggerisce il\nnome, una forma diChain-of-Thoughtmaterializzata in unoscratchpad. L’idea\nnon è (solo) rendere la ricerca step wise, maadattiva: invece di eseguire un\nsingolo retrieval top-ksulla query originale, il sistema alterna step di retrieval,\naggiornamento della ’memoria di ricerca’ data dallo scratchpad e decisione di\n45\nstop oppure generazione di una nuova query mirata a colmare l’informazione\nmancante; l’adattività ed il concetto CoT sta, proprio, in questa generazione\ndi sub-queries: il modello si concentra nel cercare solamente ciò che gli serve\nper soddisfare l’information need ed adattivamente rispetto alle informazioni già\ntrovate; questo permette di costruire, iterativamente, queries molto più puntuali\ned aumentare la precisione della risposta, con buona flessibilità rispetto la com-\nplessità della query iniziale in realtà.\nIn letteratura, questa famiglia di metodi ha una forte interazione con approc-\nci che si basano sull’interleavingtra reasoning e retrieval (ad es. IRCoT) o\nche formalizzano la decomposizione in sotto-domande e l’uso di strumenti ester-\nni (ad es. Self-Ask, ReAct), dove lo “stato” intermedio funge da guida per la\nricerca successiva; molto delle scelte progettuali alla base dell’approccio so-\nno ispiratr a lavori abbastanza recenti [Trivedi et al., 2023, Press et al., 2022,\nYao et al., 2022], in particolare: [Nye et al., 2021]. Nel codice, tutto viene or-\nchestrato con LangChain (template, parsing, costruzione della risposta) e l’end-\npoint di Groq per interagire con il modello scelto. Un aspetto interessante è che\nil CoT RAG è agnostico a come organizzi il corpora: puoi, o meno, utilizzare\nstrutture gerarchiche e valutare la ricerca adattiva su queste, ciò dipende dalle\nscelte progettuali; la pipeline prodotta è, infatti,backend agnostice ti permette\ndi indicare direttamente se stai lavorando su un grafo o meno. Ci torneremo\nsuccessivamente nelle considerazioni di questa sezione.\n5.3.1 Scratchpad\nLoscratchpad, come accennato precedentemente, è una memoria testualeac-\ncumulativache rappresenta lo stato corrente di ciò che il modello “conosce” in\nmodogrounded; insomma, la storia del retrieval fino a quel momento. Ad ogni\niterazione, l’agente LLM riceve: la domanda originale, lo scratchpad corrente,\nuna history breve degli step recenti (query emesse e doc-id recuperati) e le nuove\ninformazioni ottenute dal retreival, con cui aggiornare lo scratchpad e costruire\nle query per il passo successivo. Essendoci abbastanza passaggi da gestire, la\nmaniera più coerente è che il LM produca un output strutturato (JSON) con\ncui si facilita l’estrazione delle singole componenti:\n•unaggiornamentodello scratchpad (fatti in bullet-point derivatisolo\ndalle evidenze)\n•una decisione booleana distop(enough) se l’evidenza è sufficiente a\nsoddisfare l’information need\n•una descrizione dicosa manca(missing) se così non fosse\n•unanext queryunica e mirata per il passo successivo\nQuello che, personalmente, mi piace molto di quest’approccio è la forte intera-\nzione con il vero motivo per cui oggi si utilizzano estensivamente i Language\n46\nModels: il dinamismo nel Question Answering, la gestione naturale dei follow-\nup e, più in generale, la forte componente d’interazione che c’è utente-modello.\nValutando l’approccio, ho spesso salvato l’evoluzione dello scratchpad a fini di\naudit e, tralasciando piccole imprecisioni che puoi risolvere con un po’ di prompt\nengineering, emergeva chiaramente proprio questa adattività: la ricerca veniva\n“guidata” verso query sempre più puntuali, man mano che il sistema compren-\ndesse lo stato corrente dell’informazione e ciò che mancasse raffinando le query\nsuccessive.\n5.3.2 Step-Back Prompting\nLoStep-Back Promptingè una tecnica in cui, prima di avviare il retrieval, si\nchiede al modello di riscrivere la domanda ad un livello di astrazione superio-\nre, producendo una query più generica che catturiconcettoedintento. L’idea\nè sempre quella di rollup e drilldown classica degli approcci gerarchici, in una\nforma più ’light’ e semplice da realizzare.\nL’approccio CoT RAG che ho presentato prima, in realtà, deriva da una va-\nriante iniziale che concettualmente sfruttava proprio lo Step-Back Prompting\ncon l’adozione di unPlanner: si genera unastepback_questione unintent,\npoi un planner produce un piano di 2–4 step con sub-questions e query iniziale,\naggiornaloscratchpadedecidestopofollow-up. Rispettoall’approcciodescritto\nprecedentemente, qui il piano viene definito ’a priori’: gli step vengono costruiti\nprima della fase di retrieval e, quindi, prima di poter attingere effettivamen-\nte ai chunk, mentre l’approccio adottato si basa direttamente su ’cosa manca’\nrispetto all’informazione effettivamente raccolta; il Planner iniziale vincola mag-\ngiormente la direzione della ricerca. L’intuizione alla base viene, proprio, dallo\nStep-Back Prompting: l’idea era quella di guidare il modello nel costruire query\na diversi livelli di specificità, partendo da una step-back iniziale e scendendo più\nnel dettaglio con query successive. L’impostazione adottata è, però, più vicina\nal concetto di retrievaladaptivee, infatti, è stata preferita; tuttavia, quando\ndiscuteremo successivamente diquery drift, la versione con Planner è molto\npiù robusta a tale fenomeno, nonché generalmente più cost-efficient, perdendo,\nperò, proprio sull’adattività della ricerca.\nNell’approccio Plannerless non è stato introdotto esplicitamente lo Step-Back\nPrompting, poiché il beneficio di “cambiare scala” nella formulazione delle query\nè, in linea di principio, garantito strutturalmente dall’adattività stessa: la CoT\npuò guidare sia query progressivamente più specifiche quando manca informa-\nzione, sia generalizzazioni quando serve contesto o quando i tentativi troppo\nspecifici falliscono. Ad inferenza, però, il comportamento più comune e natu-\nrale è una sequenza di query sempre più specifiche; tuttavia, nulla ti vieta di\naggiungere dei fallback dove, ad esempio, doponstep senza miglioramento si\ngenera una query più astratta (step-back) per recuperare background o riorien-\ntare la ricerca. Il CoT RAG è un approccio interessantissimo proprio perché\nchi progetta ha molta libertà di scelta e, d’altronde, progettualmente hai tan-\n47\nte alternative con molto potenziale: vantaggio strutturale, anch’esso, di tutti i\nprodotti LLM-centric.\nConsiderazioni\nNel Naive RAG la query è valutatasingle-shoted il retrieval top-kè direttamen-\nte ciò che condiziona la generazione. Nel CoT RAG, invece, la query diventa\nunprocesso: l’LLM usa lo scratchpad per capire se l’evidenza è sufficiente e,\nin caso contrario, produce query successive più specifiche. L’effetto atteso è un\nmiglioramento generale su query di medio-alta complessità grazie alla CoT, co-\nme maggior robustezza alla variabilità lessicale: d’altronde, la scrittura di query\npiù specifiche è strutturalmente un query rewriting. In questo senso, non è diffe-\nrente l’idea rispetto a quella presentata nei capitoli precedenti, ma l’esecuzione:\nnon puoi aspettarti necessariamente di utilizzare un modello fine-tuned, a meno\nche non utilizzi diversi language models; sarebbe stato interessante scomporre\nl’agente in sub-agents più piccoli, con uno SLM, ad esempio, specializzato in un\nrewriting coerente alla distribuzione lessicale del dominio, tuttavia, non concen-\ntrandomi su un dominio specifico, è una strada che non ho percorso.\nQueste capacità adattive richiedono, tipicamente, modelli di dimensione medio-\ngrande: serve una forte comprensione testuale, conoscenza generalista, buone\nabilità di reasoning, sintesi, riscrittura... e qui arriva la nota dolente: ci sono\ntantissimi vantaggi e, progettualmente, l’impostazione non è così più complicata\ndel RAG Naive, tuttavia il costo ad inferenza cresce non solo perché usi modelli\nmediamente più costosi, ma soprattutto perché una singola domanda può inne-\nscare molti sub-step nella CoT; ogni step consuma token sia per il reasoning che\nper la gestione delle fasi intermedie (scratchpad update, query successive...) e,\nsi noti, sono comunque token generati, quindi quelli di ’costo massimo’. Prima\naccennavamo al concetto dibackend agnostic, tuttavia i risultati che presen-\nteremo non si basano su strutture gerarchiche per l’organizzazione del corpus:\nin questo modo è vero che non mitighi le problematiche fondamentali del re-\ntrieval, ma d’altro canto il costo che avresti nella gestione di tali strutture si\nandrebbe a sommare ad una componente già molto alta ad inferenza per i mo-\ntivi precedentemente discussi e, detta molto semplicemente, avresti un tradeoff\nperformance-costo estremamente sbilanciato, o meglio, tutti i vantaggi che puoi\nderivare dalla combinazione Hierarchical + CoT non sarebbero giustificati da\nquanto spendi per metterli in pratica; in casi reali quindi, spesso con forti vin-\ncoli sui costi, si sceglierebbero sempre alternative di Naive RAG come miglior\ncompromesso.\nQuery DriftOltre che le considerazioni sui costi , l’adattività cela un ri-\nschio importante diquery drift: se il controller interpreta male cosa manca,\npuò generare query che deviano dal bisogno informativo originale, accumulan-\ndo evidenze “plausibili” ma non pertinenti; questo è un modo informale per\nevidenziare ancor di più quello che dicevamo prima: hai bisogno mediamente\ndi modelli grandi e, soprattutto,instruction-tuned. Si definisconoinstruction-\n48\ntunedLanguage Models che contano una fase di post-training dove vengono\nallenati a rispondere in maniera coerente alle richieste ed i vincoli esposti dell’u-\ntente, formulati tramite prompting, riducendo proprio le stesse deviazioni che\nsono alla base del query drift; se insegni/indichi al modello come ragionare e\ncome formulare le query tramite prompting, di base vuoi che questo commetta\nmeno drift possibili rispetto a ciò che gli chiedi e, nel CoT RAG, è molto più\nvero che in tutte le altre architetture dati i molti passi intermedi nel processo di\nretrieval. Questo, in realtà, è un concetto importante indipendentemente dal-\nl’architettura RAG: per garantire che il modello utilizzi solamente il contesto\nche ricava dal retrieval, ad esempio, tipicamente glielo si chiede proprio come\nsystem prompt (Use only the provided context. If the context is insufficient, say\nso explicitly) e se il modello non è instruction-tuned, di fondo, il RAG stesso\nnon ti da le stesse garanzie. In realtà, quello che stiamo discutendo è un failure\nmode noto dei metodi reasoning+tool-use: tracce convincenti, ma errate come\nle decisioni prese dall’agente, molto discussi nella pratica in linee come ReAct\n[Yao et al., 2022]. Non ci sono metodi di mitigazione esatti a questo fenomeno,\ndifatti l’implementazione fornita include meccanismi che si basano su ragiona-\nmenti puramente euristici: l’intuizione su cui mi sono basato è che il drift, se\navviene, è molto più probabile nasca dopo un certo numero di step di adaptive\nretrieval, ovvero presuppongo che l’information need della query iniziale possa\nessere mediamente chiaro al modello e l’errore nella generazione di query avven-\nga, proprio, quando devi stimare informazioni particolarmente specifiche; questo\nmi ha portato al concetto di history: se valgono le intuizioni precedenti, allora\nle prime query progettate dal modello puoi supporre siano coerenti, di conse-\nguenza, dandogli una finestra di2−3query precedenti, il modello ha sempre\nun’ancora contestuale che funge da ’reminder’ del modo corretto di modellare\nl’informazione mancante, come una sorta di few-shot-learning.\n5.4 RAG Datasets\nNel contesto di pipeline RAG, un limite pratico alle valutazioni è l’assenza di\nuna standardizzazione forte e di benchmark “definitivi”: molte risorse disponi-\nbili online sono, di fatto, benchmark diretrievalorankingpiù che valutazioni\nend-to-end di una pipeline RAG; ai sensi di questo, infatti, la stragrande mag-\ngioranza dei dataset distribuiscono già il testo pre-chunkato, semplificando sì le\nvalutazioni ma rendendo le metriche che ne ricavi molto meno realistiche dando\ngià ’per buona’ una delle componenti più critiche della pipeline, oltre che impe-\ndendo di misurare gli effetti delle scelte che fai su strategia ed iperparametri. In\ngenerale, appunto, la letteratura propone numerose metriche per retrieval e ran-\nking: Precision, Recall, F1, NDCG, MRR, ecc.. a mio avviso, queste metriche\nsono complessivamente sopravvalutate in contesti di retrieval e, in un setting\nRAG come il mio, ancora di più: non essendoci uno scenario domain-specific\ned adottando soluzioni il più possibilmente plug-and-play (vedi SentenceTran-\nsformer), la subottimalità del ranking che ottengo è un presupposto già noto a\npriori e non ho bisogno di misurarlo, oltre che metriche come Precision e Recall\ntendono a sovrastimare aspetti “meccanici” del recupero rispetto all’obiettivo\n49\nreale del sistema. In un RAG, infatti, ciò che conta primariamente è soddisfare\nl’information needdell’utente in modo corretto e supportato dal contesto; per\nquesto, la valutazione è formulata come judging rispetto ad una gold answer:\nverificando sia l’allineamento informativo, sia la correttezza rispetto ai contesti\nrecuperati, indirettamente otteniamo anche un segnale sulla qualità del retrie-\nval; d’altronde, se la risposta è corretta e vincolata alle informazioni grounded,\nil sistema ha necessariamente recuperato chunk utili trai primi posti e, quindi,\nottenuto un buon ranking.\nA livello implementativo, il judge riceve: la domanda, la gold answer (tipica-\nmente concisa), la risposta del modello (tipicamente più verbosa) ed il contesto\nrecuperato; poi, assegna tre giudizi:\n•answer_correctness, inteso come correttezza complessiva della risposta\nrispetto alla domanda e alle informazioni della gold answer; può essere\nformulata diversamente o più in dettaglio, ma non deve introdurre errori\no contraddizioni\n•evidence_coverage, inteso come copertura dei fattinecessaridella gold\nanswer ed intendibile come una sorta checklist di entità o dettagli obbli-\ngatori (nomi, date, luoghi, relazioni...). Questa viene dal fatto che, ge-\nneralmente, una risposta può essere complessivamente corretta, o meglio,\n’giusta’ rispetto alla domanda che è stata posta ma non soddisfare comple-\ntamente tutte le informazioni che ci si aspettava di trovare; infatti, proprio\nquesta ci fornisce una componente più retrieval-oriented, collegandoci ai\nragionamenti di sopra\n•faithfulness_to_context, inteso come allineamento tra le affermazioni\nnel contesto e nella risposta, penalizzando anche il caso in cui la risposta\nsia corretta ma non supportata dal retrieval\nUn punto critico su cui mi sono concentrato nel prompt è la mitigazione del\nbias verso risposte brevi: poiché la gold answer è spesso un riassunto fattuale,\nil judge viene istruito anon penalizzarerisposte più lunghe se e solo se le in-\nformazioni aggiuntive sono rilevanti, supportate dal contesto e coerenti con la\nreference; benché si utilizzi un modello LLM (grande, meno bias prone) come\njudge, parliamo di GPT-4o, comunque è un guardrail che, a mio avviso, rimane\nnecessario. La scelta di GPT-4o e non di, magari, modelli più stato dell’arte\ncome Gemini 3, GPT 5.2, Claude Opus 4.6... viene dal fatto che il LLM jud-\nging è tanto una pratica ’nuova’ quanto lo sono i Language Models e GPT-4o\nè uno dei judge più usati e testati nella letteratura, quindi complessivamente è\nun’alternativa affidabile a parità di costo.\nSebbene il prompt produca valori numerici in[0,1], la scala è intenzionalmente\ncategoriale (0.0, 0.5, 1.0): è concettualmente equivalente a etichette discre-\nte (“pessimo”, “buono”, “ottimo”), ma mi evitava una mappatura ulteriore tra\ncategorie e numeri senza cambiare la logica della valutazione; è importante\n50\nsottolineare questo, altrimenti potrebbe sembrare ci sia una discrepanza con i\nragionamenti fatti nell’introduzione, dove intimavo di come sia sempre meglio\nfar generare etichette categoriche, piuttosto che corrispettivi numerici, al judge.\n5.4.1 GraphRAG Bench\nIl datasetGraphRAG-Bench/GraphRAG-Bench, disponibile su Hugging Face, è\nstrutturato in due configurazioni principali:novelemedical, ciascuna accom-\npagnata da un file dedicato di domande. Il benchmark nasce con l’obiettivo\nesplicito di confrontare approcci RAG “tradizionali” e variantigraph-basedlun-\ngo l’intera pipeline, dalla costruzione della struttura (grafo) al retrieval, fino alla\ngenerazione, includendo task eterogenei e a difficoltà crescente, come: semplice\nfact retrieval, reasoning multi-hop, summarization e creative generation.\nMi sono concentrato sulla porzioneNoveldel dataset e gli iniziali risultati che\npresenteremo fanno riferimento a questo; la formulazione delle query è spesso\nmarcatamenteentity-oriented: molte domande citano persone, luoghi o ogget-\nti direttamente; questa caratteristica ha una conseguenza pratica importante:\nmodelli di retrieval lessicale (BM25) o ibridi (lessicale + denso) risultano di base\nestremamente efficaci, poiché possono agganciarsi a termini altamente discrimi-\nnanti già presenti nella domanda, riducendo la necessità di un vero reasoning\nmulti-hop. Sostanzialmente, il segnale lessicale è spesso già sufficiente a indi-\nviduare un insieme di chunk rilevanti nel caso generale e la componente densa\ntende a fungere più da ausilio che da fattore determinante. A mio avviso, in-\nfatti, il dataset non è un benchmark tanto buono: la complessità effettiva di\nalcune istanze etichettate comeComplex Reasoning, ovvero le query con massi-\nma hardness nel dataset, spesso vengono risposte con, ad esempio, una CoT che\nsi ferma al primissimo step, quindi di fondo un Hybrid Search Naive classico: la\npresenza esplicita di entities nella query è direttamente collegata a questa ten-\ndenza, essendo che match dell’entità→chunk rilevante→risposta, senza che\nemerga in modo netto il vantaggio di strategie adattive o gerarchice. Tuttavia,\nl’orientamento entity-centric è coerente con la finalità del benchmark: in un set-\nting in cui il seed è facilmente individuabile, diventa più facile misurare quanto\nun approccioGraphRAGriesca a sfruttare relazioni e struttura per rispondere\nall’information need; questo, però, è vero se l’approccio graph è entity-oriented,\ncome Microsoft GraphRAG: non il nostro setting quindi.\nLa domanda quindi sorge spontanea:perché è stato scelto come benchmark\nse sembra inadeguato?Principalmente per due motivi:\n•è un dataset dove puoi agilmente ricostruire il testo di partenza non chun-\nked, quindi la valutazione della pipeline è influenzata dalla strategia che\nadotti\n•molte delle considerazioni fondamentali alla base del lavoro vengono pro-\nprio confermate dai risultati quantitativi che ricaviamo qui, in particolar\nmodo su Naive e CoT RAG\n51\nQuello che mi piace del benchmark, in particolare, è proprio la possibilità di\nattingere direttamente ai documenti da chunkare: puoi intervenire end-to-end,\nevitare di doverti basare sul testo già chunked o, addirittura, dover costruire\npipeline di scraping complesse per ovviare al problema fondamentale preceden-\nte; senza avere a disposizione direttamente la totalità del testo non è possibile\nosservare, come abbiamo fatto nei risultati che seguono, l’importanza della stra-\ntegia di chunking stessa. Come accennato prima, questa è una qualità che non\nmolti benchmark hanno, il che lo rende più adatto, rispetto alla maggior parte,\na ricavare statistiche significative a supporto delle intuizioni su cui si basa il mio\nlavoro.\nPer ilCoT RAG, d’altronde, il dataset si è rivelato comunque utile soprattutto\nper la fase di prompt engineering e la definizione di vincoli sulla generazione\ndelle sub-questions; osservando direttamente il cicloevidence→scratchpad→\nnext_query, infatti, è possibileformalizzare regole che evitano follow-up “innatu-\nrali”, ad esempio query troppo simili alle precedenti, magari solo riformulazioni,\npiuttosto che evitare di utilizzare l’informazione ricavata dagli step precedenti\ndi retrieval; in questo senso, le prestazioni risultano fortemente dipendenti dal\nprompte, più in generale, da come si istruisce il modello: non intervenendo con\nfine-tuning ad hoc, il contributo principale passa inevitabilmente dal prompt\nengineering. Anche in presenza di un set di domande relativamente “facili” ri-\nspetto alla complessità media del retrieval, l’osservazione qualitativa degli errori\nè sufficiente a progettarepatchefficaci, sfruttando anche l’osservazione dell’evo-\nluzione dello scratchpad. Un esempio concreto è il concetto diEntity Bridging:\nnel ciclo CoT ho osservato che il modello tendeva sì a concentrarsi sull’infor-\nmazione mancante, ma spesso generava query successive molto simili tra loro e,\nsoprattutto, che aumentavano di poco la copertura semantica: in sostanza, il\nrewriting si concentrava spesso su parafrasi, o meglio, identificato cosa mancasse\nil modello era meno tendente ad utilizzare, se presenti, informazioni importanti\nnel contesto retrieved come le entities. Per mitigare questo failure, ho introdotto\nistruzioni che vincolano esplicitamente il modello ad utilizzare le entità (nomi\npropri, luoghi, oggetti) quando queste compaiono nel contesto recuperato: così,\nla query successiva non è solo più specifica, ma anche piùgroundedrispetto a\nciò che il sistema ha effettivamente osservato e, se puoi, strutturare la ricerca su\ninterazioni tra entità è molto efficace, basta vedere i ragionamenti fatti in pre-\ncedenza su GraphRAG o, più generalmente, sui KGs; il modello prova quindi a\ncolmare l’informazione mancante sfruttando direttamente concetti il più possi-\nbilmente discriminanti, che rendono già la ricerca full-text altamente precisa e\nquesto migliora drammaticamente la qualità delle sub-questions. Ovviamente, è\ndifficile presentare risultati quantitativi rispetto quest’ultima affermazione: non\ncisonometrichechedefinisconoquantounaquerysiabuonarispettoadun’altra,\nquindi le considerazioni derivano prevalentemente da un audit manuale.\n52\n5.4.2 Risultati\nPer i risultati che andremo a presentare, si sono utilizzati due modelli disponibili\nsull’APIdiGroq:moonshotai/kimi-k2-instruct-0905ellama-3.1-8b-instant.\nLa scelta tra questi due, semplicemente, deriva da un compromesso che volevo\nraggiungere sui risultati: ci sono approcci di RAG che sono più dipendenti dalla\nqualità del LM utilizzato, come quelli CoT, ed altri che, invece, sono ragione-\nvolmente agnostici a questo. Kimi è un modello MoE (Mixture of Experts) tra\ni più famosi e performanti sui maggiori benchmark, al momento della scrittura\ndi questa tesi oscilla trai top-10 modelli presenti sul mercato, ed è la componen-\nte ’Large’ dei risultati; d’altra parte, la componente ’Small’ è data da uno dei\nmodelli Llama, scelto prevalentemente perché, a parità di size, è notoriamente\nperformante e molto usato in progetti di ricerca.\nIl confronto tra Kimi e Llama è stato condotto su 324 questions di GraphRAG\nBench, distribuite rispetto alle tipologie proposte dal dataset; complessivamente\nsi disponeva circa di2kqueries e 324, da un’analisi quantitativa preliminare, ga-\nrantisce rappresentatività: non ha senso provare gli approcci sulla totalità della\nqueries, basta che i sample scelti siano rappresentativi per ricavare statistiche\ncon una buona confidenza.\nMetric Kimi Mean Llama Mean∆Mean\nanswer_correctness 0.771 0.633 -0.138\nfaithfulness_to_context 0.881 0.675 -0.206\nevidence_coverage 0.762 0.611 -0.150\nCoT steps (mean)1.715 1.573 -0.142\nTabella 4: Confronto Kimi - Llama in CoT RAG.\nNel complesso, i giudizi assegnati a Kimi risultano sistematicamente più alti ri-\nspetto a quelli di Llama su tutte e tre le metriche del judge (GPT-4o), con il gap\npiù marcato sufaithfulness_to_context. Mediamente, infatti, la differenza\n∆ =Llama−Kimi è sempre negativa; un’interpretazione coerente è che, quan-\ndo il retrieval non fornisce un contesto sufficiente (o chiaramente risolutivo),\nLlama tenda a incorrere più spesso in due comportamenti alternativi:over-\ngeneration, introducendo dettagli plausibili ma non supportati dalle evidenze,\ncheabbassafaithfulness_to_context, oppureunder-answering, piùconserva-\ntivo e producendo risposte parziali o vaghe, penalizzandoanswer_correctness\neevidence_coverage. Ovviamente, risulta abbastaza semplice capire come,\nnel caso ideale, vorremmo porci in uno stato ragionevolmente intermedio tra\nquesti due comportamenti: il deficit di Llama non è solo “quanto” risponde,\nmacomegestisce l’incertezza aggiungendo contenuto non grounded o perdendo\ninformazione supportata dal testo. Questa osservazione, in realtà, è sia coeren-\nte con il comportamento più “rigidio” di un judge rispetto all’allineamento tra\naffermazioni e contesto, sia con la differenza qualitativa trai modelli: a parità\ndi correttezza rispetto alla gold answer, la valutazione può ridursi sensibilmente\n53\nse la risposta contiene dettagli non supportati dal retrieval, oppure se il judge\ninterpreta tali dettagli come non verificabili nel contesto, ciò non toglie però\nla tendenza di Llama ad introdurre dettagli non supportati e, nonostante le\nindicazioni, ad inferire informazione non direttamente citata dal testo. Questo\nrisultato è particolarmente rilevante e si collega direttamente ai concetti discus-\nsi nel capitolo sul CoT RAG: in un’impostazione adattiva, il retrieval non è\npiù un’operazione single-shot, ma un processo iterativo in cui il modello deve\ninterpretare correttamente ciò che manca, trasformarlo in una query utile e man-\ntenere coerenza con l’information need iniziale. Modelli più piccoli tendono a\nessere più error prone in questi compiti: il reasoning intermedio può degenerare\npiù facilmente inquery drift, poiché errori di comprensione o di pianificazione si\npropagano step dopo step, accumulando informazioni plausibili ma non perti-\nnenti alla domanda; al contrario, modelli più grandi e meglioinstruction-tuned\nhanno in media maggiore capacità di mantenere il vincolo di grounding e di\ngenerare follow-up più stabili, riducendo la probabilità che il ciclo “scratchpad\n→next_query” si disallinei dall’information need. In CoT RAG quindi, questo\ncorrobora quanto la scelta del modello non sia una componente fondamentale\nper raggiungere trade-off qualità-costo target.\nLa scomposizione perquestion_typerende più chiaro il punto fondamenta-\nle: il vantaggio di Kimi non è legato solo alla “qualità della risposta”, ma a una\nmigliore comprensione testuale nel senso generico; in un setting RAG, questo\nsi traduce in: capacità di comprendere la domanda con precisione, interpre-\ntare correttamente cosa contengono i chunk recuperati ed evitare di produrre\naffermazioni che non siano tracciabili dal contesto. Questo si riflette, come di-\nscutevamoprima, soprattuttoinfaithfulness_to_context, doveilgaprimane\nsistematicamente elevato in tutte le categorie: Kimi tende a mantenere più fa-\ncilmente il grounding, mentre Llama introduce più spesso dettagli “ragionevoli”\nma non supportati nei chunk, degradando l’aderenza al retrieval. Il fatto che\nil divario sia marcato anche suFact Retrievalè particolarmente interessante:\nin questi casi non è richiesta una decomposizione multi-hop complessa, quindi\nla differenza non può essere attribuita solo alla capacità di ragionamento del\nmodello, ma proprio alla comprensione dell’information need stesso; sostanzial-\nmente, anche quando l’informazione necessaria è direttamente contenuta nelle\nevidenze, Kimi sembra più consistente nel selezionare e costruire la risposta ri-\nspetto alle informazioni che soddisfano la query, mentre Llama è più incline a\ncompletare la risposta con inferenze non richieste o non presenti nel contesto,\npenalizzando faithfulness e, di conseguenza, ancheevidence_coverage. Nel\nCoT RAG, poi, non basta “rispondere bene” ma bisognacapire cosa manca\ne trasformarlo in unanext_queryrealmente utile: i risultati sugli step sug-\ngeriscono che Kimi, in media, ’triggera’ più spesso l’adattività, soprattutto su\nContextual Summarize, il che è coerente con un modello più capace di ricono-\nscere un soddisfacimento parziale dell’information need e la necessità di cercare\nulteriormente prima di fermarsi. Questa proprietà è esattamente ciò che ci si\naspetta da un approccio adattivo: la qualità non deriva solo dal primo top-k,\nma dalla capacità del modello di formulare follow-up più puntuali; d’altronde,\n54\nil fatto che generi più step non vuol dire che la qualità delle follow-up questions\nsia minore, ma è perfettamente spiegabile dalle informazioni che le metriche\nti danno sulla comprensione testuale: Llama dichiaraenoughprematuramente,\nma perché non comprende la totalità della semantica mancante.\nUn risultato importante e che, secondo me, emerge molto chiaramente è che,\nin un setting RAG, affidarsi a modelli piccoli (SLM) tipicamente non ti porta\nmolto lontano. Il caso delle queryFact Retrievalè particolarmente emblema-\ntico: si tratta di domande in cui l’information need è spesso localizzabile in un\nsingolo passaggio e, d’altronde, in questo dataset l’adattività tende a fermarsi\npresto proprio perché “bastano” i chunk recuperato già dal primo passo di re-\ntrieval per rispondere; eppure, sembrando le condizioni teoricamente favorevoli,\nnon dovrebbero emergere differenze nette sulla comprensione testuali dei model-\nli data la semplicità del task, cosa che però non avviene: Llama mostra difficoltà\npalesi nel compito fondamentale di estrarre dai chunk ed organizzare all’utente\nl’informazione necessaria per rispondere alla query in ingresso e qui, si noti, non\nsi parla più di bottleneck relativo al retrieval. Infatti, l’informazione chiave è\nspesso già presente nel contesto recuperato, ma il modello non riesce comunque\na valorizzarla: la seleziona male, la comprime in modo scorretto, oppure la di-\nstorce. Questo ridimensiona un’intuizione piuttosto diffusa sul RAG, cioè che\n“basti” un retrieval perfetto per rendere competitivo anche uno SLM: in realtà,\nla qualità finale non dipende solo da cosa recuperi, ma anche da quanto bene\nil modello sa leggere, integrare e fare ’stitching’ di quelle evidenze; non è raro,\nappunto, osservare il caso controintuitivo in cui uno SLM con retrieval migliore\nproduce risposte meno affidabili o meno preferibili di un LLM che parte da un\ncontesto recuperato leggermente peggiore, ma lo interpreta molto meglio.\nSullo stesso benchmark, ho testato anche l’approccio Naive RAG ed i risul-\ntati sono riassunti nella tabella che segue.\nMetric Kimi Mean Llama Mean∆Mean\nanswer_correctness 0.783 0.711 -0.072\nfaithfulness_to_context 0.828 0.752 -0.077\nevidence_coverage 0.777 0.672 -0.105\nTabella 5: Confronto Kimi–Llama in Naive RAG\nI risultati evidenziano chiaramente che l’effetto del CoT RAG non è “univer-\nsale”, ma dipende fortemente dalla dalla qualità del modello: capacità di com-\nprensione, formulazione dei follow-up e attinenza all’information need. Nel ca-\nso diKimi, l’introduzione dello scratchpad produce un guadagno netto sulla\nfaithfulness_to_context(da0.828a0.881), mentreanswer_correctness\nedevidence_coveragerimangono quasi invariate: la direzione è leggermen-\nte negativa ed, in questo senso, il modello sembra adottare un comportamen-\nto più conservativo, preferendo generare risposte leggermente meno complete\nma meglio ancorate al contesto recuperato, piuttosto che rischiare affermazioni\n55\nnon direttamente supportate. Questo pattern è coerente con l’obiettivo fonda-\nmentale del RAG: non solo produrre risposte corrette, ma garantire che ogni\naffermazione sia tracciabile rispetto al contesto fornito. La lievi riduzioni in\nanswer_correctness(−0.012) eevidence_coverage(−0.015), essendo relati-\nvamente piccole rispetto alle differenze osservate, potrebbero sicuramente essere\nsignificative di una tendenza reale ma è anche plausibile che siano semplici\nfluttuazioni, per questo si può considerare praticamente invariata la qualità e\ncopertura delle risposte con buona approssimazione. Questo comportamento,\nd’altronde, è perfettamente coerente con un modello che usa l’adattività prin-\ncipalmente perridurre affermazioni non supportate: lo scratchpad rende più\nesplicita la distinzione tra ciò che è stato recuperato dalle fasi precedenti di\nretrieval e ciò che, invece, sono deduzioni/inferenze del modello, migliorando\nquindi il grounding complessivo della risposta; è un risultato molto interessan-\nte, ai sensi del fatto che, come suddetto, il principio alla base del RAG stesso è\nnon solo rispondere bene, ma in maniera supportata dal contesto.\nMetric∆Kimi(CoT−Classic)∆Llama(CoT−Classic)\nanswer_correctness−0.012−0.078\nfaithfulness_to_context+0.053−0.077\nevidence_coverage−0.015−0.061\nTabella 6: Variazioneintra-modellopassando da RAG Naive a CoT RAG\nIlquadrocomplessivoèdiametralmenteoppostoperLlama: passandodalNaive\nRAG al CoT RAG, tutte le metriche peggiorano in modo consistente, come rias-\nsunto in Tabella 6. Questo non necessariamente vuol dire che il CoT non sia una\nbuona strategia, anzi: i risultati confermano quanto dicevamo precedentemen-\nte sull’importanza delle capacità del modello, per Llama il CoT non aumenta\nl’informazione utile, ma aumenta le opportunità di errore; più step significano\npiù query generate, più evidenze da integrare, più passaggi intermedi da gestire\ne un modello meno versatile può deviare più facilmente dalla domanda iniziale\n(query drift) o interpretare male le informazioni ricavate; d’altronde, c’è un calo\nmolto drastico sia infaithfulness_to_contextche inanswer_correctness.\nUna maniera interessante per vederla è qusta: l’adattività agisce come unmol-\ntiplicatore, quando il modello è sufficientemente capace e instruction-following,\nl’iteratività migliora il grounding; quando non lo è, amplifica la deriva e degra-\nda sia la qualità del contesto sia la risposta finale. Questo risultato corrobora\nancor di più l’idea precedebte che l’adaptive retrieval richieda tipicamente mo-\ndelli medio-grandi, tuttavia evidenzia un altro aspetto importante: sebbene,\nintuitivamente, il Naive RAG possa sembrare una baseline ’strutturale’ del CoT\nRAG, ai sensi del fatto che, non triggerando l’adaptiveness, il comportamento\ndegenera alla variante classica, questo non è supportato dai risultati in maniera\ndiretta; mediamente lo scratchpad, le istruzioni CoT, il reasoning... confonde\nmodelli SLM, che performano peggio della baseline Naive.\nPotrebbe sembrare strano il fatto che la qualità delle risposte e la copertura\n56\nsemantica, date daanswer_correctness, eevidence_coverage, non migliori-\nno con il CoT per un modello grande come Kimi. In realtà, in un setting come il\nnostro il punto chiave è proprio chenon peggiorano: introdurre additività (più\nchiamate LLM, più token, più punti in cui può emergere drift) senza degradare\nla qualità media rispetto alla baseline è già un risultato non banale, perché in-\ndica che il modello non sta “rompendo” il comportamento del RAG classico nei\ncasi in cui il retrieval single-shot è sufficiente, guadagnando però in modo netto\nsullafaithfulness_to_context; per riassumere all’osso: l’adattività, quando\nattivata, è utile al modello. Mi aspettavo, tuttavia, che rispetto a una baseli-\nne Naive la qualità complessiva della risposta rimanesse mediamente simile: gli\nstep adattivi vengono valutati poche volte (si guardi i medium steps) e, anche se\nlo fossero più spesso, siamo comunque in un dominio generico con componenti\nplug-and-play; come suddetto, il prompt engineering può mitigare failure mode e\npermetterti di migliorare le performance, ma generalmente non ti rende compe-\ntitivo e questo vale in generale, non solo nel retrieval. Se cerchi incrementi netti\ndianswer_correctness, questi richiedono tipicamente architetture più “sofisti-\ncate” e facenti riferimento all’idea di ensembling presentata nel capitolo sul CoT\nRAG: è con modelli specializzati al query rewriting, fine-tuning supervisionati\nper seguire uno schema di reasoning specifico o decomposizioni in sub-agents\ncon ruoli distinti, che vedi davvero quanto un approccio CoT migliori le perfor-\nmance della pipeline, soluzioni che qui non adottiamo. D’altronde, l’intuizione\nè semplice: quando fu formulata, la CoT veniva prevalentemente indotta con il\nprompting, poi si passò a fasi di post-training dove ai modelli viene direttamente\ninsegnato come ragionare, fino ad implementazioni più recenti diReinforcement\nLearning, ed è lì che si ebbe l’incremento consistente per uno standard de facto,\nad oggi, nei LM [Mukherjee et al., 2023, DeepSeek-AI et al., 2025]. Per questo,\ndicevo, è assolutamente significativo che le performance non degradino rispet-\nto alla baseline Naive e conferma il punto centrale della tesi: il Naive RAG è\nsorprendentemente difficile da battere e rimane, nella pratica, la soluzionego-to\npiù robusta in un generic domain.\nQuest’ultimo aspetto sulla \"migliore soluzione go-to\" è confermato anche da\nanalisi successive sui costi. Il chunking adottato è di tipo Recursive a finestra\nfissa di300token ed al modello, per ogni step di retrieval, vengono consegnati\ni top-5chunk come contesto: aggiungendo a questo la lunghezza media dello\nscratchpad, il numero medio di step di adaptiveness valutati, il numero medio\ndi token per follow-up query generata, possiamo ottenere stime abbastanza pre-\ncise sull’aumento dei costi rispetto alla baseline classica; d’altronde, il CoT non\nè solomoltiplicatoredi grounding, ma anche di costi!\nI più attenti si accorgeranno che non c’è un fattore moltiplicativo sulla voce\nTotal Tokens: questo viene dal fatto che i costi delle API dei maggiori LLM,\nad oggi, sono dominate dai token di output; essendo che, quindi, un token di in-\nput non pesa come un token prodotto, ricavare dal rapporto trai due un fattore\nmoltiplicativo non sarebbe stato necessariamente corretto e significativo dell’au-\nmento dei costi. Usando i costi del pricing model di Groq, al momento della\nscrittura di questa tesi, otteniamo: un costo medio per query diCCoT ≈0.00606$\n57\nModel Metric CoT Classic F actor\nKimi\nInput tokens 5259 3060 1.7×\nOutput tokens 267 62 4.3×\nTotal tokens 5526 3122\nLlama\nInput tokens 5382 3060 1.8×\nOutput tokens 1261 173 7.3×\nTotal tokens 6643 3234\nTabella 7: Analisi dei Costi: Average Tokens - CoT RAG vs Classic\ncontroC Classic ≈0.00325$perKimi(fattore≈1.87×), eC CoT ≈0.00037$con-\ntroC Classic ≈0.00017$perLlama(fattore≈2.22×). Guardando a possibili\nmiglioramenti dell’architettura CoT, come quelli citati precedentemente, questi\nnon dovrebbero perturbare eccessivamente i costi, rimanendo confrontabili alla\nsituazione attuale differentemente dalle performance: ovviamente, i costi cre-\nscono molto in base alla difficoltà delle domande, o meglio, al numero medio di\nvolte che viene valutata l’adattività, tuttavia, da questi risultati, sembra come\nla direzione del tradeoff tra incremento possibile delle performance della pipeline\ne costo sia, in realtà, favorevole sopratutto per modelli di grandi dimensioni.\nChunk SizeIn assenza di un dominio specifico, diverse linee guida ’indu-\nstriali’ indicano intervalli di200−400token come raccomandati; per i risultati\nprecedenti ho, appunto, scelto300token come chunk size [Singh et al., 2024,\nCoveo, 2024]. Per studiare meglio quanto le performance della pipeline dipen-\ndano da quest’ultimo, ho ripetuto gli stessi esperimenti con, stavolta, una con-\nfigurazione di600 token: raddoppio della baseline, chunk mediamente più\nauto-contenuti e contesto generalmente meno frammentato; la scelta di raddop-\npiare il chunk size è, d’altronde, abbastanza coerente a risultati sperimentali in\nletteratura, come quelli presentati da [Juvekar and Purwar, 2024], che propon-\ngono un range più ampio tra512−1024token quando serve mediamente più\ncontesto alla generazione.\nMetric Kimi Mean Llama Mean∆Mean\nanswer_correctness 0.785 0.624 -0.161\nfaithfulness_to_context 0.878 0.661 -0.217\nevidence_coverage 0.776 0.604 -0.172\nTabella 8: Confronto Kimi–Llama in CoT RAG (600 token)\nI risultati con chunk da 600 token confermano il pattern già emerso nelle di-\nscussioni precedenti: Kimi mantiene prestazioni sostanzialmente stabili, mentre\nLlama degrada in modo marcato e, soprattutto, sistematico su tutte le metri-\nche del judge, con un gap particolarmente ampio sufaithfulness_to_context.\nAumentare il chunk size, cioè fornire più contesto e meno frammentato al model-\n58\nlo, non “aiuta” Llama come ci si potrebbe aspettare intuitivamente, al contrario,\namplifica le difficoltà nel selezionare ed estrarre l’informazione importante dal\ncontesto recuperato e questo è assolutamente coerente con le difficoltà di com-\nprensione testuale precedentemente esposte. Chunk più lunghi introducono più\nsegnali, chiamiamoli, concorrenti (entità, eventi, descrizioni...) e richiedono una\nmigliore capacità di identificare i chunk realmente rilevanti rispetto alla doman-\nda, mantenere coerenza nella costruzione della risposta e, soprattutto, sintetiz-\nzare meglio più informazioni e più dettagli; il chunking a 600 token riduce la\nframmentazione, ma sposta il carico sul modello: serve maggiore compressione\nsemantica. Il fatto che Kimi regga meglio questo regime, mentre Llama perda\nsia inanswer_correctnesssia inevidence_coverage, suggerisce che l’aumen-\nto di contesto diventa un vantaggio solo quando il modello ha capacità sufficienti\nper sfruttarlo, altrimenti la maggiore lunghezza si traduce in rumore aggiuntivo\ne in peggior grounding; concettualmente, le considerazioni precedenti, in realtà,\nsi basano direttamente sull’idea che più contesto sia difficile da gestire per uno\nSLM: di per sé, però, non stiamo isolando l’effetto dell’adattività, quindi dell’ul-\nteriore capacità di comprensione testuale che richiediamo al modello. Le analisi\nsuccessive, infatti, vogliono rispondere alla domanda:è l’adattività o l’aumento\ndi token che pesa allo SLM?\nI risultati precedenti rinforzano, d’altronde, una delle sfumature centrali di que-\nsto lavoro: in un setting zero-shot e generic-domain, il chunk size non è un\nsolamente un iperparametro “universale” della pipeline, ma interagisce forte-\nmente con la qualità del modello scelto. Un chunk più grande può rendere il\nretrieval più coerente per testi narrativi magari, ma rende anche più evidente\nla differenza tra un LLM in grado di sfruttare contesto lungo e uno SLM che\nfatica ad isolare le informazioni rilevanti, anche quando queste sono effettiva-\nmente presente nei chunk recuperati. Nel setting di riferimento per CoT RAG,\nin realtà, raddoppiando il chunk size hai un incremento proporzionale dei costi,\nquindi i300token scelti inizialmente sono una scelta molto più sensata.\nMetric Kimi Mean Llama Mean∆Mean\nanswer_correctness 0.824 0.729 -0.095\nfaithfulness_to_context 0.841 0.762 -0.079\nevidence_coverage 0.816 0.697 -0.119\nTabella 9: Confronto Kimi–Llama in Naive RAG (600 token).\nL’incremento dei costi, invece, è assolutamente supportato dal gain sulle per-\nformance in Tabella 9: aumentando la dimensione del chunk, diminuisce la\nframmentazione e cresce la probabilità che l’informazione richiesta sia contenu-\nta nello stesso passaggio recuperato, i chunk sono più auto-contenuti insomma.\nQuesto effetto è particolarmente evidente perKimi, che mostra un incremen-\nto netto sia inanswer_correctnesssia inevidence_coverage: un modello\npiù capace riesce a sfruttare meglio chunk più auto-contenuti, avendo maggio-\n59\nre capacità di gestire l’informazione. In realtà, dall’analisi delle performance\ndiLlamaun beneficio esiste: sebbene avevamo discusso di come Llama abbia\nuna minore capacità di Kimi nella comprensione testuale, la crescita del chunk\nsize migliore la qualità delle risposte prodotte dal modello su tutti i fronti; que-\nsto non è un risultato che contraddice le discussioni precedenti, anzi, permette\ndi evidenziare un punto importante: nel Naive RAG, un contesto più lungo\npuò essere vantaggioso perché, di base, riduce il bisogno di adattività disponen-\ndo di più informazione, mentre nel CoT RAG questo diventa un problema in\nrelazione al fatto che occorre una comprensionedinamica, cioè la capacità di\nmantenere coerenza lungo più step, aggiornare correttamente uno stato (scrat-\nchpad), riconoscere cosa manca rispetto all’information need e trasformarlo in\nunanext_queryutile senza introdurre drift. L’aumento dei token, infatti, non\nè un problema necessariamente per uno SLM fin tanto che sia necessario ’sta-\ntismo’ nella comprensione testuale: l’adattività richiede capacità superiori di\ncomprensione semantica e non solo di ’trovare’ strettamente l’informazione nel\ntesto, moltiplicando i punti decisionali e rende il modello più error prone. Un\nLLM come Kimi tende a beneficiare dell’adattività perché riesce a controllarla,\nmentre un SLM come Llama può peggiorare non perché “vede troppi token”, ma\nperché fatica a gestire la dinamica del processo e questo spiega la covarianza\nnegativa tra incremento di performance nel Naive a all’aumentare dei token e\ndecremento prestazionale nel CoT per Llama, risponendo alla domanda posta\nprecedentemente.\n5.4.3 HotpotQA\nPer estendere l’analisi ad un benchmark più ’challenging’ rispetto alla porzione\nNoveldi GraphRAG Bench, ho utilizzatoHotpotQA[Yang et al., 2018]: da-\ntaset di question answering su Wikipedia progettato esplicitamente per ilmulti-\nhop reasoning. Oltre a fornire unagold answerper ogni domanda, includesup-\nporting factsa livello di frase e tipologie di quesiti (es.bridgeecomparison) che\nrichiedono di combinare evidenze da più documenti. La differenza sostanziale\nrispetto a GraphRAG-Bench non è tanto la forma della query, infatti rimangono\nsimili anche in HotpotQA con, spesso e volentieri, entities ed information need\nchiari, bensì il fatto che la risposta non è tipicamente recuperabile con un sin-\ngolo match: nelle domandebridge, ad esempio, l’entità citata funge daancora\nper individuare un’entità ponte in un primo documento e recuperare un secon-\ndo documento contenente il fatto risolutivo e che, d’altronde, è uno dei prompt\ntunings che avevamo discusso precedentemente con il CoT; questo è un punto\nimportante: la facilità di GraphRAG Bench non è unicamente data dalla forma\ndelle queries, d’altronde per poter valutare un sistema di retrieval devi fissare\ninformation need chiari, anche se questo rimane un problema da non trascura-\nre, infatti ad inferenza potresti avere situazioni molto diverse e con maggiore\nambiguità, ma più che altro dal fatto che la maggior parte viene risolta già con\nil primo step di retrieval e l’adattività non viene valutata sempre: HotpotQA\nè diverso proprio in questo senso. Nelle domandecomparison, per esempio, è\nnecessario recuperare e confrontare informazioni da due pagine distinte ed, in\n60\nquesto setting quindi, la componente di retrieval pesa in modo sostanzialmente\npiù forte end-to-end: il sistema deve portare tra i primi risultati (e poi nel con-\ntesto)tuttigli ’hop’ richiesti.\nCome già discusso, la stragrande maggioranza dei benchmark non propone il\ntesto ’raw’ da indicizzare ed HotpotQA non è da meno: per questo, nella co-\nstruzione del dataset, ho valutato una fase di scraping sfruttando l’API di Wiki-\npedia, chunkando ed indicizzando il testo completo. Mi sono concentrato sulla\nporzione ’validation’ di HotpotQA, fissando 400 esempi: questo è comprensivo\nsolo delle queries labeled ’hard’, quindi il massimo grado di difficoltà,\n5.5 UI Streamlit\nÈ possibile interagire con tutte le architetture RAG tramite una semplice UI\nStreamlitdisponibile nella repository GitHub a supporto della tesi (https://\ngithub.com/ddemarchis11/TesiMagistrale_AdaptiveHierarchicalRAG). L’u-\ntente può selezionare la pipeline RAG tramite un menu multi-choice, oltre che\ntarare parametri di esecuzione ad alto impatto come: numero di chunk con-\nsegnati al modello dal retrieval, numero di step CoT... dando una maggiore\ninterazione con le componenti più tecniche dell’architettura. Come ampiamen-\nte trattato, il concetto fondamentale del RAG è favorire trasparenza rispetto\nalle fonti utilizzate, difatti l’interfaccia permette di visionare direttamente i\nchunk recuperati e, opzionalmente, informazioni di debug utili per analizzare il\ncomportamento del retriever e la qualità del contesto fornito al modello.\nFigura 1: Layout UI Streamlit - CoT Architecture\n61\n5.5.1 Text Extraction\nLa UI ci permette, d’altronde, di discutere di una fase fondamentale per progetti\ndi RAG ’reali’: laText Extraction; l’utente può caricare file in formato.pdfo\n.txt, questi verranno, poi, indicizzati su Elasticsearch e resi, difatto, interroga-\nbili. La mia idea era, appunto, mostrare come le UI non debbano soltanto essere\n“front-end” di QA, ma soprattutto un punto di ingresso per caricare i documenti\nstessi in modo autonomo, basandomi su formati molto semplici e che non richie-\ndono eccessivo effort in progettazione. L’importanza della Text Extraction è\nstata già discussa, soprattutto in relazione a strategie di Chunking basate sulla\nstruttura del documento e non ha molto senso ripeterci ancora. Per i PDF,\nl’estrazione può essere eseguita con librerie Python abbastanza semplici come\npypdf, che offrono metodi diretti per ricavare il contenuto testuale; tuttavia,\nquesta soluzione “semplice” è spesso insufficiente quando l’obiettivo è preservare\nla struttura logica o, più generalmente, il layout del documento: intestazioni,\nparagrafi, colonne, tabelle, note a piè di pagina... possono venire riordinate o\nmescolate con il testo da approcci semplici comepypdfe questo viene dal fatto\nche il PDF è principalmente un formato orientato al layout grafico più che alla\nsemantica del contenuto [Atagong et al., 2025, Ramakrishnan et al., 2012].\nPreservare la struttura non è un dettaglio puramente estetico: determina diret-\ntamente la qualità del testo indicizzato. Nei PDF con layout complessi (multi-\ncolonna, tabelle, moduli), un’estrazione “plug-and-play” tende a mescolare testo\ned elementi strutturali, producendo chunk in cui contenuto utile e rumore (righe\ndi tabella, campi di form, intestazioni ripetute...) si sovrappongono, degradan-\ndo sia il retrieval (dense, full-text o ibrido) sia la generazione. Se la struttura\nnon viene mantenuta o ricostruita, diventa infatti difficile applicare chunking\nmirato (per sezioni, blocchi o tabelle), che spesso è una soluzione estremamen-\nte semplice ed efficiente in contesti domain specific (vedi manuali, benchmark\nlegali...). Questo collega naturalmente l’estrazione a tecniche OCR edocument\nunderstanding: avendo parlato delle SDK di Azure, infatti, per popolare lo\nStorage potresti sfruttareAzure AI Document Intelligence, il precedente Form\nRecognizer), che include modelliReadper OCR e modelliLayoutper l’analisi\ndella struttura del documento, con capacità di estrarre non solo testo ma anche\ntabelle ed altri elementi strutturali, offrendo possibilità sia di utilizzare modelli\npre-trained su documenti molto comuni (ricevute, bilanci...) che di fine-tunings\ndegli stessi sul tuo scenario [Microsoft, 2025, Microsoft, 2024].\n62\n6 RAG per Structured Data\n6.1 Data Lakes\n6.2 Semantic Table Graph\n6.3 HybridQA Dataset\n6.4 Risultati\nNaive soluzione to - go.\nRiferimenti bibliografici\n[Atagong et al., 2025] Atagong, S. D., Tonnang, H., Senagi, K., Wamalwa, M.,\nAgboka, K.M., andOdindi, J.(2025). Areviewonknowledgeandinformation\nextraction from pdf documents and storage approaches.Frontiers in Artificial\nIntelligence, 8:1466092.\n[Blondel et al., 2008] Blondel, V. D., Guillaume, J.-L., Lambiotte, R., and Le-\nfebvre, E. (2008). Fast unfolding of communities in large networks.Journal\nof Statistical Mechanics: Theory and Experiment, 2008(10):P10008.\n[Chen et al., 2025a] Chen, H. et al. (2025a). Interpreting the curse of di-\nmensionality from distance concentration and manifold effect.arXiv\npreprint.\n[Chen, 2025] Chen, M. (2025). What are small language models (slms)? how\ndo they work?https://www.oracle.com/it/artificial-intelligence/\nsmall-language-models/. Accessed: 2026-02-06.\n[Chen et al., 2025b] Chen, Z., Pradeep, R., and Lin, J. (2025b). Accelerating\nlistwise reranking: Reproducing and enhancing first. InProceedings of the\n48th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR ’25), Padua, Italy. ACM.\n[Coveo, 2024] Coveo (2024). Chunking strategy / passage retrieval best practi-\nces (fixed-size chunking). La documentazione riporta una media di 300 token\nper chunk (min 200, max 400) per fixed-size chunking in contesti di passage\nretrieval.\n[DeepSeek-AI et al., 2025] DeepSeek-AI et al. (2025). Deepseek-r1: Incentivi-\nzing reasoning capability in llms via reinforcement learning.arXiv preprint\narXiv:2501.12948. Dimostracomeilpost-trainingconRL(\"Large-ScaleRein-\nforcement Learning\") incentivi il modello a generare autonomamente tracce\ndi ragionamento (CoT) per risolvere problemi complessi.\n[Edge et al., 2024] Edge, D., Trinh, H., Cheng, Y., Bradley, J., Chao, A., Mody,\nA., Truitt, S., and Larson, J. (2024). From local to global: A graph rag\napproach to query-focused summarization.arXiv preprint arXiv:2404.16130.\n63\n[Feldbauer and Flexer, 2019] Feldbauer, R. and Flexer, A. (2019). A com-\nprehensive empirical comparison of hubness reduction in high-dimensional\nspaces.Knowledge and Information Systems.\n[Fortunato, 2010] Fortunato, S. (2010). Community detection in graphs.\nPhysics Reports, 486(3–5):75–174.\n[Jacob et al., 2024] Jacob, M., Lindgren, E., Zaharia, M., Carbin, M., Khattab,\nO., and Drozdov, A. (2024). Drowning in documents: Consequences of scaling\nreranker inference.arXiv preprint.\n[Juvekar and Purwar, 2024] Juvekar, K. and Purwar, A. (2024). Introducing\na new hyper-parameter for rag: Context window utilization.arXiv preprint\narXiv:2407.19794.\n[Kojima et al., 2022] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. (2022). Large language models are zero-shot reasoners.arXiv preprint\narXiv:2205.11916.\n[Liu et al., 2023] Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C.\n(2023). G-eval: Nlg evaluation using gpt-4 with better human alignment.\narXiv preprint arXiv:2303.16634.\n[Microsoft, 2024] Microsoft (2024). Document layout analysis (layout model)\n— azure ai document intelligence.https://learn.microsoft.com/en-us/\nazure/ai-services/document-intelligence/prebuilt/layout?view=\ndoc-intel-4.0.0. Accessed: 2026-02-11.\n[Microsoft, 2025] Microsoft (2025). Read model ocr data extraction —\nazure ai document intelligence.https://learn.microsoft.com/en-us/\nazure/ai-services/document-intelligence/prebuilt/read?view=\ndoc-intel-4.0.0. Accessed: 2026-02-11.\n[Mukherjee et al., 2023] Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S.,\nIyyer, M., andAwadallah, A.(2023). Orca: Progressivelearningfromcomplex\nexplanationtraces ofgpt-4.arXiv preprint arXiv:2306.02707. Introduce l’idea\ndi \"Explanation Tuning\": addestrare un modello piccolo usando le tracce di\nragionamento (CoT) di un modello più grande, rendendo il ragionamento una\ncapacità appresa tramite SFT.\n[Nye et al., 2021] Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sut-\nton, C., and Odena, A. (2021). Show your work: Scratchpads for intermediate\ncomputation with language models.\n[Press et al., 2022] Press, O., Smith, N. A., and Lewis, M. (2022). Measuring\nand narrowing the compositionality gap in language models.arXiv preprint\narXiv:2210.03350. Introduces Self-Ask prompting and shows how to plug in\nsearch.\n64\n[Qu et al., 2025] Qu, R., Tu, R., and Bao, F. S. (2025). Is semantic chunking\nworth the computational cost? InFindings of the Association for Com-\nputational Linguistics: NAACL 2025, pages 2155–2177, Albuquerque, New\nMexico. Association for Computational Linguistics.\n[Ramakrishnan et al., 2012] Ramakrishnan, C., Patnia, A., Hovy, E., and\nBurns, G. A. P. C. (2012). Layout-aware text extraction from full-text pdf\nof scientific articles.Source Code for Biology and Medicine, 7(1):7. Accessed:\n2026-02-11.\n[Reimers and Gurevych, 2021] Reimers, N. and Gurevych, I. (2021). The cur-\nse of dense low-dimensional information retrieval for large index sizes. In\nProceedings of ACL-IJCNLP (Short Papers).\n[Sarthi et al., 2024] Sarthi, P. et al. (2024). Raptor: Recursive abstractive\nprocessing for tree-organized retrieval. InICLR.\n[Singh et al., 2024] Singh, S., Pecora, C., and Khanuja, M. (2024). Amazon be-\ndrock knowledge bases now supports advanced parsing, chunking, and query\nreformulation giving greater control of accuracy in rag based applications.\nNel post viene indicato un valore raccomandato di 300 per il parametromax\ntoken size for a chunk.\n[Subramanian et al., 2025] Subramanian, S., Elango, V., and Gungor, M.\n(2025). Small language models (slms) can still pack a punch: A survey.\narXiv preprint arXiv:2501.05465.\n[Traag et al., 2019] Traag, V. A., Waltman, L., and van Eck, N. J. (2019).\nFrom louvain to leiden: guaranteeing well-connected communities.Scientific\nReports.\n[Trivedi et al., 2023] Trivedi, H., Balasubramanian, N., Khot, T., and Sabhar-\nwal, A. (2023). Interleaving retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions.arXiv preprint arXiv:2212.10509.\n[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jo-\nnes, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all\nyou need. InAdvances in Neural Information Processing Systems (NeurIPS).\n[Wang et al., 2022] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and\nZhou, D. (2022). Self-consistency improves chain of thought reasoning in\nlanguage models.arXiv preprint arXiv:2203.11171.\n[Wei et al., 2022] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E. H.,\nLe, Q. V., and Zhou, D. (2022). Chain-of-thought prompting elicits reasoning\nin large language models.arXiv preprint arXiv:2201.11903.\n[Yang et al., 2018] Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W.,\nSalakhutdinov, R., and Manning, C. D. (2018). Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering. InProceedings of the\n65\n2018 Conference on Empirical Methods in Natural Language Processing, pages\n2369–2380. Association for Computational Linguistics.\n[Yao et al., 2022] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. (2022). React: Synergizing reasoning and acting in language\nmodels.arXiv preprint arXiv:2210.03629.\n[Zheng et al., 2023] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-\njudge with mt-bench and chatbot arena.arXiv preprint arXiv:2306.05685.\n66"}
